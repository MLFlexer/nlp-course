{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Week 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move from binary classification to span-based QA, i.e., identifying the span in the document that answers the question, when it is answerable.\n",
    "Let k be the number of members in your group. Using the training data, implement k different sequence labellers for each of the three languages, which predict which tokens in a document are part of the answer to the correspond- ing question. Evaluate the sequence labellers on the respective validation sets, report and analyse the performance for each language and compare the scores across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bpemb in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: numpy in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (4.65.0)\n",
      "Requirement already satisfied: requests in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (2.29.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (0.1.99)\n",
      "Requirement already satisfied: gensim in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim->bpemb) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim->bpemb) (1.10.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (3.1.0)\n",
      "Requirement already satisfied: gensim in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (1.10.1)\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: setuptools in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (66.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.29.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.15)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: fasttext in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from fasttext) (2.11.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from fasttext) (66.0.0)\n",
      "Requirement already satisfied: numpy in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from fasttext) (1.24.3)\n",
      "Requirement already satisfied: datasets in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (2.29.0)\n",
      "Requirement already satisfied: xxhash in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: packaging in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: multiprocess in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: filelock in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sklearn in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (0.0.post5)\n"
     ]
    }
   ],
   "source": [
    "!pip install bpemb\n",
    "!pip install gensim\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install fasttext\n",
    "!pip install datasets\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import torch\n",
    "import random\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
    "from typing import List, Tuple, AnyStr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/emmastoklundlee/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4590ae68624c8fae10ee98d2ecc1b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116067\n",
      "13325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language   \n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish  \\\n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations   \n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...  \\\n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext   \n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...  \\\n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
    "\n",
    "\n",
    "def answer_text(df):\n",
    "    # create new column with 1 if answerable, 0 if not answerable\n",
    "    df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
    "    # drop all rows with answerable = 0\n",
    "    # df = df[df['answerable'] == 1]\n",
    "    # return answer_text from annotations\n",
    "    df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
    "    # create new column with answer_start converted to int\n",
    "    df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load spaCy model (you can choose a different model if needed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "def label(df):\n",
    "    # Initialize labels with zeros for each document token for each document in the dataframe\n",
    "    df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
    "\n",
    "    # Tokenize the answer text\n",
    "    df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
    "\n",
    "    # Tokenize and process the document plaintext\n",
    "    df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
    "\n",
    "    # Find the starting index of answer_text in document_plaintext for each document\n",
    "    df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
    "\n",
    "    # Mark the corresponding tokens in document_plaintext_tokenized with 1\n",
    "    df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
    "    df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1017 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_58656/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_train_english = label(answer_text(df_train_english))\n",
    "df_val_english = label(answer_text(df_val_english))\n",
    "# df_train_bengali = label(answer_text(df_train_bengali))\n",
    "# df_val_bengali = label(answer_text(df_val_bengali))\n",
    "# df_train_arabic = label(answer_text(df_train_arabic))\n",
    "# df_val_arabic = label(answer_text(df_val_arabic))\n",
    "# df_train_indonesian = label(answer_text(df_train_indonesian))\n",
    "# df_val_indonesian = label(answer_text(df_val_indonesian))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answerable\n",
      "1    3696\n",
      "0    3693\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count values in answerable column\n",
    "print(df_train_english['answerable'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25001, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bpemb import BPEmb\n",
    "\n",
    "# Load English model with 25k word-pieces\n",
    "bpemb_id = BPEmb(lang='eng', dim=100, vs=25000)\n",
    "\n",
    "# Assuming bpemb_id is your pre-trained word embeddings (e.g., fastText, Word2Vec, or GloVe)\n",
    "\n",
    "# Extract the embeddings\n",
    "pretrained_embeddings = bpemb_id.emb.vectors\n",
    "\n",
    "# Define the [PAD] token embedding as all zeros\n",
    "pad_embedding = np.zeros(shape=(1, 100))\n",
    "\n",
    "# Concatenate the embeddings with the [PAD] token\n",
    "pretrained_embeddings_with_pad = np.concatenate([pretrained_embeddings, pad_embedding], axis=0)\n",
    "\n",
    "# Extract the vocab and add an extra [PAD] token\n",
    "vocabulary = bpemb_id.emb.index_to_key + ['[PAD]']\n",
    "\n",
    "# Create a dictionary from the embeddings\n",
    "embedding_dict = {token: embedding for token, embedding in zip(vocabulary, pretrained_embeddings_with_pad)}\n",
    "\n",
    "# Ensure that the shape of pretrained_embeddings_with_pad is correct\n",
    "print(pretrained_embeddings_with_pad.shape)\n",
    "\n",
    "\n",
    "# Define a function to tokenize and embed text\n",
    "def embed_text(df, embedding_dict):\n",
    "    tokenized_text_list = []\n",
    "\n",
    "    for document_text in df['document_plaintext_tokenized']:\n",
    "        # Tokenize the document text\n",
    "        tokens = [token for token in document_text]\n",
    "\n",
    "        # Map tokens to embeddings using the dictionary\n",
    "        token_embeddings = [embedding_dict.get(token, embedding_dict['[PAD]']) for token in tokens]\n",
    "\n",
    "        # Append the token embeddings to the list\n",
    "        tokenized_text_list.extend(token_embeddings)\n",
    "\n",
    "    # Return the sequence embeddings as a NumPy array\n",
    "    sequence_embedding = np.array(tokenized_text_list)\n",
    "    return sequence_embedding\n",
    "\n",
    "# Usage:\n",
    "sequence_embedding = embed_text(df_train_english, embedding_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_embed_train_english = embed_text(df_train_english, embedding_dict)\n",
    "seq_embed_val_english = embed_text(df_val_english, embedding_dict)\n",
    "# seq_embed_train_bengali = embed_text(df_train_bengali, embedding_dict)\n",
    "# seq_embed_val_bengali = embed_text(df_val_bengali, embedding_dict)\n",
    "# seq_embed_train_arabic = embed_text(df_train_arabic, embedding_dict)\n",
    "# seq_embed_val_arabic = embed_text(df_val_arabic, embedding_dict)\n",
    "# seq_embed_train_indonesian = embed_text(df_train_indonesian, embedding_dict)\n",
    "# seq_embed_val_indonesian = embed_text(df_val_indonesian, embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983911, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_embed_train_english.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex dataframe\n",
    "df_train_english = df_train_english.reset_index(drop=True)\n",
    "df_val_english = df_val_english.reset_index(drop=True)\n",
    "# df_train_bengali = df_train_bengali.reset_index(drop=True)\n",
    "# df_val_bengali = df_val_bengali.reset_index(drop=True)\n",
    "# df_train_arabic = df_train_arabic.reset_index(drop=True)\n",
    "# df_val_arabic = df_val_arabic.reset_index(drop=True)\n",
    "# df_train_indonesian = df_train_indonesian.reset_index(drop=True)\n",
    "# df_val_indonesian = df_val_indonesian.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one long list of the lists in df_train_english['labels']\n",
    "\n",
    "def get_labels(df):\n",
    "    labels = []\n",
    "    for i in range(len(df)):\n",
    "        labels.extend(df['labels'][i])\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iob_labels(df):\n",
    "    labels = []\n",
    "    for i in range(len(df)):\n",
    "        labels.extend(df['iob_tags'][i])\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_labels = get_labels(df_train_english)\n",
    "english_val_labels = get_labels(df_val_english)\n",
    "# bengali_train_labels = get_labels(df_train_bengali)\n",
    "# bengali_val_labels = get_labels(df_val_bengali)\n",
    "# arabic_train_labels = get_labels(df_train_arabic)\n",
    "# arabic_val_labels = get_labels(df_val_arabic)\n",
    "# indonesian_train_labels = get_labels(df_train_indonesian)\n",
    "# indonesian_val_labels = get_labels(df_val_indonesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_train_labels[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_iob_labels = get_iob_labels(df_train_english)\n",
    "english_val_iob_labels = get_iob_labels(df_val_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>answerable</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start_int</th>\n",
       "      <th>labels</th>\n",
       "      <th>answer_text_tokenized</th>\n",
       "      <th>document_plaintext_tokenized</th>\n",
       "      <th>start_index</th>\n",
       "      <th>iob_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [159], 'answer_text': ['1920s']}</td>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "      <td>1</td>\n",
       "      <td>1920s</td>\n",
       "      <td>159</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1920s]</td>\n",
       "      <td>[quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>31</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
       "      <td>List of Nobel laureates in Literature</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [610], 'answer_text': ['Sully...</td>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sully Prudhomme</td>\n",
       "      <td>610</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[sull, ##y, pr, ##ud, ##hom, ##me]</td>\n",
       "      <td>[the, nobel, prize, in, literature, (, swedish...</td>\n",
       "      <td>120</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When is the dialectical method used?</td>\n",
       "      <td>Dialectic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [129], 'answer_text': ['disco...</td>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
       "      <td>1</td>\n",
       "      <td>discourse between two or more people holding d...</td>\n",
       "      <td>129</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[discourse, between, two, or, more, people, ho...</td>\n",
       "      <td>[dialect, ##ic, or, dialect, ##ics, (, greek, ...</td>\n",
       "      <td>33</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who invented Hangul?</td>\n",
       "      <td>Origin of Hangul</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [88], 'answer_text': ['Sejong...</td>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>Sejong the Great</td>\n",
       "      <td>88</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[se, ##jong, the, great]</td>\n",
       "      <td>[hangul, was, personally, created, and, promu,...</td>\n",
       "      <td>18</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do Grasshoppers eat?</td>\n",
       "      <td>Grasshopper</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [0], 'answer_text': ['Grassho...</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
       "      <td>1</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[grasshoppers, are, plant, -, eat, ##ers, ,, w...</td>\n",
       "      <td>[grasshoppers, are, plant, -, eat, ##ers, ,, w...</td>\n",
       "      <td>0</td>\n",
       "      <td>[B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>What was Neil Brooks' fastest recorded time?</td>\n",
       "      <td>Swimming at the 1980 Summer Olympics – Men's 4...</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>The medley relay was scheduled in the Olympisk...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Swimming%20at%20...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[the, medley, relay, was, scheduled, in, the, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>Who are the three most important eastern philo...</td>\n",
       "      <td>Eastern philosophy</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Sāmkhya is a dualist philosophical tradition b...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Eastern%20philos...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[sam, ##kh, ##ya, is, a, dual, ##ist, philosop...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>Who was costume designer for the first Star Wa...</td>\n",
       "      <td>John Mollo</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>Mollo was surprised by the success of Star War...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/John%20Mollo</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[moll, ##o, was, surprise, ##d, by, the, succe...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>Who developed the first thermonuclear weapon?</td>\n",
       "      <td>History of nuclear weapons</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>In the end, President Truman made the final de...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/History%20of%20n...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[in, the, end, ,, president, truman, made, the...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>What is the population of Mahwah, NJ?</td>\n",
       "      <td>Mahwah, New Jersey</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [-1], 'answer_text': ['']}</td>\n",
       "      <td>The previous mayor, Bill Laforet faced a recal...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mahwah%2C%20New%...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[the, previous, mayor, ,, bill, la, ##for, ##e...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7389 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question_text   \n",
       "0              When was quantum field theory developed?  \\\n",
       "1     Who was the first Nobel prize winner for Liter...   \n",
       "2                  When is the dialectical method used?   \n",
       "3                                  Who invented Hangul?   \n",
       "4                             What do Grasshoppers eat?   \n",
       "...                                                 ...   \n",
       "7384       What was Neil Brooks' fastest recorded time?   \n",
       "7385  Who are the three most important eastern philo...   \n",
       "7386  Who was costume designer for the first Star Wa...   \n",
       "7387      Who developed the first thermonuclear weapon?   \n",
       "7388              What is the population of Mahwah, NJ?   \n",
       "\n",
       "                                         document_title language   \n",
       "0                                  Quantum field theory  english  \\\n",
       "1                 List of Nobel laureates in Literature  english   \n",
       "2                                             Dialectic  english   \n",
       "3                                      Origin of Hangul  english   \n",
       "4                                           Grasshopper  english   \n",
       "...                                                 ...      ...   \n",
       "7384  Swimming at the 1980 Summer Olympics – Men's 4...  english   \n",
       "7385                                 Eastern philosophy  english   \n",
       "7386                                         John Mollo  english   \n",
       "7387                         History of nuclear weapons  english   \n",
       "7388                                 Mahwah, New Jersey  english   \n",
       "\n",
       "                                            annotations   \n",
       "0     {'answer_start': [159], 'answer_text': ['1920s']}  \\\n",
       "1     {'answer_start': [610], 'answer_text': ['Sully...   \n",
       "2     {'answer_start': [129], 'answer_text': ['disco...   \n",
       "3     {'answer_start': [88], 'answer_text': ['Sejong...   \n",
       "4     {'answer_start': [0], 'answer_text': ['Grassho...   \n",
       "...                                                 ...   \n",
       "7384        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "7385        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "7386        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "7387        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "7388        {'answer_start': [-1], 'answer_text': ['']}   \n",
       "\n",
       "                                     document_plaintext   \n",
       "0     Quantum field theory naturally began with the ...  \\\n",
       "1     The Nobel Prize in Literature (Swedish: Nobelp...   \n",
       "2     Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
       "3     Hangul was personally created and promulgated ...   \n",
       "4     Grasshoppers are plant-eaters, with a few spec...   \n",
       "...                                                 ...   \n",
       "7384  The medley relay was scheduled in the Olympisk...   \n",
       "7385  Sāmkhya is a dualist philosophical tradition b...   \n",
       "7386  Mollo was surprised by the success of Star War...   \n",
       "7387  In the end, President Truman made the final de...   \n",
       "7388  The previous mayor, Bill Laforet faced a recal...   \n",
       "\n",
       "                                           document_url  answerable   \n",
       "0     https://en.wikipedia.org/wiki/Quantum%20field%...           1  \\\n",
       "1     https://en.wikipedia.org/wiki/List%20of%20Nobe...           1   \n",
       "2               https://en.wikipedia.org/wiki/Dialectic           1   \n",
       "3     https://en.wikipedia.org/wiki/Origin%20of%20Ha...           1   \n",
       "4             https://en.wikipedia.org/wiki/Grasshopper           1   \n",
       "...                                                 ...         ...   \n",
       "7384  https://en.wikipedia.org/wiki/Swimming%20at%20...           0   \n",
       "7385  https://en.wikipedia.org/wiki/Eastern%20philos...           0   \n",
       "7386         https://en.wikipedia.org/wiki/John%20Mollo           0   \n",
       "7387  https://en.wikipedia.org/wiki/History%20of%20n...           0   \n",
       "7388  https://en.wikipedia.org/wiki/Mahwah%2C%20New%...           0   \n",
       "\n",
       "                                            answer_text  answer_start_int   \n",
       "0                                                 1920s               159  \\\n",
       "1                                       Sully Prudhomme               610   \n",
       "2     discourse between two or more people holding d...               129   \n",
       "3                                      Sejong the Great                88   \n",
       "4     Grasshoppers are plant-eaters, with a few spec...                 0   \n",
       "...                                                 ...               ...   \n",
       "7384                                                                   -1   \n",
       "7385                                                                   -1   \n",
       "7386                                                                   -1   \n",
       "7387                                                                   -1   \n",
       "7388                                                                   -1   \n",
       "\n",
       "                                                 labels   \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \\\n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "7384  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7385  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7386  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7387  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "7388  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  answer_text_tokenized   \n",
       "0                                               [1920s]  \\\n",
       "1                    [sull, ##y, pr, ##ud, ##hom, ##me]   \n",
       "2     [discourse, between, two, or, more, people, ho...   \n",
       "3                              [se, ##jong, the, great]   \n",
       "4     [grasshoppers, are, plant, -, eat, ##ers, ,, w...   \n",
       "...                                                 ...   \n",
       "7384                                                 []   \n",
       "7385                                                 []   \n",
       "7386                                                 []   \n",
       "7387                                                 []   \n",
       "7388                                                 []   \n",
       "\n",
       "                           document_plaintext_tokenized  start_index   \n",
       "0     [quantum, field, theory, naturally, began, wit...           31  \\\n",
       "1     [the, nobel, prize, in, literature, (, swedish...          120   \n",
       "2     [dialect, ##ic, or, dialect, ##ics, (, greek, ...           33   \n",
       "3     [hangul, was, personally, created, and, promu,...           18   \n",
       "4     [grasshoppers, are, plant, -, eat, ##ers, ,, w...            0   \n",
       "...                                                 ...          ...   \n",
       "7384  [the, medley, relay, was, scheduled, in, the, ...           -1   \n",
       "7385  [sam, ##kh, ##ya, is, a, dual, ##ist, philosop...           -1   \n",
       "7386  [moll, ##o, was, surprise, ##d, by, the, succe...           -1   \n",
       "7387  [in, the, end, ,, president, truman, made, the...           -1   \n",
       "7388  [the, previous, mayor, ,, bill, la, ##for, ##e...           -1   \n",
       "\n",
       "                                               iob_tags  \n",
       "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4     [B, I, I, I, I, I, I, I, I, I, I, I, I, I, I, ...  \n",
       "...                                                 ...  \n",
       "7384  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "7385  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "7386  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "7387  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "7388  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[7389 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983911"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_train_iob_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9770893912152624"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-((sum(english_train_iob_labels == 'B')+ sum(english_train_iob_labels == 'I'))/len(english_train_iob_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENGLISH - Logistic Regression\n",
      "Accuracy: 0.974860459262988\n",
      "Precision: 0.950352915034444\n",
      "Recall: 0.974860459262988\n",
      "F1: 0.9624506993158521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# define parameters\n",
    "\n",
    "model_english = LogisticRegression()\n",
    "model_english.fit(seq_embed_train_english, english_train_iob_labels)\n",
    "\n",
    "# English\n",
    "y_pred_english = model_english.predict(seq_embed_val_english)\n",
    "print()\n",
    "print(\"ENGLISH - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(english_val_iob_labels, y_pred_english))\n",
    "print(\"Precision:\", precision_score(english_val_iob_labels, y_pred_english, average='weighted'))\n",
    "print(\"Recall:\", recall_score(english_val_iob_labels, y_pred_english, average='weighted'))\n",
    "print(\"F1:\", f1_score(english_val_iob_labels, y_pred_english, average='weighted'))\n",
    "\n",
    "# model_bengali = LogisticRegression()\n",
    "# model_bengali.fit(seq_embed_train_bengali, bengali_train_labels)\n",
    "\n",
    "# # Bengali\n",
    "# y_pred_bengali = model_bengali.predict(seq_embed_val_bengali)\n",
    "# print()\n",
    "# print(\"BENGALI - Logistic Regression\")\n",
    "# print(\"Accuracy:\", accuracy_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"Precision:\", precision_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"Recall:\", recall_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"F1:\", f1_score(bengali_val_labels, y_pred_bengali))\n",
    "\n",
    "# model_arabic = LogisticRegression()\n",
    "# model_arabic.fit(seq_embed_train_arabic, arabic_train_labels)\n",
    "\n",
    "# # Arabic\n",
    "# y_pred_arabic = model_arabic.predict(seq_embed_val_arabic)\n",
    "# print()\n",
    "# print(\"ARABIC - Logistic Regression\")\n",
    "# print(\"Accuracy:\", accuracy_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Precision:\", precision_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Recall:\", recall_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"F1:\", f1_score(arabic_val_labels, y_pred_arabic))\n",
    "\n",
    "# model_indonesian = LogisticRegression()\n",
    "# model_indonesian.fit(seq_embed_train_indonesian, indonesian_train_labels)\n",
    "\n",
    "# # Indonesian\n",
    "# y_pred_indonesian = model_indonesian.predict(seq_embed_val_indonesian)\n",
    "# print()\n",
    "# print(\"INDONESIAN - Logistic Regression\")\n",
    "# print(\"Accuracy:\", accuracy_score(indonesian_val_labels, y_pred_indonesian))\n",
    "# print(\"Precision:\", precision_score(indonesian_val_labels, y_pred_indonesian))\n",
    "# print(\"Recall:\", recall_score(indonesian_val_labels, y_pred_indonesian))\n",
    "# print(\"F1:\", f1_score(indonesian_val_labels, y_pred_indonesian))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['O'], dtype='<U1'), array(['O'], dtype='<U1'), array(['O'], dtype='<U1'), array(['O'], dtype='<U1'), array(['O'], dtype='<U1')]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a list of tokens\n",
    "token_list = seq_embed_val_english\n",
    "\n",
    "# Create an empty list to store the classifications\n",
    "classifications = []\n",
    "\n",
    "# Loop through each token and classify it using your scikit-learn model\n",
    "for token in token_list:\n",
    "    # Perform classification using your model (replace with your actual model)\n",
    "    classification = model_english.predict([token])  # Assuming your_model is trained and can classify a single token\n",
    "    classifications.append(classification)\n",
    "\n",
    "# Now, 'classifications' contains the classification results for each token\n",
    "print(classifications[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:879: RuntimeWarning: invalid value encountered in divide\n",
      "  proba /= len(self.estimators_)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_english\u001b[39m.\u001b[39;49mpredict(seq_embed_val_english[\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    820\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_\u001b[39m.\u001b[39;49mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    825\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m     n_samples \u001b[39m=\u001b[39m proba[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "model_english.predict(seq_embed_val_english[5].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENGLISH - Random Forest\n",
      "Accuracy: 0.9748456538797506\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mENGLISH - Random Forest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m\"\u001b[39m, accuracy_score(english_val_iob_labels, y_pred_english))\n\u001b[0;32m---> 51\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPrecision:\u001b[39m\u001b[39m\"\u001b[39m, precision_score(english_val_iob_labels, y_pred_english))\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRecall:\u001b[39m\u001b[39m\"\u001b[39m, recall_score(english_val_iob_labels, y_pred_english))\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mF1:\u001b[39m\u001b[39m\"\u001b[39m, f1_score(english_val_iob_labels, y_pred_english))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[1;32m   1826\u001b[0m     y_true,\n\u001b[1;32m   1827\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1834\u001b[0m ):\n\u001b[1;32m   1835\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \n\u001b[1;32m   1837\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1954\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1955\u001b[0m         y_true,\n\u001b[1;32m   1956\u001b[0m         y_pred,\n\u001b[1;32m   1957\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1958\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[1;32m   1959\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[1;32m   1960\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[1;32m   1961\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1962\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[1;32m   1963\u001b[0m     )\n\u001b[1;32m   1964\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[1;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1391\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1390\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1392\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1393\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[1;32m   1394\u001b[0m         )\n\u001b[1;32m   1395\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[1;32m   1396\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1397\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   1402\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "\n",
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "# model_indonesian = RandomForestClassifier()\n",
    "# model_bengali = RandomForestClassifier()\n",
    "# model_arabic = RandomForestClassifier()\n",
    "model_english = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_english.fit(seq_embed_train_english, english_train_iob_labels)\n",
    "# model_indonesian.fit(seq_embed_train_indonesian, indonesian_train_labels)\n",
    "# model_bengali.fit(seq_embed_train_bengali, bengali_train_labels)\n",
    "# model_arabic.fit(seq_embed_train_arabic, arabic_train_labels)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "# # Indonesian\n",
    "# y_pred_indonesian = model_indonesian.predict(seq_embed_val_indonesian)\n",
    "# print()\n",
    "# print(\"INDONESIAN - Random Forest\")\n",
    "# print(\"Accuracy:\", accuracy_score(indonesian_val_labels, y_pred_indonesian))\n",
    "# print(\"Precision:\", precision_score(indonesian_val_labels, y_pred_indonesian))\n",
    "# print(\"Recall:\", recall_score(indonesian_val_labels, y_pred_indonesian))\n",
    "# print(\"F1:\", f1_score(indonesian_val_labels, y_pred_indonesian))\n",
    "\n",
    "# # Bengali\n",
    "# y_pred_bengali = model_bengali.predict(seq_embed_val_bengali)\n",
    "# print()\n",
    "# print(\"BENGALI - Random Forest\")\n",
    "# print(\"Accuracy:\", accuracy_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"Precision:\", precision_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"Recall:\", recall_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"F1:\", f1_score(bengali_val_labels, y_pred_bengali))\n",
    "\n",
    "\n",
    "# # Arabic\n",
    "# y_pred_arabic = model_arabic.predict(seq_embed_val_arabic)\n",
    "# print()\n",
    "# print(\"ARABIC - Random Forest\")\n",
    "# print(\"Accuracy:\", accuracy_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Precision:\", precision_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Recall:\", recall_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"F1:\", f1_score(arabic_val_labels, y_pred_arabic))\n",
    "\n",
    "# English\n",
    "y_pred_english = model_english.predict(seq_embed_val_english)\n",
    "print()\n",
    "print(\"ENGLISH - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(english_val_iob_labels, y_pred_english))\n",
    "print(\"Precision:\", precision_score(english_val_iob_labels, y_pred_english))\n",
    "print(\"Recall:\", recall_score(english_val_iob_labels, y_pred_english))\n",
    "print(\"F1:\", f1_score(english_val_iob_labels, y_pred_english))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Create a tqdm progress bar to track the processing\n",
    "with tqdm(total=len(ner_results)) as pbar:\n",
    "    for result in ner_results:\n",
    "        # Process each NER result here if needed\n",
    "        print(result)\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version using the larger model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "nlp_large = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in df applying nlp to document_plaintext_tokenized\n",
    "def ner(df):\n",
    "    df['ner'] = df['document_plaintext'].apply(lambda x: nlp(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in df applying nlp to document_plaintext_tokenized\n",
    "def ner_large(df):\n",
    "    df['ner_large'] = df['document_plaintext'].apply(lambda x: nlp_large(x))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Week 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move from binary classification to span-based QA, i.e., identifying the span in the document that answers the question, when it is answerable.\n",
    "Let k be the number of members in your group. Using the training data, implement k different sequence labellers for each of the three languages, which predict which tokens in a document are part of the answer to the correspond- ing question. Evaluate the sequence labellers on the respective validation sets, report and analyse the performance for each language and compare the scores across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bpemb\n",
    "# !pip install gensim\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install fasttext\n",
    "# !pip install datasets\n",
    "# !pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import torch\n",
    "import random\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
    "from typing import List, Tuple, AnyStr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import heapq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/emmastoklundlee/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c0d1c7cd2f419eb09637db6b81264c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116067\n",
      "13325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language   \n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish  \\\n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations   \n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...  \\\n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext   \n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...  \\\n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
    "\n",
    "\n",
    "def answer_text(df):\n",
    "    # create new column with 1 if answerable, 0 if not answerable\n",
    "    df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
    "    # drop all rows with answerable = 0\n",
    "    # df = df[df['answerable'] == 1]\n",
    "    # return answer_text from annotations\n",
    "    df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
    "    # create new column with answer_start converted to int\n",
    "    df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load spaCy model (you can choose a different model if needed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "def label(df):\n",
    "    # Initialize labels with zeros for each document token for each document in the dataframe\n",
    "    df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
    "\n",
    "    # Tokenize the answer text\n",
    "    df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
    "\n",
    "    # Tokenize and process the document plaintext\n",
    "    df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
    "\n",
    "    # Find the starting index of answer_text in document_plaintext for each document\n",
    "    df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
    "\n",
    "    # Mark the corresponding tokens in document_plaintext_tokenized with 1\n",
    "    df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
    "    df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1017 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2628830032.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df['document_plaintext'].apply(lambda x: [0] * len(mbert_tokeniser.tokenize(x)))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text_tokenized'] = df['answer_text'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['document_plaintext_tokenized'] = df['document_plaintext'].apply(mbert_tokeniser.tokenize)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['start_index'] = df.apply(lambda x: x['document_plaintext_tokenized'].index(x['answer_text_tokenized'][0]) if x['answer_text_tokenized'] and x['answer_text_tokenized'][0] in x['document_plaintext_tokenized'] else -1, axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['labels'] = df.apply(lambda x: [1 if i >= x['start_index'] and i < x['start_index'] + len(x['answer_text_tokenized']) else 0 for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_2426/2198016652.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['iob_tags'] = df.apply(lambda x: ['O' if i < x['start_index'] or i >= x['start_index'] + len(x['answer_text_tokenized']) else 'B' if i == x['start_index'] else 'I' for i in range(len(x['document_plaintext_tokenized']))], axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_train_english = label(answer_text(df_train_english))\n",
    "df_val_english = label(answer_text(df_val_english))\n",
    "df_train_bengali = label(answer_text(df_train_bengali))\n",
    "df_val_bengali = label(answer_text(df_val_bengali))\n",
    "df_train_arabic = label(answer_text(df_train_arabic))\n",
    "df_val_arabic = label(answer_text(df_val_arabic))\n",
    "df_train_indonesian = label(answer_text(df_train_indonesian))\n",
    "df_val_indonesian = label(answer_text(df_val_indonesian))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from bpemb import BPEmb\n",
    "\n",
    "# import numpy as np\n",
    "# from bpemb import BPEmb\n",
    "\n",
    "# def load_pretrained_embeddings(lang_code, dim=100, vs=25000):\n",
    "#     # Initialize BPEmb for the specified language\n",
    "#     bpemb_lang = BPEmb(lang=lang_code, dim=dim, vs=vs)\n",
    "\n",
    "#     # Extract the embeddings\n",
    "#     pretrained_embeddings = bpemb_lang.emb.vectors\n",
    "\n",
    "#     # Define the [PAD] token embedding as all zeros\n",
    "#     pad_embedding = np.zeros(shape=(1, dim))\n",
    "\n",
    "#     # Concatenate the embeddings with the [PAD] token\n",
    "#     pretrained_embeddings_with_pad = np.concatenate([pretrained_embeddings, pad_embedding], axis=0)\n",
    "\n",
    "#     # Extract the vocab and add an extra [PAD] token\n",
    "#     vocabulary = bpemb_lang.emb.index_to_key + ['[PAD]']\n",
    "\n",
    "#     # Create a dictionary from the embeddings\n",
    "#     embedding_dict = {token: embedding for token, embedding in zip(vocabulary, pretrained_embeddings_with_pad)}\n",
    "\n",
    "#     # Ensure that the shape of pretrained_embeddings_with_pad is correct\n",
    "#     print(pretrained_embeddings_with_pad.shape)\n",
    "\n",
    "#     return embedding_dict\n",
    "\n",
    "# # Example usage:\n",
    "# # Load pretrained embeddings for Indonesian\n",
    "# indonesian_embeddings = load_pretrained_embeddings(lang_code='id')\n",
    "\n",
    "# # Load pretrained embeddings for Arabic\n",
    "# arabic_embeddings = load_pretrained_embeddings(lang_code='ar')\n",
    "\n",
    "# # Load pretrained embeddings for Bengali\n",
    "# bengali_embeddings = load_pretrained_embeddings(lang_code='bn')\n",
    "\n",
    "# # Load pretrained embeddings for English\n",
    "# english_embeddings = load_pretrained_embeddings(lang_code='en')\n",
    "\n",
    "\n",
    "\n",
    "# # Define a function to tokenize and embed text\n",
    "# def embed_text(df, embedding_dict):\n",
    "#     tokenized_text_list = []\n",
    "\n",
    "#     for document_text in df['document_plaintext_tokenized']:\n",
    "#         # Tokenize the document text\n",
    "#         tokens = [token for token in document_text]\n",
    "\n",
    "#         # Map tokens to embeddings using the dictionary\n",
    "#         token_embeddings = [embedding_dict.get(token, embedding_dict['[PAD]']) for token in tokens]\n",
    "\n",
    "#         # Append the token embeddings to the list\n",
    "#         tokenized_text_list.extend(token_embeddings)\n",
    "\n",
    "#     # Return the sequence embeddings as a NumPy array\n",
    "#     sequence_embedding = np.array(tokenized_text_list)\n",
    "#     return sequence_embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# # Load the pre-trained BERT model and tokenizer\n",
    "# model_name = \"bert-base-multilingual-uncased\"  # You can choose a different BERT variant\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# def bert_embeddings(df):\n",
    "#     # Assuming you have a DataFrame 'df' with a column 'document_plaintext_tokenized'\n",
    "\n",
    "#     # Tokenize the text in your DataFrame\n",
    "#     tokenized_texts = [tokenizer.encode(doc, add_special_tokens=True) for doc in df['document_plaintext_tokenized']]\n",
    "\n",
    "#     # Truncate or pad the sequences to a fixed length\n",
    "#     max_seq_length = 128  # Set your desired maximum sequence length\n",
    "#     truncated_padded_tokenized_texts = []\n",
    "#     for tokens in tokenized_texts:\n",
    "#         if len(tokens) > max_seq_length:\n",
    "#             tokens = tokens[:max_seq_length]  # Truncate if longer than max_seq_length\n",
    "#         else:\n",
    "#             padding_length = max_seq_length - len(tokens)\n",
    "#             tokens = tokens + [tokenizer.pad_token_id] * padding_length\n",
    "#         truncated_padded_tokenized_texts.append(tokens)\n",
    "\n",
    "#     # Convert tokenized texts to tensors\n",
    "#     input_ids = torch.tensor(truncated_padded_tokenized_texts)\n",
    "\n",
    "#     # Generate BERT embeddings\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids)\n",
    "#         embeddings = outputs.last_hidden_state  # Use the embeddings from the last layer\n",
    "\n",
    "#     # The 'embeddings' variable now contains the BERT embeddings for your tokenized text\n",
    "#     return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Tokenize the text\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Indonesian\n",
    "X_train_indonesian_tokenized = vectorizer.fit_transform(df_train_indonesian['document_plaintext'].ravel())\n",
    "X_val_tokenized_indonesian = vectorizer.transform(df_val_indonesian['document_plaintext'].ravel())\n",
    "Y_train_indonesian = df_train_indonesian['iob_tags'].ravel()\n",
    "Y_val_indonesian = df_val_indonesian['iob_tags'].ravel()\n",
    "# X_val_tokenized_indonesian = vectorizer.transform(X_val_indonesian.ravel())\n",
    "\n",
    "# # Bengali\n",
    "# X_train_bengali_tokenized = vectorizer.fit_transform(X_train_bengali.ravel())\n",
    "# X_val_tokenized_bengali = vectorizer.transform(X_val_bengali.ravel())\n",
    "\n",
    "# # Arabic\n",
    "# X_train_arabic_tokenized = vectorizer.fit_transform(X_train_arabic.ravel())\n",
    "# X_val_tokenized_arabic = vectorizer.transform(X_val_arabic.ravel())\n",
    "\n",
    "# # English\n",
    "# X_train_english_tokenized = vectorizer.fit_transform(X_train_english.ravel())\n",
    "# X_val_tokenized_english = vectorizer.transform(X_val_english.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding of Os to the end of each list in Y_train_indonesian and Y_val_indonesian\n",
    "max_length = X_train_indonesian_tokenized.shape[1]\n",
    "Y_train_indonesian = [label + [\"O\"] * (max_length - len(label)) for label in Y_train_indonesian]\n",
    "Y_val_indonesian = [label + [\"O\"] * (max_length - len(label)) for label in Y_val_indonesian]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'unknown'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Indonesian\u001b[39;00m\n\u001b[1;32m      6\u001b[0m rf_indonesian \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m rf_indonesian\u001b[39m.\u001b[39;49mfit(X_train_indonesian_tokenized, Y_train_indonesian)\n\u001b[1;32m      8\u001b[0m Y_pred_indonesian \u001b[39m=\u001b[39m rf_indonesian\u001b[39m.\u001b[39mpredict(X_val_tokenized_indonesian)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:385\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    379\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSum of y is not strictly positive which \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis necessary for Poisson regression.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         )\n\u001b[1;32m    383\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 385\u001b[0m y, expanded_class_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_y_class_weight(y)\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(y, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m!=\u001b[39m DOUBLE \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m y\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mcontiguous:\n\u001b[1;32m    388\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mascontiguousarray(y, dtype\u001b[39m=\u001b[39mDOUBLE)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:746\u001b[0m, in \u001b[0;36mForestClassifier._validate_y_class_weight\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_y_class_weight\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[0;32m--> 746\u001b[0m     check_classification_targets(y)\n\u001b[1;32m    748\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(y)\n\u001b[1;32m    749\u001b[0m     expanded_class_weight \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/utils/multiclass.py:218\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    210\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\n\u001b[1;32m    212\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    213\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmultilabel-sequences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m ]:\n\u001b[0;32m--> 218\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown label type: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'unknown'"
     ]
    }
   ],
   "source": [
    "# fit random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Indonesian\n",
    "rf_indonesian = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_indonesian.fit(X_train_indonesian_tokenized, Y_train_indonesian)\n",
    "Y_pred_indonesian = rf_indonesian.predict(X_val_tokenized_indonesian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_embed_train_english = tfidf_embeddings(df_train_english)\n",
    "# seq_embed_val_english = tfidf_embeddings(df_val_english)\n",
    "# seq_embed_train_bengali = tfidf_embeddings(df_train_bengali)\n",
    "# seq_embed_val_bengali = tfidf_embeddings(df_val_bengali)\n",
    "# seq_embed_train_arabic = tfidf_embeddings(df_train_arabic)\n",
    "# seq_embed_val_arabic = tfidf_embeddings(df_val_arabic)\n",
    "# seq_embed_train_indonesian = tfidf_embeddings(df_train_indonesian)\n",
    "# seq_embed_val_indonesian = tfidf_embeddings(df_val_indonesian)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the maximum length of TF-IDF vectors\n",
    "# max_tfidf_length = seq_embed_train_english.shape[1]\n",
    "\n",
    "# # Add padding (\"O\" tags) to the end of each IOB tag sequence to match the maximum length\n",
    "# padded_iob_tags = [tags + [\"O\"] * (max_tfidf_length - len(tags)) for tags in english_train_iob_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50046"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_iob_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7389, 50046)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_embed_train_english.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex dataframe\n",
    "df_train_english = df_train_english.reset_index(drop=True)\n",
    "df_val_english = df_val_english.reset_index(drop=True)\n",
    "df_train_bengali = df_train_bengali.reset_index(drop=True)\n",
    "df_val_bengali = df_val_bengali.reset_index(drop=True)\n",
    "df_train_arabic = df_train_arabic.reset_index(drop=True)\n",
    "df_val_arabic = df_val_arabic.reset_index(drop=True)\n",
    "df_train_indonesian = df_train_indonesian.reset_index(drop=True)\n",
    "df_val_indonesian = df_val_indonesian.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one long list of the lists in df_train_english['labels']\n",
    "\n",
    "def get_labels(df):\n",
    "    labels = []\n",
    "    for i in range(len(df)):\n",
    "        labels.append(df['labels'][i])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iob_labels(df):\n",
    "    labels = []\n",
    "    for i in range(len(df)):\n",
    "        labels.append(df['iob_tags'][i])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_labels = get_labels(df_train_english)\n",
    "english_val_labels = get_labels(df_val_english)\n",
    "bengali_train_labels = get_labels(df_train_bengali)\n",
    "bengali_val_labels = get_labels(df_val_bengali)\n",
    "arabic_train_labels = get_labels(df_train_arabic)\n",
    "arabic_val_labels = get_labels(df_val_arabic)\n",
    "indonesian_train_labels = get_labels(df_train_indonesian)\n",
    "indonesian_val_labels = get_labels(df_val_indonesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_train_iob_labels = get_iob_labels(df_train_english)\n",
    "english_val_iob_labels = get_iob_labels(df_val_english)\n",
    "bengali_train_iob_labels = get_iob_labels(df_train_bengali)\n",
    "bengali_val_iob_labels = get_iob_labels(df_val_bengali)\n",
    "arabic_train_iob_labels = get_iob_labels(df_train_arabic)\n",
    "arabic_val_iob_labels = get_iob_labels(df_val_arabic)\n",
    "indonesian_train_iob_labels = get_iob_labels(df_train_indonesian)\n",
    "indonesian_val_iob_labels = get_iob_labels(df_val_indonesian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7389, 983911]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m english_train_iob_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(english_train_iob_labels)\n\u001b[1;32m     18\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model_english\u001b[39m.\u001b[39;49mfit(seq_embed_train_english_dense, english_train_iob_labels)\n\u001b[1;32m     21\u001b[0m \u001b[39m# Now, you should be able to use the fitted model\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1196\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[0;32m-> 1196\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1197\u001b[0m     X,\n\u001b[1;32m   1198\u001b[0m     y,\n\u001b[1;32m   1199\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1200\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[1;32m   1201\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1202\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1203\u001b[0m )\n\u001b[1;32m   1204\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7389, 983911]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Convert the CSR matrix to a dense NumPy array\n",
    "seq_embed_train_english_dense = seq_embed_train_english.toarray()\n",
    "\n",
    "# Define the model\n",
    "model_english = LogisticRegression()\n",
    "\n",
    "# Convert the list of labels to a NumPy array\n",
    "# Flatten the list of lists into a 1D list\n",
    "english_train_iob_labels = [label for sublist in english_train_iob_labels for label in sublist]\n",
    "\n",
    "# Now, convert it to a NumPy array\n",
    "english_train_iob_labels = np.array(english_train_iob_labels)\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "model_english.fit(seq_embed_train_english_dense, english_train_iob_labels)\n",
    "\n",
    "# Now, you should be able to use the fitted model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7389, 50046)\n",
      "7389\n"
     ]
    }
   ],
   "source": [
    "print(seq_embed_train_english.shape)\n",
    "print(len(padded_iob_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(seq_embed_train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_indonesian_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# define parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m# model_english = LogisticRegression()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m# print(\"Recall:\", recall_score(arabic_val_labels, y_pred_arabic))\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m# print(\"F1:\", f1_score(arabic_val_labels, y_pred_arabic))\u001b[39;00m\n\u001b[1;32m     42\u001b[0m model_indonesian \u001b[39m=\u001b[39m LogisticRegression()\n\u001b[0;32m---> 43\u001b[0m model_indonesian\u001b[39m.\u001b[39mfit(X_train_indonesian_tokenized, Y_train_indonesian)\n\u001b[1;32m     45\u001b[0m \u001b[39m# Indonesian\u001b[39;00m\n\u001b[1;32m     46\u001b[0m y_pred_indonesian \u001b[39m=\u001b[39m model_indonesian\u001b[39m.\u001b[39mpredict(X_val_tokenized_indonesian)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_indonesian_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# define parameters\n",
    "\n",
    "# model_english = LogisticRegression()\n",
    "# model_english.fit(seq_embed_train_english, padded_iob_tags)\n",
    "\n",
    "# # English\n",
    "# y_pred_english = model_english.predict(seq_embed_val_english)\n",
    "# print()\n",
    "# print(\"ENGLISH - Logistic Regression\")\n",
    "# print(\"Accuracy:\", accuracy_score(padded_iob_tags, y_pred_english))\n",
    "# print(\"Precision:\", precision_score(padded_iob_tags, y_pred_english, average='weighted'))\n",
    "# print(\"Recall:\", recall_score(padded_iob_tags, y_pred_english, average='weighted'))\n",
    "# print(\"F1:\", f1_score(padded_iob_tags, y_pred_english, average='weighted'))\n",
    "\n",
    "# model_bengali = LogisticRegression()\n",
    "# model_bengali.fit(seq_embed_train_bengali, bengali_train_iob_labels)\n",
    "\n",
    "# # Bengali\n",
    "# y_pred_bengali = model_bengali.predict(seq_embed_val_bengali)\n",
    "# print()\n",
    "# print(\"BENGALI - Logistic Regression\")\n",
    "# print(\"Accuracy:\", accuracy_score(bengali_val_iob_labels, y_pred_bengali))\n",
    "# print(\"Precision:\", precision_score(bengali_val_iob_labels, y_pred_bengali, average='weighted'))\n",
    "# print(\"Recall:\", recall_score(bengali_val_iob_labels, y_pred_bengali, average='weighted'))\n",
    "# print(\"F1:\", f1_score(bengali_val_iob_labels, y_pred_bengali, average='weighted'))\n",
    "\n",
    "# model_arabic = LogisticRegression()\n",
    "# model_arabic.fit(seq_embed_train_arabic, arabic_train_labels)\n",
    "\n",
    "# # Arabic\n",
    "# y_pred_arabic = model_arabic.predict(seq_embed_val_arabic)\n",
    "# print()\n",
    "# print(\"ARABIC - Logistic Regression\")\n",
    "# print(\"Accuracy:\", accuracy_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Precision:\", precision_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Recall:\", recall_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"F1:\", f1_score(arabic_val_labels, y_pred_arabic))\n",
    "\n",
    "model_indonesian = LogisticRegression()\n",
    "model_indonesian.fit(X_train_indonesian_tokenized, Y_train_indonesian)\n",
    "\n",
    "# Indonesian\n",
    "y_pred_indonesian = model_indonesian.predict(X_val_tokenized_indonesian)\n",
    "print()\n",
    "print(\"INDONESIAN - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(Y_val_indonesian, y_pred_indonesian, average='weighted'))\n",
    "print(\"Precision:\", precision_score(Y_val_indonesian, y_pred_indonesian, average='weighted'))\n",
    "print(\"Recall:\", recall_score(Y_val_indonesian, y_pred_indonesian, average='weighted'))\n",
    "print(\"F1:\", f1_score(Y_val_indonesian, y_pred_indonesian, average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['O'], dtype='<U1'), array(['O'], dtype='<U1'), array(['O'], dtype='<U1'), array(['O'], dtype='<U1'), array(['O'], dtype='<U1')]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a list of tokens\n",
    "token_list = seq_embed_val_english\n",
    "\n",
    "# Create an empty list to store the classifications\n",
    "classifications = []\n",
    "\n",
    "# Loop through each token and classify it using your scikit-learn model\n",
    "for token in token_list:\n",
    "    # Perform classification using your model (replace with your actual model)\n",
    "    classification = model_english.predict([token])  # Assuming your_model is trained and can classify a single token\n",
    "    classifications.append(classification)\n",
    "\n",
    "# Now, 'classifications' contains the classification results for each token\n",
    "print(classifications[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:879: RuntimeWarning: invalid value encountered in divide\n",
      "  proba /= len(self.estimators_)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'take'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_english\u001b[39m.\u001b[39;49mpredict(seq_embed_val_english[\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    820\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_\u001b[39m.\u001b[39;49mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    825\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m     n_samples \u001b[39m=\u001b[39m proba[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'take'"
     ]
    }
   ],
   "source": [
    "model_english.predict(seq_embed_val_english[5].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "\n",
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "# model_indonesian = RandomForestClassifier()\n",
    "# model_bengali = RandomForestClassifier()\n",
    "# model_arabic = RandomForestClassifier()\n",
    "# model_english = RandomForestClassifier()\n",
    "\n",
    "# # Fit the model to the data\n",
    "# model_english.fit(seq_embed_train_english, english_train_iob_labels)\n",
    "# model_indonesian.fit(seq_embed_train_indonesian, indonesian_train_labels)\n",
    "# model_bengali.fit(seq_embed_train_bengali, bengali_train_labels)\n",
    "# model_arabic.fit(seq_embed_train_arabic, arabic_train_labels)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "# # Indonesian\n",
    "model_indonesian = RandomForestClassifier()\n",
    "model_indonesian.fit(X_train_indonesian_tokenized, Y_train_indonesian)\n",
    "\n",
    "# Indonesian\n",
    "y_pred_indonesian = model_indonesian.predict(X_val_tokenized_indonesian)\n",
    "print()\n",
    "print(\"INDONESIAN - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(Y_val_indonesian, y_pred_indonesian))\n",
    "print(\"Precision:\", precision_score(Y_val_indonesian, y_pred_indonesian))\n",
    "print(\"Recall:\", recall_score(Y_val_indonesian, y_pred_indonesian))\n",
    "print(\"F1:\", f1_score(Y_val_indonesian, y_pred_indonesian))\n",
    "\n",
    "# # Bengali\n",
    "# y_pred_bengali = model_bengali.predict(seq_embed_val_bengali)\n",
    "# print()\n",
    "# print(\"BENGALI - Random Forest\")\n",
    "# print(\"Accuracy:\", accuracy_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"Precision:\", precision_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"Recall:\", recall_score(bengali_val_labels, y_pred_bengali))\n",
    "# print(\"F1:\", f1_score(bengali_val_labels, y_pred_bengali))\n",
    "\n",
    "\n",
    "# # Arabic\n",
    "# y_pred_arabic = model_arabic.predict(seq_embed_val_arabic)\n",
    "# print()\n",
    "# print(\"ARABIC - Random Forest\")\n",
    "# print(\"Accuracy:\", accuracy_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Precision:\", precision_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"Recall:\", recall_score(arabic_val_labels, y_pred_arabic))\n",
    "# print(\"F1:\", f1_score(arabic_val_labels, y_pred_arabic))\n",
    "\n",
    "# English\n",
    "# y_pred_english = model_english.predict(seq_embed_val_english)\n",
    "# print()\n",
    "# print(\"ENGLISH - Random Forest\")\n",
    "# print(\"Accuracy:\", accuracy_score(english_val_iob_labels, y_pred_english))\n",
    "# print(\"Precision:\", precision_score(english_val_iob_labels, y_pred_english))\n",
    "# print(\"Recall:\", recall_score(english_val_iob_labels, y_pred_english))\n",
    "# print(\"F1:\", f1_score(english_val_iob_labels, y_pred_english))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding of Os to the end of each list in Y_train_indonesian and Y_val_indonesian\n",
    "max_length = X_train_indonesian_tokenized.shape[1]\n",
    "Y_train_indonesian = [label + [\"O\"] * (max_length - len(label)) for label in Y_train_indonesian]\n",
    "Y_val_indonesian = [label + [\"O\"] * (max_length - len(label)) for label in Y_val_indonesian]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Create a tqdm progress bar to track the processing\n",
    "with tqdm(total=len(ner_results)) as pbar:\n",
    "    for result in ner_results:\n",
    "        # Process each NER result here if needed\n",
    "        print(result)\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version using the larger model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-large-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-large-NER\")\n",
    "\n",
    "nlp_large = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in df applying nlp to document_plaintext_tokenized\n",
    "def ner(df):\n",
    "    df['ner'] = df['document_plaintext'].apply(lambda x: nlp(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in df applying nlp to document_plaintext_tokenized\n",
    "def ner_large(df):\n",
    "    df['ner_large'] = df['document_plaintext'].apply(lambda x: nlp_large(x))\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

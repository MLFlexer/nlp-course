{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Week 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let k be the number of members in your group. Implement and train k different supervised classifiers for each of the three languages separately, using the training data for that language. The classifiers must only use the document and question as input. Evaluate the classifiers on the respective validation sets, report and analyse the performance for each language and compare the scores across languages.\n",
    "\n",
    "The classifiers can use linguistic/lexical features, e.g., bag-of-words, n-gram counts, overlaps of words between question and document, etc.; word embed- dings, or word/sentence representations from neural language models. You can, for example, find pretrained Transformer language models for different languages, trained with different language objectives, and fine-tuned for differ- entdownstreamtasks,fromHuggingFace.9 Youcanalsotrainorfine-tuneyour own neural language models on the dataset. Motivate your choice of features and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bpemb\n",
    "!pip install gensim\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116067\n",
      "13325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language  \\\n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish   \n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext  \\\n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>answerable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Wound care encourages and speeds wound healing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Brothers Amos and Wilfrid Ayre founded Burntis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>For species of mammals, larger brains (in abso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>As from 31 March 1989, fishing vessel registra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>When Quezon City was created in 1939, the foll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  answerable\n",
       "30   Wound care encourages and speeds wound healing...           1\n",
       "47   Brothers Amos and Wilfrid Ayre founded Burntis...           1\n",
       "59   For species of mammals, larger brains (in abso...           1\n",
       "77   As from 31 March 1989, fishing vessel registra...           1\n",
       "106  When Quezon City was created in 1939, the foll...           1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with the combined documents and questions and add if they are answerable\n",
    "df_train_bengali_merged = pd.DataFrame({\n",
    "    'text':(df_train_bengali[\"document_plaintext\"] + df_train_bengali[\"question_text\"]),\n",
    "    'answerable':(df_train_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_train_arabic_merged = pd.DataFrame({\n",
    "    'text': (df_train_arabic[\"document_plaintext\"] + df_train_arabic[\"question_text\"]),\n",
    "    'answerable': (df_train_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "                                    })\n",
    "df_train_indonesian_merged = pd.DataFrame({\n",
    "    'text':(df_train_indonesian[\"document_plaintext\"] + df_train_indonesian[\"question_text\"]),\n",
    "    'answerable':(df_train_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_train_english_merged = pd.DataFrame({\n",
    "    'text':(df_train_english[\"document_plaintext\"] + df_train_english[\"question_text\"]),\n",
    "    'answerable':(df_train_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "\n",
    "\n",
    "## Same for validation data\n",
    "df_val_bengali_merged = pd.DataFrame({\n",
    "    'text':(df_val_bengali[\"document_plaintext\"] + df_val_bengali[\"question_text\"]),\n",
    "    'answerable':(df_val_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_val_arabic_merged = pd.DataFrame({\n",
    "    'text': (df_val_arabic[\"document_plaintext\"] + df_val_arabic[\"question_text\"]),\n",
    "    'answerable': (df_val_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "                                    })\n",
    "df_val_indonesian_merged = pd.DataFrame({\n",
    "    'text':(df_val_indonesian[\"document_plaintext\"] + df_val_indonesian[\"question_text\"]),\n",
    "    'answerable':(df_val_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_val_english_merged = pd.DataFrame({\n",
    "    'text':(df_val_english[\"document_plaintext\"] + df_val_english[\"question_text\"]),\n",
    "    'answerable':(df_val_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "\n",
    "df_val_english_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/private/Desktop/nlp-course/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/private/Desktop/nlp-course/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/private/Desktop/nlp-course/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INDONESIAN - Logistic Regression\n",
      "Accuracy: 0.7380352644836272\n",
      "Precision: 0.7766990291262136\n",
      "Recall: 0.6700167504187605\n",
      "F1: 0.7194244604316546\n",
      "\n",
      "BENGALI - Logistic Regression\n",
      "Accuracy: 0.7098214285714286\n",
      "Precision: 0.688\n",
      "Recall: 0.7678571428571429\n",
      "F1: 0.7257383966244725\n",
      "\n",
      "ARABIC - Logistic Regression\n",
      "Accuracy: 0.7923238696109358\n",
      "Precision: 0.8054945054945055\n",
      "Recall: 0.7707676130389064\n",
      "F1: 0.7877485222998388\n",
      "\n",
      "ENGLISH - Logistic Regression\n",
      "Accuracy: 0.7121212121212122\n",
      "Precision: 0.7160493827160493\n",
      "Recall: 0.703030303030303\n",
      "F1: 0.7094801223241589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/private/Desktop/nlp-course/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "## Train the model on the Bengali training data\n",
    "# select the input and label columns\n",
    "# Indonesian\n",
    "X_train_indonesian = df_train_indonesian_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_indonesian = df_train_indonesian_merged.iloc[:, 1].values\n",
    "\n",
    "# Bengali\n",
    "X_train_bengali = df_train_bengali_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_bengali = df_train_bengali_merged.iloc[:, 1].values\n",
    "\n",
    "#Arabic\n",
    "X_train_arabic = df_train_arabic_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_arabic = df_train_arabic_merged.iloc[:, 1].values\n",
    "\n",
    "# English\n",
    "X_train_english = df_train_english_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_english = df_train_english_merged.iloc[:, 1].values\n",
    "\n",
    "# Validation data\n",
    "# Indonesian\n",
    "X_val_indonesian = df_val_indonesian_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_indosnesian = df_val_indonesian_merged.iloc[:, 1].values\n",
    "\n",
    "# Bengali\n",
    "X_val_bengali = df_val_bengali_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_bengali = df_val_bengali_merged.iloc[:, 1].values\n",
    "\n",
    "#Arabic\n",
    "X_val_arabic = df_val_arabic_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_arabic = df_val_arabic_merged.iloc[:, 1].values\n",
    "\n",
    "# English\n",
    "X_val_english = df_val_english_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_english = df_val_english_merged.iloc[:, 1].values\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Indonesian\n",
    "X_train_indonesian_tokenized = vectorizer.fit_transform(X_train_indonesian.ravel())\n",
    "X_val_tokenized_indonesian = vectorizer.transform(X_val_indonesian.ravel())\n",
    "\n",
    "# Bengali\n",
    "X_train_bengali_tokenized = vectorizer.fit_transform(X_train_bengali.ravel())\n",
    "X_val_tokenized_bengali = vectorizer.transform(X_val_bengali.ravel())\n",
    "\n",
    "# Arabic\n",
    "X_train_arabic_tokenized = vectorizer.fit_transform(X_train_arabic.ravel())\n",
    "X_val_tokenized_arabic = vectorizer.transform(X_val_arabic.ravel())\n",
    "\n",
    "# English\n",
    "X_train_english_tokenized = vectorizer.fit_transform(X_train_english.ravel())\n",
    "X_val_tokenized_english = vectorizer.transform(X_val_english.ravel())\n",
    "\n",
    "# Create a logistic regression model\n",
    "model_indonesian = LogisticRegression()\n",
    "model_bengali = LogisticRegression()\n",
    "model_arabic = LogisticRegression()\n",
    "model_english = LogisticRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_indonesian.fit(X_train_indonesian_tokenized, y_train_indonesian)\n",
    "model_bengali.fit(X_train_bengali_tokenized, y_train_bengali)\n",
    "model_arabic.fit(X_train_arabic_tokenized, y_train_arabic)\n",
    "model_english.fit(X_train_english_tokenized, y_train_english)\n",
    "\n",
    "\n",
    "## Test the model on the validation data\n",
    "\n",
    "# Indonesian\n",
    "y_pred_indonesian = model_indonesian.predict(X_val_tokenized_indonesian)\n",
    "print()\n",
    "print(\"INDONESIAN - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Precision:\", precision_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Recall:\", recall_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"F1:\", f1_score(y_val_indosnesian, y_pred_indonesian))\n",
    "\n",
    "# Bengali\n",
    "y_pred_bengali = model_bengali.predict(X_val_tokenized_bengali)\n",
    "print()\n",
    "print(\"BENGALI - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Precision:\", precision_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Recall:\", recall_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"F1:\", f1_score(y_val_bengali, y_pred_bengali))\n",
    "\n",
    "# Arabic\n",
    "y_pred_arabic = model_arabic.predict(X_val_tokenized_arabic)\n",
    "print()\n",
    "print(\"ARABIC - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Precision:\", precision_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Recall:\", recall_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"F1:\", f1_score(y_val_arabic, y_pred_arabic))\n",
    "\n",
    "# English\n",
    "y_pred_english = model_english.predict(X_val_tokenized_english)\n",
    "print()\n",
    "print(\"ENGLISH - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_english, y_pred_english))\n",
    "print(\"Precision:\", precision_score(y_val_english, y_pred_english))\n",
    "print(\"Recall:\", recall_score(y_val_english, y_pred_english))\n",
    "print(\"F1:\", f1_score(y_val_english, y_pred_english))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INDONESIAN - Random Forest\n",
      "Accuracy: 0.7674223341729639\n",
      "Precision: 0.7973977695167286\n",
      "Recall: 0.7185929648241206\n",
      "F1: 0.7559471365638766\n",
      "\n",
      "BENGALI - Random Forest\n",
      "Accuracy: 0.6785714285714286\n",
      "Precision: 0.6754385964912281\n",
      "Recall: 0.6875\n",
      "F1: 0.6814159292035398\n",
      "\n",
      "ARABIC - Random Forest\n",
      "Accuracy: 0.7765509989484752\n",
      "Precision: 0.8002283105022832\n",
      "Recall: 0.7371188222923238\n",
      "F1: 0.7673782156540777\n",
      "\n",
      "ENGLISH - Random Forest\n",
      "Accuracy: 0.7242424242424242\n",
      "Precision: 0.7211155378486056\n",
      "Recall: 0.7313131313131314\n",
      "F1: 0.7261785356068206\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "model_indonesian = RandomForestClassifier()\n",
    "model_bengali = RandomForestClassifier()\n",
    "model_arabic = RandomForestClassifier()\n",
    "model_english = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_indonesian.fit(X_train_indonesian_tokenized, y_train_indonesian)\n",
    "model_bengali.fit(X_train_bengali_tokenized, y_train_bengali)\n",
    "model_arabic.fit(X_train_arabic_tokenized, y_train_arabic)\n",
    "model_english.fit(X_train_english_tokenized, y_train_english)\n",
    "\n",
    "# Evaluate the model\n",
    "# Indonesian\n",
    "y_pred_indonesian = model_indonesian.predict(X_val_tokenized_indonesian)\n",
    "print()\n",
    "print(\"INDONESIAN - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Precision:\", precision_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Recall:\", recall_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"F1:\", f1_score(y_val_indosnesian, y_pred_indonesian))\n",
    "\n",
    "# Bengali\n",
    "y_pred_bengali = model_bengali.predict(X_val_tokenized_bengali)\n",
    "print()\n",
    "print(\"BENGALI - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Precision:\", precision_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Recall:\", recall_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"F1:\", f1_score(y_val_bengali, y_pred_bengali))\n",
    "\n",
    "# Arabic\n",
    "y_pred_arabic = model_arabic.predict(X_val_tokenized_arabic)\n",
    "print()\n",
    "print(\"ARABIC - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Precision:\", precision_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Recall:\", recall_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"F1:\", f1_score(y_val_arabic, y_pred_arabic))\n",
    "\n",
    "# English\n",
    "y_pred_english = model_english.predict(X_val_tokenized_english)\n",
    "print()\n",
    "print(\"ENGLISH - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_english, y_pred_english))\n",
    "print(\"Precision:\", precision_score(y_val_english, y_pred_english))\n",
    "print(\"Recall:\", recall_score(y_val_english, y_pred_english))\n",
    "print(\"F1:\", f1_score(y_val_english, y_pred_english))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on word counts vs GloVe embeddings and BPEMB embeddings (remember for report)\n",
    "Question: Just as a note, you can actually get much better performance using simple word counts -- why do you think this is?\n",
    "\n",
    "Possible answer:\n",
    "The reason simple word counts can sometimes outperform more complex models like GloVe or BPEmb embeddings in certain tasks is due to the nature of the data and the task itself.\n",
    "\n",
    "**In some tasks, the presence or absence of specific words can be highly indicative of the class or category. For example, in sentiment analysis, words like 'good', 'awesome', 'bad', 'terrible' etc. can be strong indicators of the sentiment. A simple word count vectorizer captures this information effectively.**\n",
    "\n",
    "On the other hand, word embeddings like GloVe or BPEmb capture semantic and syntactic relationships between words, which can be very useful for tasks that require understanding of context or when dealing with words not present in the training set. However, these embeddings might introduce noise for tasks that can be solved based on simple word occurrence statistics.\n",
    "\n",
    "In summary, the effectiveness of a method depends on the specific task and the nature of the data. It's always a good idea to start with simpler methods and then move to more complex ones if necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is taken from lab_2.ipynb and modified to fit the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data into a model\n",
    "\n",
    "A simple and common way that data is read in PyTorch is to use the two following classes: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "The `Dataset` class can be extended to read in and store the data you are using for your experiment. The only requirements are to implement the `__len__` and `__getitem__` methods. `__len__` simply returns the size of your dataset and `__getitem__` takes an index and returns that sample from your dataset, processed in whatever way is necessary to be input to your model.\n",
    "\n",
    "The `DataLoader` class determines how to iterate through your `Dataset`, including how to shuffle and batch your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_batch_bilstm(text: List, tokenizer, max_len=512) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Creates a tokenized batch for input to a bilstm model\n",
    "    :param text: A list of sentences to tokenize\n",
    "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
    "    :return: Tokenized text as well as the length of the input sequence\n",
    "    \"\"\"\n",
    "    # Some light preprocessing\n",
    "    input_ids = [tokenizer.encode_ids_with_eos(t)[:max_len] for t in text]\n",
    "\n",
    "    return input_ids, [len(ids) for ids in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    input_ids = [i[0][0] for i in input_data]\n",
    "    seq_lens = [i[1][0] for i in input_data]\n",
    "    labels = [i[2] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    # Pad all of the input samples to the max length (25000 is the ID of the [PAD] token)\n",
    "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
    "\n",
    "    # Make sure each sample is max_length long\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will load the dataset and process it lazily in the __getitem__ function\n",
    "class ClassificationDatasetReader(Dataset):\n",
    "  def __init__(self, df, tokenizer):\n",
    "    self.df = df\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.df.values[idx]\n",
    "    # Calls the text_to_batch function\n",
    "    input_ids,seq_lens = text_to_batch_bilstm([row[0]], self.tokenizer)\n",
    "    label = row[1]\n",
    "    return input_ids, seq_lens, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "# Load english model with 25k word-pieces\n",
    "bpemb_id= BPEmb(lang='id', dim=100, vs=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings and add a randomly initialized embedding for our extra [PAD] token\n",
    "pretrained_embeddings = np.concatenate([bpemb_id.emb.vectors, np.zeros(shape=(1,100))], axis=0)\n",
    "# Extract the vocab and add an extra [PAD] token\n",
    "vocabulary = bpemb_id.emb.index_to_key + ['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   59,\n",
       "   0,\n",
       "   24795,\n",
       "   24764,\n",
       "   24955,\n",
       "   24835,\n",
       "   24837,\n",
       "   24913,\n",
       "   24835,\n",
       "   24846,\n",
       "   24764,\n",
       "   24965,\n",
       "   24826,\n",
       "   15738,\n",
       "   24764,\n",
       "   0,\n",
       "   24791,\n",
       "   0,\n",
       "   24791,\n",
       "   0,\n",
       "   24795,\n",
       "   24764,\n",
       "   0,\n",
       "   24767,\n",
       "   0,\n",
       "   24776,\n",
       "   0,\n",
       "   24770,\n",
       "   24764,\n",
       "   24967,\n",
       "   24763,\n",
       "   0,\n",
       "   24784,\n",
       "   8606,\n",
       "   187,\n",
       "   0,\n",
       "   24780,\n",
       "   0,\n",
       "   24801,\n",
       "   0,\n",
       "   1081,\n",
       "   690,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24784,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   59,\n",
       "   0,\n",
       "   24795,\n",
       "   19359,\n",
       "   0,\n",
       "   24882,\n",
       "   0,\n",
       "   24835,\n",
       "   24837,\n",
       "   2286,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   59,\n",
       "   24841,\n",
       "   24858,\n",
       "   0,\n",
       "   16176,\n",
       "   24831,\n",
       "   24837,\n",
       "   24913,\n",
       "   24831,\n",
       "   24860,\n",
       "   24764,\n",
       "   24896,\n",
       "   24860,\n",
       "   24896,\n",
       "   24846,\n",
       "   24802,\n",
       "   62,\n",
       "   241,\n",
       "   690,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24780,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   24764,\n",
       "   0,\n",
       "   12923,\n",
       "   2]],\n",
       " [309],\n",
       " 1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = ClassificationDatasetReader(df_train_bengali_merged, bpemb_id)\n",
    "reader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "Next we will create a BiLSTM model with BPE word-piece embeddings. In this case we will extend the PyTorch class `torch.nn.Module`. To create your own module, you need only define your model architecture in the `__init__` function, and define how tensors are processed by your model in the `__forward__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Define a default lstm_dim\n",
    "lstm_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class BiLSTMNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic BiLSTM network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic BiLSTM network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
    "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(BiLSTMNetwork, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'bilstm': nn.LSTM(\n",
    "                pretrained_embeddings.shape[1],\n",
    "                lstm_dim,\n",
    "                1,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'cls': nn.Linear(2*lstm_dim, n_classes)\n",
    "        })\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
    "                     list(self.model['cls'].named_parameters())\n",
    "        for n,p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
    "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # Max pool along the last dimension\n",
    "        ff_in = self.dropout(torch.max(lstm_out, 1)[0])\n",
    "        # Some magic to get the last output of the BiLSTM for classification (b x 2*lstm_dim)\n",
    "        #ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
    "\n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.model['cls'](ff_in).view(-1, self.n_classes)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Xentropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"cuda available\")\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = BiLSTMNetwork(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings),\n",
    "    lstm_dim=lstm_dim,\n",
    "    dropout_prob=0.1,\n",
    "    n_classes=2\n",
    "  ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "  logits = np.asarray(logits).reshape(-1, len(logits[0]))\n",
    "  labels = np.asarray(labels).reshape(-1)\n",
    "  return np.sum(np.argmax(logits, axis=-1) == labels).astype(np.float32) / float(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
    "  \"\"\"\n",
    "  Evaluates the model on the given dataset\n",
    "  :param model: The model under evaluation\n",
    "  :param valid_dl: A `DataLoader` reading validation data\n",
    "  :return: The accuracy of the model on the dataset\n",
    "  \"\"\"\n",
    "  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n",
    "  # layer normalization and dropout\n",
    "  model.eval()\n",
    "  labels_all = []\n",
    "  logits_all = []\n",
    "\n",
    "  # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      seq_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      _, logits = model(input_ids, seq_lens, labels=labels)\n",
    "      labels_all.extend(list(labels.detach().cpu().numpy()))\n",
    "      logits_all.extend(list(logits.detach().cpu().numpy()))\n",
    "    acc = accuracy(logits_all, labels_all)\n",
    "\n",
    "    return acc,labels_all,logits_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dl: DataLoader,\n",
    "    valid_dl: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    n_epochs: int,\n",
    "    device: torch.device,\n",
    "    patience: int = 10\n",
    "):\n",
    "  \"\"\"\n",
    "  The main training loop which will optimize a given model on a given dataset\n",
    "  :param model: The model being optimized\n",
    "  :param train_dl: The training dataset\n",
    "  :param valid_dl: A validation dataset\n",
    "  :param optimizer: The optimizer used to update the model parameters\n",
    "  :param n_epochs: Number of epochs to train for\n",
    "  :param device: The device to train on\n",
    "  :return: (model, losses) The best model and the losses per iteration\n",
    "  \"\"\"\n",
    "\n",
    "  # Keep track of the loss and best accuracy\n",
    "  losses = []\n",
    "  best_acc = 0.0\n",
    "  pcounter = 0\n",
    "\n",
    "  # Iterate through epochs\n",
    "  for ep in range(n_epochs):\n",
    "\n",
    "    loss_epoch = []\n",
    "\n",
    "    #Iterate through each batch in the dataloader\n",
    "    for batch in tqdm(train_dl):\n",
    "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n",
    "      # things like dropout and layer normalization\n",
    "      model.train()\n",
    "\n",
    "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
    "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
    "      # zero them out\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Place each tensor on the GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      seq_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      # Pass the inputs through the model, get the current loss and logits\n",
    "      loss, logits = model(input_ids, seq_lens, labels=labels)\n",
    "      losses.append(loss.item())\n",
    "      loss_epoch.append(loss.item())\n",
    "\n",
    "      # Calculate all of the gradients and weight updates for the model\n",
    "      loss.backward()\n",
    "\n",
    "      # Optional: clip gradients\n",
    "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      # Finally, update the weights of the model\n",
    "      optimizer.step()\n",
    "      #gc.collect()\n",
    "\n",
    "    # Perform inline evaluation at the end of the epoch\n",
    "    acc,_,_ = evaluate(model, valid_dl)\n",
    "    print(f'Validation accuracy: {acc}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
    "\n",
    "    # Keep track of the best model based on the accuracy\n",
    "    if acc > best_acc:\n",
    "      torch.save(model.state_dict(), 'best_model')\n",
    "      best_acc = acc\n",
    "      pcounter = 0\n",
    "    else:\n",
    "      pcounter += 1\n",
    "      if pcounter == patience:\n",
    "        break\n",
    "        #gc.collect()\n",
    "\n",
    "  model.load_state_dict(torch.load('best_model'))\n",
    "  return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyperparameters\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_79486/3271786820.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_dl):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53168a8c8f0544068ad520d5383bf595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_79486/1187767117.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(valid_dl, desc='Evaluation'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062b8c2d93cb495ea23be468f846b0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7053571428571429, train loss: 0.6428279948234558\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9414768234c4189a38c0ea3eb0c0e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a866b11462584c26975150e92d99bbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7098214285714286, train loss: 0.5990146553516388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c10b3c23314d33b938e6b7f8571a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e21995362744ad976450995297e947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7008928571428571, train loss: 0.5851875883340836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc93aed2bc3646abbb23bbc1f1c6f81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4f7a451038436baa9060e996289bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7098214285714286, train loss: 0.5742191980282466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce1313615574b1fa1224618524d305c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ee6430d8104a68a4f5471de81ced3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7008928571428571, train loss: 0.569000807205836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110392e9e9cc412eba75adfefad969c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c1483dc9bc48e2af78d94cc3860543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6964285714285714, train loss: 0.5626910463968913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a02f7e11b4b45c0bf1a4d85c3919e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4f47f0f0fd4640819ae00c014ed70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6830357142857143, train loss: 0.562864805261294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f2c361478441aebfe093d3040d7012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d792ca7d134295b4159863a45a660e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.71875, train loss: 0.5586183309555054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a60c8562ea74d2494fca4bd0a8b8df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the dataset readers\n",
    "train_dataset = ClassificationDatasetReader(df_train_bengali_merged[:5000], bpemb_id)\n",
    "# dataset loaded lazily with N workers in parallel\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm)\n",
    "\n",
    "valid_dataset = ClassificationDatasetReader(df_val_bengali_merged[:1000], bpemb_id)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=len(df_val_bengali_merged[:1000]), collate_fn=collate_batch_bilstm)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyORsGOJW2Z5ZFtw56gBQm56","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"26d4aa7cf7284d44beccaffc11cb051c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67897069762c40658ced65a742355869","IPY_MODEL_13ecca4ce65b48fb9288073930d0fd45","IPY_MODEL_2fc457ef6897481482750337181d8a98"],"layout":"IPY_MODEL_82553d004cad48e69feefaace5bd5944"}},"67897069762c40658ced65a742355869":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a994b43feab49b29bef35159151c868","placeholder":"​","style":"IPY_MODEL_ff3a59810f56463ea86f075032454ce8","value":"Map: 100%"}},"13ecca4ce65b48fb9288073930d0fd45":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d9b7c9dad2542d1be687f4bb4143ad6","max":11394,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f070dd90187d40b182bb7d3260acbe78","value":11394}},"2fc457ef6897481482750337181d8a98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_498e82b151cc40848c8ae1f528bdb4db","placeholder":"​","style":"IPY_MODEL_82c025e832744f87acfb987639e0e875","value":" 11394/11394 [00:18&lt;00:00, 742.98 examples/s]"}},"82553d004cad48e69feefaace5bd5944":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a994b43feab49b29bef35159151c868":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff3a59810f56463ea86f075032454ce8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d9b7c9dad2542d1be687f4bb4143ad6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f070dd90187d40b182bb7d3260acbe78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"498e82b151cc40848c8ae1f528bdb4db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82c025e832744f87acfb987639e0e875":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff373df92b834036ad963a91955bbd1b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_955f869efde2433188550c262be29fa6","IPY_MODEL_217e97a5a69c41f8be85c6123affeb03","IPY_MODEL_ed08cd57502444c9abb8d49259d2f18b"],"layout":"IPY_MODEL_8d10165cde2942d1ba9a2c512d1aad14"}},"955f869efde2433188550c262be29fa6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b824e331c524d8a89d0b8ac67bc78fa","placeholder":"​","style":"IPY_MODEL_95a3a1b8b32843f19ab7ad0b6b62b683","value":"Map: 100%"}},"217e97a5a69c41f8be85c6123affeb03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10448e87ef3d4dde898f230ff819a659","max":1191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8787200ae0ab42d2a280649406dccd3d","value":1191}},"ed08cd57502444c9abb8d49259d2f18b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0d1a10c9c944dbd8c0d7866efd1c588","placeholder":"​","style":"IPY_MODEL_cc7b263869274af79be78e140481a476","value":" 1191/1191 [00:01&lt;00:00, 1147.95 examples/s]"}},"8d10165cde2942d1ba9a2c512d1aad14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b824e331c524d8a89d0b8ac67bc78fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95a3a1b8b32843f19ab7ad0b6b62b683":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10448e87ef3d4dde898f230ff819a659":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8787200ae0ab42d2a280649406dccd3d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b0d1a10c9c944dbd8c0d7866efd1c588":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc7b263869274af79be78e140481a476":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/MLFlexer/nlp-course/blob/malthe/assignments/w4_bert_indo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"Made with the help of this guide: https://huggingface.co/docs/transformers/tasks/question_answering and lab 6","metadata":{"id":"ZTpCe2R1L_yd"}},{"cell_type":"code","source":"!pip install update transformers\n!pip install datasets\n!pip install evaluate","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mAF--cPLf5y","outputId":"e7836240-0678-45c4-da92-a2e03df66b9e","execution":{"iopub.status.busy":"2023-10-27T07:46:04.056809Z","iopub.execute_input":"2023-10-27T07:46:04.057728Z","iopub.status.idle":"2023-10-27T07:46:39.266059Z","shell.execute_reply.started":"2023-10-27T07:46:04.057691Z","shell.execute_reply":"2023-10-27T07:46:39.264901Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: update in /opt/conda/lib/python3.10/site-packages (0.0.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: style==1.1.0 in /opt/conda/lib/python3.10/site-packages (from update) (1.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import load_metric\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForQuestionAnswering\nfrom transformers import AutoConfig\nfrom functools import partial\nimport torch\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch import nn\nfrom collections import defaultdict, OrderedDict\n# MODEL_NAME = 'xlm-roberta-base'\nMODEL_NAME = 'bert-base-multilingual-uncased'\nNUM_SUBSAMPLES = 11394\n#bengali: 4779\n#Arabic: 29598\n#Indonesian: 11394\nLANGUAGE = \"indonesian\" # \"bengali\" \"arabic\"","metadata":{"id":"c4d_FN4UqwhT","execution":{"iopub.status.busy":"2023-10-27T07:46:39.268676Z","iopub.execute_input":"2023-10-27T07:46:39.269666Z","iopub.status.idle":"2023-10-27T07:46:39.277046Z","shell.execute_reply.started":"2023-10-27T07:46:39.269620Z","shell.execute_reply":"2023-10-27T07:46:39.276078Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def enforce_reproducibility(seed=42):\n    # Sets seed manually for both CPU and CUDA\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # For atomic operations there is currently\n    # no simple way to enforce determinism, as\n    # the order of parallel operations is not known.\n    # CUDNN\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # System based\n    random.seed(seed)\n    np.random.seed(seed)\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n  device = torch.device(\"cuda\")\n\nenforce_reproducibility()","metadata":{"id":"kC8A3_ytq43o","execution":{"iopub.status.busy":"2023-10-27T07:46:39.278310Z","iopub.execute_input":"2023-10-27T07:46:39.278575Z","iopub.status.idle":"2023-10-27T07:46:39.293240Z","shell.execute_reply.started":"2023-10-27T07:46:39.278552Z","shell.execute_reply":"2023-10-27T07:46:39.292434Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"copenlu/answerable_tydiqa\")\n\nfiltered_dataset = dataset.filter(lambda entry: entry[\"language\"] in [LANGUAGE])#, \"arabic\", \"bengali\"])\n#filtered_dataset = filtered_dataset.filter(lambda entry: entry[\"annotations\"][\"answer_start\"][0] == -1)\n\ntrain_set = filtered_dataset[\"train\"]\nvalidation_set = filtered_dataset[\"validation\"]","metadata":{"id":"XB3XXwrpLZLp","execution":{"iopub.status.busy":"2023-10-27T07:46:39.295288Z","iopub.execute_input":"2023-10-27T07:46:39.295590Z","iopub.status.idle":"2023-10-27T07:46:40.017486Z","shell.execute_reply.started":"2023-10-27T07:46:39.295547Z","shell.execute_reply":"2023-10-27T07:46:40.016585Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63546f27277e4ab9b17bf731ae71ad93"}},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{"id":"Ee9oqmJSRwWT"}},{"cell_type":"code","source":"no_answers = validation_set[\"annotations\"].apply(lambda row: len(row[\"answer_text\"][0]) == 0)\nlen(no_answers)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T07:32:57.932949Z","iopub.execute_input":"2023-10-27T07:32:57.933311Z","iopub.status.idle":"2023-10-27T07:32:57.992691Z","shell.execute_reply.started":"2023-10-27T07:32:57.933282Z","shell.execute_reply":"2023-10-27T07:32:57.991553Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m no_answers \u001b[38;5;241m=\u001b[39m \u001b[43mvalidation_set\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28mlen\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mlen\u001b[39m(no_answers)\n","\u001b[0;31mNameError\u001b[0m: name 'validation_set' is not defined"],"ename":"NameError","evalue":"name 'validation_set' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\ntrain_set_df = train_set.to_pandas()\ntrain_set_df['id'] = range(len(train_set_df))\nvalidation_set_df = validation_set.to_pandas()\nvalidation_set_df['id'] = range(len(validation_set_df))\n\ntrain_set = Dataset.from_pandas(train_set_df)\nvalidation_set = Dataset.from_pandas(validation_set_df)","metadata":{"id":"Q2Hu9T0qh0IL","execution":{"iopub.status.busy":"2023-10-27T07:46:40.018793Z","iopub.execute_input":"2023-10-27T07:46:40.019144Z","iopub.status.idle":"2023-10-27T07:46:40.385343Z","shell.execute_reply.started":"2023-10-27T07:46:40.019108Z","shell.execute_reply":"2023-10-27T07:46:40.384516Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print(len(validation_set))\ntrain_set[2]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zeTDJL35LvwO","outputId":"749262f2-79dc-4f22-a274-5334794b9898","execution":{"iopub.status.busy":"2023-10-27T07:39:16.461208Z","iopub.execute_input":"2023-10-27T07:39:16.461502Z","iopub.status.idle":"2023-10-27T07:39:16.469048Z","shell.execute_reply.started":"2023-10-27T07:39:16.461477Z","shell.execute_reply":"2023-10-27T07:39:16.468072Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"1191\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'question_text': 'Kapan PBB mulai terbentuk ?',\n 'document_title': 'Perserikatan Bangsa-Bangsa',\n 'language': 'indonesian',\n 'annotations': {'answer_start': [360], 'answer_text': ['24 Oktober 1945']},\n 'document_plaintext': 'Sebagai tindak lanjut Atlantic Charter tersebut, pada tanggal 25 April 1945, Konferensi PBB tentang Organisasi Internasional diadakan di San Francisco, dengan dihadiri oleh 50 pemerintah negara, dan sejumlah organisasi non-pemerintah yang terlibat dalam penyusunan Piagam Perserikatan Bangsa-Bangsa (Declaration of the United Nations). PBB resmi dibentuk pada 24 Oktober 1945 atas ratifikasi Piagam oleh lima anggota tetap Dewan Keamanan -Perancis, Republik Tiongkok, Uni Soviet, Inggris dan Amerika Serikat- dan mayoritas dari 46 negara anggota lainnya.',\n 'document_url': 'https://id.wikipedia.org/wiki/Perserikatan%20Bangsa-Bangsa',\n 'id': 2}"},"metadata":{}}]},{"cell_type":"code","source":"tk = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"id":"PXY8QobAruAB","execution":{"iopub.status.busy":"2023-10-27T07:46:40.386430Z","iopub.execute_input":"2023-10-27T07:46:40.386715Z","iopub.status.idle":"2023-10-27T07:46:40.643149Z","shell.execute_reply.started":"2023-10-27T07:46:40.386691Z","shell.execute_reply":"2023-10-27T07:46:40.642347Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def get_train_features(tk, samples):\n  '''\n  Tokenizes all of the text in the given samples, splittling inputs that are too long for our model\n  across multiple features. Finds the token offsets of the answers, which serve as the labels for\n  our inputs.\n  '''\n  batch = tk.batch_encode_plus(\n        [[q,c] for q,c in zip(samples['question_text'], samples['document_plaintext'])],\n        padding='max_length',\n        truncation='only_second',\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True\n    )\n\n  # Get a list which maps the input features index to their original index in the\n  # samples list (for split inputs). E.g. if our batch size is 4 and the second sample\n  # is split into 3 inputs because it is very large, sample_mapping would look like\n  # [0, 1, 1, 1, 2, 3]\n  sample_mapping = batch.pop('overflow_to_sample_mapping')\n  # Get all of the character offsets for each token\n  offset_mapping = batch.pop('offset_mapping')\n\n  # Store the start and end tokens\n  batch['start_tokens'] = []\n  batch['end_tokens'] = []\n\n  # Iterate through all of the offsets\n  for i, offsets in enumerate(offset_mapping):\n    # Get the right sample by mapping it to its original index\n    sample_idx = sample_mapping[i]\n    # Get the sequence IDs to know where context starts so we can ignore question tokens\n    sequence_ids = batch.sequence_ids(i)\n\n    # Get the start and end character positions of the answer\n    ans = samples['annotations'][sample_idx]\n    start_char = ans['answer_start'][0]\n    end_char = start_char + len(ans['answer_text'][0])\n    # while end_char > 0 and (end_char >= len(samples['context'][sample_idx]) or samples['context'][sample_idx][end_char] == ' '):\n    #   end_char -= 1\n\n    # Start from the first token in the context, which can be found by going to the\n    # first token where sequence_ids is 1\n    start_token = 0\n    while sequence_ids[start_token] != 1:\n      start_token += 1\n\n    end_token = len(offsets) - 1\n    while sequence_ids[end_token] != 1:\n      end_token -= 1\n\n    # By default set it to the CLS token if the answer isn't in this input\n    if start_char < offsets[start_token][0] or end_char > offsets[end_token][1]:\n      start_token = 0\n      end_token = 0\n    # Otherwise find the correct token indices\n    else:\n      # Advance the start token index until we have passed the start character index\n      while start_token < len(offsets) and offsets[start_token][0] <= start_char:\n        start_token += 1\n      start_token -= 1\n\n      # Decrease the end token index until we have passed the end character index\n      while end_token >= 0 and offsets[end_token][1] >= end_char:\n        end_token -= 1\n      end_token += 1\n\n    batch['start_tokens'].append(start_token)\n    batch['end_tokens'].append(end_token)\n\n  #batch['start_tokens'] = np.array(batch['start_tokens'])\n  #batch['end_tokens'] = np.array(batch['end_tokens'])\n\n  return batch\n\ndef collate_fn(inputs):\n  '''\n  Defines how to combine different samples in a batch\n  '''\n  input_ids = torch.tensor([i['input_ids'] for i in inputs])\n  attention_mask = torch.tensor([i['attention_mask'] for i in inputs])\n  start_tokens = torch.tensor([i['start_tokens'] for i in inputs])\n  end_tokens = torch.tensor([i['end_tokens'] for i in inputs])\n\n  # Truncate to max length\n  max_len = max(attention_mask.sum(-1))\n  input_ids = input_ids[:,:max_len]\n  attention_mask = attention_mask[:,:max_len]\n\n  return {'input_ids': input_ids, 'attention_mask': attention_mask, 'start_tokens': start_tokens, 'end_tokens': end_tokens}","metadata":{"id":"lO4rSfT3saW4","execution":{"iopub.status.busy":"2023-10-27T07:46:40.644541Z","iopub.execute_input":"2023-10-27T07:46:40.644901Z","iopub.status.idle":"2023-10-27T07:46:40.659693Z","shell.execute_reply.started":"2023-10-27T07:46:40.644865Z","shell.execute_reply":"2023-10-27T07:46:40.658708Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = train_set.map(partial(get_train_features, tk), batched=True, remove_columns=train_set.column_names)","metadata":{"id":"8dBau9r7seZU","outputId":"45994b4f-0a79-4395-f152-1c2e29f7f137","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["26d4aa7cf7284d44beccaffc11cb051c","67897069762c40658ced65a742355869","13ecca4ce65b48fb9288073930d0fd45","2fc457ef6897481482750337181d8a98","82553d004cad48e69feefaace5bd5944","1a994b43feab49b29bef35159151c868","ff3a59810f56463ea86f075032454ce8","8d9b7c9dad2542d1be687f4bb4143ad6","f070dd90187d40b182bb7d3260acbe78","498e82b151cc40848c8ae1f528bdb4db","82c025e832744f87acfb987639e0e875"]},"execution":{"iopub.status.busy":"2023-10-27T07:46:40.660850Z","iopub.execute_input":"2023-10-27T07:46:40.661125Z","iopub.status.idle":"2023-10-27T07:46:49.131989Z","shell.execute_reply.started":"2023-10-27T07:46:40.661101Z","shell.execute_reply":"2023-10-27T07:46:49.131107Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be3840a9c4904041b2a384cc9e3361d2"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"id":"w66oxEb9tBdt","outputId":"5d3d90cb-52aa-4fb2-a540-fddb56a24110","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-27T07:46:49.133193Z","iopub.execute_input":"2023-10-27T07:46:49.133534Z","iopub.status.idle":"2023-10-27T07:46:49.139400Z","shell.execute_reply.started":"2023-10-27T07:46:49.133504Z","shell.execute_reply":"2023-10-27T07:46:49.138601Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_tokens', 'end_tokens'],\n    num_rows: 11594\n})"},"metadata":{}}]},{"cell_type":"code","source":"samples = random.sample(list(range(len(tokenized_dataset))), NUM_SUBSAMPLES)\ntokenized_dataset = tokenized_dataset.select(samples)\ntrain_dl = DataLoader(tokenized_dataset, collate_fn=collate_fn, shuffle=True, batch_size=4)","metadata":{"id":"ExRWEMevtE8z","execution":{"iopub.status.busy":"2023-10-27T07:39:52.900067Z","iopub.execute_input":"2023-10-27T07:39:52.900409Z","iopub.status.idle":"2023-10-27T07:39:52.964438Z","shell.execute_reply.started":"2023-10-27T07:39:52.900384Z","shell.execute_reply":"2023-10-27T07:39:52.963700Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import gc","metadata":{"id":"Fuszid5REYXG","execution":{"iopub.status.busy":"2023-10-26T15:10:26.478144Z","iopub.execute_input":"2023-10-26T15:10:26.478420Z","iopub.status.idle":"2023-10-26T15:10:26.482428Z","shell.execute_reply.started":"2023-10-26T15:10:26.478398Z","shell.execute_reply":"2023-10-26T15:10:26.481530Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train(\n    model: nn.Module,\n    train_dl: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    schedule: LambdaLR,\n    n_epochs: int,\n    device: torch.device\n):\n  \"\"\"\n  The main training loop which will optimize a given model on a given dataset\n  :param model: The model being optimized\n  :param train_dl: The training dataset\n  :param optimizer: The optimizer used to update the model parameters\n  :param n_epochs: Number of epochs to train for\n  :param device: The device to train on\n  \"\"\"\n\n  # Keep track of the loss and best accuracy\n  losses = []\n  best_acc = 0.0\n  pcounter = 0\n\n  # Iterate through epochs\n  for ep in range(n_epochs):\n\n    loss_epoch = []\n\n    #Iterate through each batch in the dataloader\n    for batch in tqdm(train_dl):\n      # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n      # things like dropout and layer normalization\n      model.train()\n\n      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n      # keeps track of these dynamically in its computation graph so you need to explicitly\n      # zero them out\n      optimizer.zero_grad()\n\n      # Place each tensor on the GPU\n      batch = {b: batch[b].to(device) for b in batch}\n\n      # Pass the inputs through the model, get the current loss and logits\n      outputs = model(\n          input_ids=batch['input_ids'],\n          attention_mask=batch['attention_mask'],\n          start_positions=batch['start_tokens'],\n          end_positions=batch['end_tokens']\n      )\n      loss = outputs['loss']\n      losses.append(loss.item())\n      loss_epoch.append(loss.item())\n\n      # Calculate all of the gradients and weight updates for the model\n      loss.backward()\n\n      # Optional: clip gradients\n      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n      # Finally, update the weights of the model and advance the LR schedule\n      optimizer.step()\n      scheduler.step()\n      gc.collect()\n  return losses","metadata":{"id":"L1t6iqEbtc9D","execution":{"iopub.status.busy":"2023-10-26T15:10:26.483769Z","iopub.execute_input":"2023-10-26T15:10:26.484035Z","iopub.status.idle":"2023-10-26T15:10:26.494291Z","shell.execute_reply.started":"2023-10-26T15:10:26.484013Z","shell.execute_reply":"2023-10-26T15:10:26.493583Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)","metadata":{"id":"a9oBUnR-tezT","outputId":"568b0c18-e623-4221-9e29-715ae02969e4","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-26T15:10:26.495261Z","iopub.execute_input":"2023-10-26T15:10:26.495502Z","iopub.status.idle":"2023-10-26T15:10:35.795425Z","shell.execute_reply.started":"2023-10-26T15:10:26.495481Z","shell.execute_reply":"2023-10-26T15:10:35.794356Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84034f62c064e4aa41876ca2c8f30fb"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create the optimizer\nlr=2e-5\nn_epochs = 5\nweight_decay = 0.01\nwarmup_steps = 200\n\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n      'weight_decay': weight_decay},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\n# optimizer = Adam(optimizer_grouped_parameters, lr=1e-3)\n# scheduler = None\noptimizer = AdamW(optimizer_grouped_parameters, lr=lr)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    warmup_steps,\n    n_epochs * len(train_dl)\n)","metadata":{"id":"JkQShWSQth50","outputId":"bdb49912-8bbe-405d-92f3-465174210281","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-26T15:10:35.796847Z","iopub.execute_input":"2023-10-26T15:10:35.797610Z","iopub.status.idle":"2023-10-26T15:10:35.811840Z","shell.execute_reply.started":"2023-10-26T15:10:35.797559Z","shell.execute_reply":"2023-10-26T15:10:35.810905Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"losses = train(\n    model,\n    train_dl,\n    optimizer,\n    scheduler,\n    n_epochs,\n    device\n)","metadata":{"id":"aP17eWFRtk2x","outputId":"329b2b05-2196-4bf7-8e55-923799635af8","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-26T15:10:35.813126Z","iopub.execute_input":"2023-10-26T15:10:35.813454Z","iopub.status.idle":"2023-10-26T16:36:11.360318Z","shell.execute_reply.started":"2023-10-26T15:10:35.813422Z","shell.execute_reply":"2023-10-26T16:36:11.359418Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"100%|██████████| 2849/2849 [17:04<00:00,  2.78it/s]\n100%|██████████| 2849/2849 [17:04<00:00,  2.78it/s]\n100%|██████████| 2849/2849 [17:11<00:00,  2.76it/s]\n100%|██████████| 2849/2849 [17:03<00:00,  2.78it/s]\n100%|██████████| 2849/2849 [17:11<00:00,  2.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_validation_features(tk, samples):\n  # First, tokenize the text. We get the offsets and return overflowing sequences in\n  # order to break up long sequences into multiple inputs. The offsets will help us\n  # determine the original answer text\n  batch = tk.batch_encode_plus(\n        [[q,c] for q,c in zip(samples['question_text'], samples['document_plaintext'])],\n        padding='max_length',\n        truncation='only_second',\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True\n    )\n\n  # We'll store the ID of the samples to calculate squad score\n  batch['example_id'] = []\n  # The overflow sample map tells us which input each sample corresponds to\n  sample_map = batch.pop('overflow_to_sample_mapping')\n\n  for i in range(len(batch['input_ids'])):\n    # The sample index tells us which of the values in \"samples\" these features belong to\n    sample_idx = sample_map[i]\n    sequence_ids = batch.sequence_ids(i)\n\n    # Add the ID to map these features back to the correct sample\n    batch['example_id'].append(samples['id'][sample_idx])\n\n    #Set offsets for non-context words to be None for ease of processing\n    batch['offset_mapping'][i] = [o if sequence_ids[k] == 1 else None for k,o in enumerate(batch['offset_mapping'][i])]\n\n  return batch\n\ndef val_collate_fn(inputs):\n  input_ids = torch.tensor([i['input_ids'] for i in inputs])\n  attention_mask = torch.tensor([i['attention_mask'] for i in inputs])\n\n  # Truncate to max length\n  max_len = max(attention_mask.sum(-1))\n  input_ids = input_ids[:,:max_len]\n  attention_mask = attention_mask[:,:max_len]\n\n  return {'input_ids': input_ids, 'attention_mask': attention_mask}","metadata":{"id":"Dz4_lMebtpbX","execution":{"iopub.status.busy":"2023-10-27T07:46:51.208257Z","iopub.execute_input":"2023-10-27T07:46:51.208566Z","iopub.status.idle":"2023-10-27T07:46:51.218260Z","shell.execute_reply.started":"2023-10-27T07:46:51.208539Z","shell.execute_reply":"2023-10-27T07:46:51.217350Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"validation_dataset = validation_set.map(partial(get_validation_features, tk), batched=True, remove_columns=validation_set.column_names)","metadata":{"id":"1ywVmDAstrPx","outputId":"7cb292fc-7fef-4f4e-fc58-b5831af0add8","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ff373df92b834036ad963a91955bbd1b","955f869efde2433188550c262be29fa6","217e97a5a69c41f8be85c6123affeb03","ed08cd57502444c9abb8d49259d2f18b","8d10165cde2942d1ba9a2c512d1aad14","7b824e331c524d8a89d0b8ac67bc78fa","95a3a1b8b32843f19ab7ad0b6b62b683","10448e87ef3d4dde898f230ff819a659","8787200ae0ab42d2a280649406dccd3d","b0d1a10c9c944dbd8c0d7866efd1c588","cc7b263869274af79be78e140481a476"]},"execution":{"iopub.status.busy":"2023-10-27T07:46:52.169824Z","iopub.execute_input":"2023-10-27T07:46:52.170784Z","iopub.status.idle":"2023-10-27T07:47:02.110582Z","shell.execute_reply.started":"2023-10-27T07:46:52.170735Z","shell.execute_reply":"2023-10-27T07:47:02.109718Z"},"trusted":true},"execution_count":66,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c8e4429c73c4e6d900f221b5221cb3a"}},"metadata":{}}]},{"cell_type":"code","source":"def predict(model: nn.Module, valid_dl: DataLoader):\n  \"\"\"\n  Evaluates the model on the given dataset\n  :param model: The model under evaluation\n  :param valid_dl: A `DataLoader` reading validation data\n  :return: The accuracy of the model on the dataset\n  \"\"\"\n  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n  # layer normalization and dropout\n  model.eval()\n  start_logits_all = []\n  end_logits_all = []\n\n  # ALSO IMPORTANT: Don't accumulate gradients during this process\n  with torch.no_grad():\n    for batch in tqdm(valid_dl, desc='Evaluation'):\n      batch = {b: batch[b].to(device) for b in batch}\n\n      # Pass the inputs through the model, get the current loss and logits\n      outputs = model(\n          input_ids=batch['input_ids'],\n          attention_mask=batch['attention_mask']\n      )\n      # Store the \"start\" class logits and \"end\" class logits for every token in the input\n      start_logits_all.extend(list(outputs['start_logits'].detach().cpu().numpy()))\n      end_logits_all.extend(list(outputs['end_logits'].detach().cpu().numpy()))\n\n\n    return start_logits_all,end_logits_all\n\ndef post_process_predictions(examples, dataset, logits, tokenizer, num_possible_answers = 20, max_answer_length = 30):\n  all_start_logits, all_end_logits = logits\n  # Build a map from example to its corresponding features. This will allow us to index from\n  # sample ID to all of the features for that sample (in case they were split up due to long input)\n  example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n  features_per_example = defaultdict(list)\n  for i, feature in enumerate(dataset):\n      features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n  # Create somewhere to store our predictions\n  predictions = OrderedDict()\n\n  # Iterate through each sample in the dataset\n  for j, sample in enumerate(tqdm(examples)):\n\n    # Get the feature indices (all of the features split across the batch)\n    feature_indices = features_per_example[j]\n    # Get the original context which predumably has the answer text\n    context = sample['document_plaintext']\n\n    preds = []\n\n    min_score_threshold = None\n\n    # Iterate through all of the features\n    for ft_idx in feature_indices:\n\n      # Get the start and end answer logits for this input\n      start_logits = all_start_logits[ft_idx]\n      end_logits = all_end_logits[ft_idx]\n\n      # Get the offsets to map token indices to character indices\n      offset_mapping = dataset[ft_idx]['offset_mapping']\n\n\n      # Update minimum null prediction.\n      cls_index = dataset[ft_idx][\"input_ids\"].index(tokenizer.cls_token_id)\n      feature_min_score_threshold = start_logits[cls_index] + end_logits[cls_index]\n      if min_score_threshold is None or min_score_threshold < feature_min_score_threshold:\n          min_score_threshold = feature_min_score_threshold\n\n      # Sort the logits and take the top N\n      start_indices = np.argsort(start_logits)[::-1][:num_possible_answers]\n      end_indices = np.argsort(end_logits)[::-1][:num_possible_answers]\n\n      # Iterate through start and end indices\n      for start_index in start_indices:\n        for end_index in end_indices:\n\n          # Ignore this combination if either the indices are not in the context\n          if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n            continue\n\n          # Also ignore if the start index is greater than the end index of the number of tokens\n          # is greater than some specified threshold\n          if start_index > end_index or end_index - start_index + 1 > max_answer_length:\n            continue\n          try:\n              ans_text = context[offset_mapping[start_index][0]:offset_mapping[end_index][1]]\n              preds.append({\n                  'score': start_logits[start_index] + end_logits[end_index],\n                  'text': ans_text\n              })\n          except Exception as e:\n              #print(start_index)\n              #print(offset_mapping[start_index])\n              #print(offset_mapping[1])\n              continue\n              #print(offset_mapping[start_index][0])\n\n    if len(preds) > 0:\n      # Sort by score to get the top answer\n      best_answer = sorted(preds, key=lambda x: x['score'], reverse=True)[0]\n    else:\n      best_answer = {'score': 0.0, 'text': \"\"}\n\n    # if the best answer is below the threshold for lowest score, give it the empty string\n\n    answer = best_answer[\"text\"] if best_answer[\"score\"] > min_score_threshold else \"\"\n    predictions[sample['id']] = answer\n  return predictions","metadata":{"id":"MnBTW-pVuC6w","execution":{"iopub.status.busy":"2023-10-27T07:47:02.112282Z","iopub.execute_input":"2023-10-27T07:47:02.112594Z","iopub.status.idle":"2023-10-27T07:47:02.130004Z","shell.execute_reply.started":"2023-10-27T07:47:02.112568Z","shell.execute_reply":"2023-10-27T07:47:02.129122Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"val_dl = DataLoader(validation_dataset, collate_fn=val_collate_fn, batch_size=32)\nlogits = predict(model, val_dl)","metadata":{"id":"48-vkpvnuJ1q","outputId":"9aab5790-7fb2-45da-d612-e7d06ebb893d","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-27T07:47:02.131216Z","iopub.execute_input":"2023-10-27T07:47:02.131485Z","iopub.status.idle":"2023-10-27T07:47:20.696023Z","shell.execute_reply.started":"2023-10-27T07:47:02.131461Z","shell.execute_reply":"2023-10-27T07:47:20.695089Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stderr","text":"Evaluation: 100%|██████████| 38/38 [00:18<00:00,  2.05it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_dataset","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnDrQ9zB1vt_","outputId":"b425f343-75a7-467c-87ee-014bb1f851ac","execution":{"iopub.status.busy":"2023-10-27T07:47:20.698455Z","iopub.execute_input":"2023-10-27T07:47:20.698745Z","iopub.status.idle":"2023-10-27T07:47:20.706113Z","shell.execute_reply.started":"2023-10-27T07:47:20.698719Z","shell.execute_reply":"2023-10-27T07:47:20.704967Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n    num_rows: 1210\n})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = post_process_predictions(validation_set, validation_dataset, logits, tk)\nformatted_predictions = [{'id': k, 'prediction_text': v} for k,v in predictions.items()]\ngold = [{'id': example['id'], 'answers': example['annotations'][\"answer_text\"][0]} for example in validation_set]","metadata":{"id":"lTy-WKaxuLKN","outputId":"aab51fb7-4a11-4796-c82b-391245d928a9","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-27T07:47:20.707253Z","iopub.execute_input":"2023-10-27T07:47:20.707532Z","iopub.status.idle":"2023-10-27T07:47:29.759434Z","shell.execute_reply.started":"2023-10-27T07:47:20.707501Z","shell.execute_reply":"2023-10-27T07:47:29.758657Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stderr","text":"100%|██████████| 1191/1191 [00:06<00:00, 192.24it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\" Official evaluation script for v1.1 of the SQuAD dataset. \"\"\"\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    if len(prediction_tokens) == 0 and len(ground_truth_tokens) == 0:\n      return 1\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    #print(prediction)\n    #print(ground_truth)\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    #print(metric_fn)\n    #print(prediction)\n    #print(ground_truths)\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    #if len(scores_for_ground_truths) == 0:\n    #  if len(prediction) == 0:\n    #    return 1 # FIX: skal ændres, så at hvis der ikke er noget svar og prediction også siger det, så skal den have god score\n     # else:\n     #   return 0\n    return max(scores_for_ground_truths)\n\n\ndef evaluate_squad(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        # print(article)\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                #print(qa)\n                #print(predictions[qa['id']])\n                total += 1\n                if qa['id'] not in predictions:\n                    message = 'Unanswered question ' + qa['id'] + \\\n                              ' will receive score 0.'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n                prediction = predictions[qa['id']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'exact_match': exact_match, 'f1': f1}\n\ndef compute_squad(predictions, references):\n  pred_dict = {prediction[\"id\"]: prediction[\"prediction_text\"] for prediction in predictions}\n  dataset = [\n      {\n          \"paragraphs\": [\n              {\n                  \"qas\": [\n                      {\n                          \"answers\": [{\"text\": ref[\"answers\"]} ],\n                          \"id\": ref[\"id\"],\n                      }\n                      for ref in references\n                  ]\n              }\n          ]\n      }\n  ]\n  score = evaluate_squad(dataset=dataset, predictions=pred_dict)\n  return score","metadata":{"id":"ko5CJBuYygA-","execution":{"iopub.status.busy":"2023-10-27T07:47:29.760711Z","iopub.execute_input":"2023-10-27T07:47:29.761062Z","iopub.status.idle":"2023-10-27T07:47:29.778617Z","shell.execute_reply.started":"2023-10-27T07:47:29.761029Z","shell.execute_reply":"2023-10-27T07:47:29.777754Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"compute_squad(references=gold, predictions=formatted_predictions)","metadata":{"id":"h99TMCPDuj6Q","outputId":"8fdbf5f0-0e7d-4360-84d5-9e4bf86de8ba","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-10-27T07:47:29.779988Z","iopub.execute_input":"2023-10-27T07:47:29.780310Z","iopub.status.idle":"2023-10-27T07:47:29.848666Z","shell.execute_reply.started":"2023-10-27T07:47:29.780279Z","shell.execute_reply":"2023-10-27T07:47:29.847829Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 75.56675062972292, 'f1': 80.50033006143278}"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained(\"./indonesian_model\")","metadata":{"execution":{"iopub.status.busy":"2023-10-26T16:42:25.568617Z","iopub.execute_input":"2023-10-26T16:42:25.569434Z","iopub.status.idle":"2023-10-26T16:42:26.639825Z","shell.execute_reply.started":"2023-10-26T16:42:25.569390Z","shell.execute_reply":"2023-10-26T16:42:26.639009Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(\"./indonesian_model\").to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T07:46:49.141561Z","iopub.execute_input":"2023-10-27T07:46:49.141860Z","iopub.status.idle":"2023-10-27T07:46:51.206494Z","shell.execute_reply.started":"2023-10-27T07:46:49.141835Z","shell.execute_reply":"2023-10-27T07:46:51.205591Z"},"trusted":true},"execution_count":64,"outputs":[]}]}
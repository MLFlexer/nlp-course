{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 1 exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .util import get_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For each of the languages Arabic, Bengali and Indonesian, report the 5 most common words in the documents from the training set. Then report the 5 most common words in the questions from the training set. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def top_5_words(word_list):\n",
    "    word_count = Counter(word_list)\n",
    "    top_5 = word_count.most_common(5)\n",
    "    return top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Top 5 Bengali words:', top_5_words(question_text_tokenized_bengali))\n",
    "print('Top 5 Arabic words:', top_5_words(question_text_tokenized_arabic))\n",
    "print('Top 5 Indonesian words:', top_5_words(question_text_tokenized_indonesian))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) implement an “oracle” function that indicates whether a question is an- swerable or not given the document and answer. That is, the function will output 1 if the answer to the question appears in the document and 0 otherwise. Then implement a rule-based classifier that predicts whether a question is answerable only using the document and question. Use the oracle function to evaluate it. What is the performance of your classifier on the validation set for each of the languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oracle function which takes a dataframe and row of a dataframe to check whether the text of the question appears in the document text\n",
    "def oracle(df, row):\n",
    "    \"\"\"\n",
    "    If text (a word) from question appears in document, assume that question is answerable\n",
    "    Return 1 if answerable\n",
    "    Return 0 if not answerable\n",
    "    \"\"\"\n",
    "    \n",
    "    question = df['question_text'][row].split()\n",
    "    document = df['document_plaintext'][row].split()\n",
    "    \n",
    "    found = False\n",
    "    for word in question:\n",
    "        if word in document:\n",
    "            found = True\n",
    "            break \n",
    "\n",
    "    if found:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column with whether the oracle function classifies the result as 0 or 1\n",
    "answer_classification = []\n",
    "\n",
    "for index, row in df_train.iterrows():\n",
    "    result = oracle(df_train, index) \n",
    "    answer_classification.append(result)\n",
    "    \n",
    "df_train['answer_classification'] = answer_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary column where if the question is answered it is equal to 1, and if not answerable it is 0\n",
    "def check_annotations(annotation):\n",
    "    return annotation == {'answer_start': [-1], 'answer_text': ['']}\n",
    "\n",
    "df_train['correct_answer'] = df_train['annotations'].apply(check_annotations)\n",
    "df_train['correct_answer'] = (~df_train['correct_answer']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics function\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def performance_metrics(df):\n",
    "    y_true = df['correct_answer']\n",
    "    y_pred = df['answer_classification']\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1}\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update other languages dataframes\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display performance metrics\n",
    "print('Overall:', performance_metrics(df_train))\n",
    "print('Bengali:', performance_metrics(df_train_bengali))\n",
    "print('Arabic:', performance_metrics(df_train_arabic))\n",
    "print('Indonesian:', performance_metrics(df_train_indonesian))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

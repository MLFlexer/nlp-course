{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXtHLHN3tVs2",
        "outputId": "b27f0855-60f8-4c48-d9ba-4074c258e1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# this is week 4 test\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preamble\n",
        "import sys\n",
        "sys.path.append('..')"
      ],
      "metadata": {
        "id": "N12ZL_vQtkwL"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
        "\n",
        "train_set = dataset[\"train\"]\n",
        "validation_set = dataset[\"validation\"]\n",
        "\n",
        "df_train = train_set.to_pandas()\n",
        "df_val = validation_set.to_pandas()\n",
        "\n",
        "print(len(df_train))\n",
        "print(len(df_val))\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "1f7OFSVfty8u",
        "outputId": "39463c43-ef0c-4653-a528-f65def98f6da"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116067\n",
            "13325\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            question_text document_title  language  \\\n",
              "0            Milloin Charles Fort syntyi?   Charles Fort   finnish   \n",
              "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
              "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
              "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
              "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
              "\n",
              "                                         annotations  \\\n",
              "0  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
              "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
              "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
              "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
              "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
              "\n",
              "                                  document_plaintext  \\\n",
              "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
              "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
              "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
              "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
              "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
              "\n",
              "                                        document_url  \n",
              "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
              "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
              "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
              "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
              "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d00b5f0-bad5-44fe-bad5-db0cdf983bea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_text</th>\n",
              "      <th>document_title</th>\n",
              "      <th>language</th>\n",
              "      <th>annotations</th>\n",
              "      <th>document_plaintext</th>\n",
              "      <th>document_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Milloin Charles Fort syntyi?</td>\n",
              "      <td>Charles Fort</td>\n",
              "      <td>finnish</td>\n",
              "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
              "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
              "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
              "      <td>ダニエル・J・キャラハン</td>\n",
              "      <td>japanese</td>\n",
              "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
              "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
              "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
              "      <td>వేప</td>\n",
              "      <td>telugu</td>\n",
              "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
              "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
              "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
              "      <td>চেঙ্গিজ খান</td>\n",
              "      <td>bengali</td>\n",
              "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
              "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
              "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
              "      <td>రెయ్యలగడ్ద</td>\n",
              "      <td>telugu</td>\n",
              "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
              "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
              "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d00b5f0-bad5-44fe-bad5-db0cdf983bea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8d00b5f0-bad5-44fe-bad5-db0cdf983bea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8d00b5f0-bad5-44fe-bad5-db0cdf983bea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-de8b83c3-eb98-47a7-ba82-873d4cd59446\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de8b83c3-eb98-47a7-ba82-873d4cd59446')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-de8b83c3-eb98-47a7-ba82-873d4cd59446 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get train and validation data for each language\n",
        "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
        "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
        "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
        "\n",
        "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
        "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
        "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
        "\n",
        "\n",
        "# For testing\n",
        "df_train_english = df_train[df_train['language'] == 'english']\n",
        "df_val_english = df_val[df_val['language'] == 'english']"
      ],
      "metadata": {
        "id": "O5CJ2hXEt2m9"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_with_answer(row):\n",
        "  start = row[\"annotations\"][\"answer_start\"][0]\n",
        "\n",
        "  if start == -1:\n",
        "    return pd.Series([row[\"document_plaintext\"], \"\", \"\"])\n",
        "  end = row[\"annotations\"][\"answer_start\"][0] + len(row[\"annotations\"][\"answer_text\"][0])\n",
        "  before = row[\"document_plaintext\"][:start]\n",
        "  after = row[\"document_plaintext\"][end:]\n",
        "  return pd.Series([before, row[\"annotations\"][\"answer_text\"][0], after])"
      ],
      "metadata": {
        "id": "D0sbXplquHQs"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[[\"before\", \"answer\", \"after\"]] = df_train_english.apply(split_text_with_answer, axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SBSr6vouqOB",
        "outputId": "a089487c-1078-491c-fd8c-d4bb5c1f6313"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-e020da73293f>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[[\"before\", \"answer\", \"after\"]] = df_train_english.apply(split_text_with_answer, axis=1)\n",
            "<ipython-input-78-e020da73293f>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[[\"before\", \"answer\", \"after\"]] = df_train_english.apply(split_text_with_answer, axis=1)\n",
            "<ipython-input-78-e020da73293f>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[[\"before\", \"answer\", \"after\"]] = df_train_english.apply(split_text_with_answer, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "id": "2A-xo9SKyjvJ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "# mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
      ],
      "metadata": {
        "id": "QGghbYczyqKl"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = AutoModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "\n",
        "# Input text\n",
        "text = \"This is an example sentence.\"\n",
        "\n",
        "# Tokenize the text\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass the tokenized input through the model to obtain embeddings\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Get the embeddings for the [CLS] token (or other tokens as needed)\n",
        "embeddings = outputs.last_hidden_state  # You can extract embeddings for specific tokens as needed\n",
        "\n",
        "# Now, you have the vector embeddings for the input text\n",
        "print(embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ygm7JadrD3i1",
        "outputId": "732e6616-0af5-477b-dee9-665a0dd1ea23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1184, -0.1115, -0.0161,  ...,  0.0863,  0.0821, -0.0635],\n",
            "         [-0.4862,  0.1382,  0.1461,  ..., -0.0828, -0.1202, -0.0031],\n",
            "         [-0.5692,  0.0700,  0.3774,  ...,  0.3269,  0.1650, -0.1602],\n",
            "         ...,\n",
            "         [-0.2747,  0.2938,  0.0482,  ..., -0.2119,  0.8361, -0.8724],\n",
            "         [-0.4139,  0.4461, -0.0500,  ..., -0.0192,  0.2051, -0.4876],\n",
            "         [-0.0107,  0.2140, -0.5340,  ...,  0.5806,  0.9596, -0.6217]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embeddings.shape)"
      ],
      "metadata": {
        "id": "hl9jMYQQEHeE",
        "outputId": "20d5c135-cf3d-4f90-c9f0-7d7fb3291f64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 8, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def tokenize_before_after(row, tokenizer):\n",
        "  before_tokens = tokenizer.tokenize(row[\"before\"])\n",
        "  answer_tokens = tokenizer.tokenize(row[\"answer\"])\n",
        "  after_tokens = tokenizer.tokenize(row[\"after\"])\n",
        "  return pd.Series([before_tokens, answer_tokens, after_tokens])\"\"\""
      ],
      "metadata": {
        "id": "2ZzVfe_evQ4M",
        "outputId": "f2b341b9-423a-4e05-b99d-11a61b5ef821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def tokenize_before_after(row, tokenizer):\\n  before_tokens = tokenizer.tokenize(row[\"before\"])\\n  answer_tokens = tokenizer.tokenize(row[\"answer\"])\\n  after_tokens = tokenizer.tokenize(row[\"after\"])\\n  return pd.Series([before_tokens, answer_tokens, after_tokens])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenizer(row):\n",
        "  before_tokens = row[\"before\"].split()\n",
        "  answer_tokens = row[\"answer\"].split()\n",
        "  after_tokens = row[\"after\"].split()\n",
        "  return pd.Series([before_tokens, answer_tokens, after_tokens])"
      ],
      "metadata": {
        "id": "9tJakjM8-TcT"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bpemb\n",
        "from bpemb import BPEmb\n",
        "\n",
        "# Load English model with 25k word-pieces\n",
        "bpemb_id = BPEmb(lang='eng', dim=100, vs=25000)\n",
        "\n",
        "# Extract the embeddings\n",
        "pretrained_embeddings = bpemb_id.emb.vectors\n",
        "\n",
        "tokenizer = bpemb_id"
      ],
      "metadata": {
        "id": "On89TVmgOJCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def BPEmb_tokenize_before_after(row, BPEmb_tokenizer):\n",
        "  before_tokens = BPEmb_tokenizer.encode(row[\"before\"])\n",
        "  answer_tokens = BPEmb_tokenizer.encode(row[\"answer\"])\n",
        "  after_tokens = BPEmb_tokenizer.encode(row[\"after\"])\n",
        "  return pd.Series([before_tokens, answer_tokens, after_tokens])"
      ],
      "metadata": {
        "id": "uhl6hgKaMvdP",
        "outputId": "a0ddf472-bea2-4cac-ab5e-5afed03eefa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2023.7.22)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df_train_english.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
        "# df_train_english[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df_train_english.apply(simple_tokenizer, axis=1)\n",
        "df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "Kyp-GtoYzKBW",
        "outputId": "9a756dcf-7663-43aa-cc11-07cb9584a1dc"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-1dffa2a88cca>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df_train_english.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
            "<ipython-input-94-1dffa2a88cca>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df_train_english.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
            "<ipython-input-94-1dffa2a88cca>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df_train_english.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         question_text  \\\n",
              "26            When was quantum field theory developed?   \n",
              "43   Who was the first Nobel prize winner for Liter...   \n",
              "112               When is the dialectical method used?   \n",
              "123                               Who invented Hangul?   \n",
              "125                          What do Grasshoppers eat?   \n",
              "\n",
              "                            document_title language  \\\n",
              "26                    Quantum field theory  english   \n",
              "43   List of Nobel laureates in Literature  english   \n",
              "112                              Dialectic  english   \n",
              "123                       Origin of Hangul  english   \n",
              "125                            Grasshopper  english   \n",
              "\n",
              "                                           annotations  \\\n",
              "26   {'answer_start': [159], 'answer_text': ['1920s']}   \n",
              "43   {'answer_start': [610], 'answer_text': ['Sully...   \n",
              "112  {'answer_start': [129], 'answer_text': ['disco...   \n",
              "123  {'answer_start': [88], 'answer_text': ['Sejong...   \n",
              "125  {'answer_start': [0], 'answer_text': ['Grassho...   \n",
              "\n",
              "                                    document_plaintext  \\\n",
              "26   Quantum field theory naturally began with the ...   \n",
              "43   The Nobel Prize in Literature (Swedish: Nobelp...   \n",
              "112  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
              "123  Hangul was personally created and promulgated ...   \n",
              "125  Grasshoppers are plant-eaters, with a few spec...   \n",
              "\n",
              "                                          document_url  \\\n",
              "26   https://en.wikipedia.org/wiki/Quantum%20field%...   \n",
              "43   https://en.wikipedia.org/wiki/List%20of%20Nobe...   \n",
              "112            https://en.wikipedia.org/wiki/Dialectic   \n",
              "123  https://en.wikipedia.org/wiki/Origin%20of%20Ha...   \n",
              "125          https://en.wikipedia.org/wiki/Grasshopper   \n",
              "\n",
              "                                                before  \\\n",
              "26   Quantum field theory naturally began with the ...   \n",
              "43   The Nobel Prize in Literature (Swedish: Nobelp...   \n",
              "112  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
              "123  Hangul was personally created and promulgated ...   \n",
              "125                                                      \n",
              "\n",
              "                                                answer  \\\n",
              "26                                               1920s   \n",
              "43                                     Sully Prudhomme   \n",
              "112  discourse between two or more people holding d...   \n",
              "123                                   Sejong the Great   \n",
              "125  Grasshoppers are plant-eaters, with a few spec...   \n",
              "\n",
              "                                                 after  \\\n",
              "26                                              .[8]:1   \n",
              "43    of France.[3] Each recipient receives a medal...   \n",
              "112  . Dialectic resembles debate, but the concept ...   \n",
              "123  .[1][2] Sejong's scholarly institute, the Hall...   \n",
              "125  . They protect themselves from predators by ca...   \n",
              "\n",
              "                                         before_tokens  \\\n",
              "26   [▁quantum, ▁field, ▁theory, ▁naturally, ▁began...   \n",
              "43   [▁the, ▁nobel, ▁prize, ▁in, ▁literature, ▁(, s...   \n",
              "112  [▁dialect, ic, ▁or, ▁dialect, ics, ▁(, greek, ...   \n",
              "123  [▁hang, ul, ▁was, ▁personally, ▁created, ▁and,...   \n",
              "125                                                 []   \n",
              "\n",
              "                                         answer_tokens  \\\n",
              "26                                          [▁0000, s]   \n",
              "43                          [▁sul, ly, ▁prud, hom, me]   \n",
              "112  [▁discourse, ▁between, ▁two, ▁or, ▁more, ▁peop...   \n",
              "123                          [▁se, jong, ▁the, ▁great]   \n",
              "125  [▁grass, hop, pers, ▁are, ▁plant, -, e, aters,...   \n",
              "\n",
              "                                          after_tokens  \n",
              "26                                   [▁., [, 0, ], :0]  \n",
              "43   [▁of, ▁france, ., [, 0, ], ▁each, ▁recipient, ...  \n",
              "112  [▁., ▁dialect, ic, ▁resembles, ▁debate, ,, ▁bu...  \n",
              "123  [▁., [, 0, ][, 0, ], ▁se, jong, ', s, ▁scholar...  \n",
              "125  [▁., ▁they, ▁protect, ▁themselves, ▁from, ▁pre...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-713d7616-6258-43ee-9780-246621c363da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_text</th>\n",
              "      <th>document_title</th>\n",
              "      <th>language</th>\n",
              "      <th>annotations</th>\n",
              "      <th>document_plaintext</th>\n",
              "      <th>document_url</th>\n",
              "      <th>before</th>\n",
              "      <th>answer</th>\n",
              "      <th>after</th>\n",
              "      <th>before_tokens</th>\n",
              "      <th>answer_tokens</th>\n",
              "      <th>after_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>When was quantum field theory developed?</td>\n",
              "      <td>Quantum field theory</td>\n",
              "      <td>english</td>\n",
              "      <td>{'answer_start': [159], 'answer_text': ['1920s']}</td>\n",
              "      <td>Quantum field theory naturally began with the ...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
              "      <td>Quantum field theory naturally began with the ...</td>\n",
              "      <td>1920s</td>\n",
              "      <td>.[8]:1</td>\n",
              "      <td>[▁quantum, ▁field, ▁theory, ▁naturally, ▁began...</td>\n",
              "      <td>[▁0000, s]</td>\n",
              "      <td>[▁., [, 0, ], :0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
              "      <td>List of Nobel laureates in Literature</td>\n",
              "      <td>english</td>\n",
              "      <td>{'answer_start': [610], 'answer_text': ['Sully...</td>\n",
              "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
              "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
              "      <td>Sully Prudhomme</td>\n",
              "      <td>of France.[3] Each recipient receives a medal...</td>\n",
              "      <td>[▁the, ▁nobel, ▁prize, ▁in, ▁literature, ▁(, s...</td>\n",
              "      <td>[▁sul, ly, ▁prud, hom, me]</td>\n",
              "      <td>[▁of, ▁france, ., [, 0, ], ▁each, ▁recipient, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>When is the dialectical method used?</td>\n",
              "      <td>Dialectic</td>\n",
              "      <td>english</td>\n",
              "      <td>{'answer_start': [129], 'answer_text': ['disco...</td>\n",
              "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
              "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
              "      <td>discourse between two or more people holding d...</td>\n",
              "      <td>. Dialectic resembles debate, but the concept ...</td>\n",
              "      <td>[▁dialect, ic, ▁or, ▁dialect, ics, ▁(, greek, ...</td>\n",
              "      <td>[▁discourse, ▁between, ▁two, ▁or, ▁more, ▁peop...</td>\n",
              "      <td>[▁., ▁dialect, ic, ▁resembles, ▁debate, ,, ▁bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Who invented Hangul?</td>\n",
              "      <td>Origin of Hangul</td>\n",
              "      <td>english</td>\n",
              "      <td>{'answer_start': [88], 'answer_text': ['Sejong...</td>\n",
              "      <td>Hangul was personally created and promulgated ...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
              "      <td>Hangul was personally created and promulgated ...</td>\n",
              "      <td>Sejong the Great</td>\n",
              "      <td>.[1][2] Sejong's scholarly institute, the Hall...</td>\n",
              "      <td>[▁hang, ul, ▁was, ▁personally, ▁created, ▁and,...</td>\n",
              "      <td>[▁se, jong, ▁the, ▁great]</td>\n",
              "      <td>[▁., [, 0, ][, 0, ], ▁se, jong, ', s, ▁scholar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>What do Grasshoppers eat?</td>\n",
              "      <td>Grasshopper</td>\n",
              "      <td>english</td>\n",
              "      <td>{'answer_start': [0], 'answer_text': ['Grassho...</td>\n",
              "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
              "      <td></td>\n",
              "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
              "      <td>. They protect themselves from predators by ca...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[▁grass, hop, pers, ▁are, ▁plant, -, e, aters,...</td>\n",
              "      <td>[▁., ▁they, ▁protect, ▁themselves, ▁from, ▁pre...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-713d7616-6258-43ee-9780-246621c363da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-713d7616-6258-43ee-9780-246621c363da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-713d7616-6258-43ee-9780-246621c363da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0bf62f6f-003b-4f01-83ed-c700b638936e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0bf62f6f-003b-4f01-83ed-c700b638936e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0bf62f6f-003b-4f01-83ed-c700b638936e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = df_train_english.apply(lambda row: len(row[\"before_tokens\"]) + len(row[\"answer\"]) + len(row[\"after_tokens\"]), axis=1).max()\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYWlMj4225jD",
        "outputId": "5806dcd3-33cf-4a4d-fbe3-7407417c9264"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[\"text_pad\"] = df_train_english.apply(lambda row: [\"[PAD]\"] * (max_len - (len(row[\"before_tokens\"]) + len(row[\"answer\"]) + len(row[\"after_tokens\"]))), axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ6WgTpW2OLi",
        "outputId": "3680be75-725f-4e5a-c24f-e533431e5658"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-97-cc718dcdef7b>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"text_pad\"] = df_train_english.apply(lambda row: [\"[PAD]\"] * (max_len - (len(row[\"before_tokens\"]) + len(row[\"answer\"]) + len(row[\"after_tokens\"]))), axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_BIO_tags_text(row):\n",
        "  before = [\"O\"] * len(row[\"before_tokens\"])\n",
        "  if len(row[\"answer_tokens\"]) == 0:\n",
        "    answer = []\n",
        "  else:\n",
        "    answer = [\"B\"] + [\"I\"] * (len(row[\"answer_tokens\"]) - 1)\n",
        "  after = [\"O\"] * len(row[\"after_tokens\"])\n",
        "  pad = [\"O\"] * len(row[\"text_pad\"])\n",
        "  return before + answer + after + pad"
      ],
      "metadata": {
        "id": "ggw6f0rtzixZ"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[\"BIO_tags_text\"] = df_train_english.apply(create_BIO_tags_text, axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSlTHquE0NzR",
        "outputId": "23aad45e-f229-474b-9a90-c92a5e11dfb9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-b19b1d6eb093>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"BIO_tags_text\"] = df_train_english.apply(create_BIO_tags_text, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_lengts_not_eq = df_train_english[df_train_english.apply(lambda row: not (len(row[\"before_tokens\"]) + len(row[\"answer_tokens\"]) + len(row[\"after_tokens\"]) + len(row[\"text_pad\"]) == len(row[\"BIO_tags_text\"])), axis=1)]\n",
        "#df_lengts_not_eq"
      ],
      "metadata": {
        "id": "Gyu1W8im0eq9"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize question:\n",
        "# tokenizer = mbert_tokeniser\n",
        "# df_train_english[\"question_tokens\"] = df_train_english.apply(lambda row: tokenizer.tokenize(row[\"question_text\"]), axis=1)\n",
        "df_train_english[\"question_tokens\"] = df_train_english.apply(lambda row: row[\"question_text\"].split(), axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zukN6sz06uNW",
        "outputId": "9496ff59-8f5c-460a-b2dc-2b260f71262a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-101-d7483d0d7550>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"question_tokens\"] = df_train_english.apply(lambda row: row[\"question_text\"].split(), axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_questions = df_train_english.apply(lambda row: len(row[\"question_tokens\"]), axis=1).max()\n",
        "print(max_len_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpTHTQ9G7yav",
        "outputId": "68e7abdc-6a39-444c-8e52-3ecd637f88cd"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[\"question_pad\"] = df_train_english.apply(lambda row: [\"[PAD]\"] * (max_len_questions - len(row[\"question_tokens\"])), axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpN1ido58AWO",
        "outputId": "c5a69ab7-4a1a-44e8-806c-e0d5aa858105"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-103-289d7fdfd640>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"question_pad\"] = df_train_english.apply(lambda row: [\"[PAD]\"] * (max_len_questions - len(row[\"question_tokens\"])), axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[\"BIO_tags_question\"] = df_train_english.apply(lambda row: [\"O\"] * max_len_questions, axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpKGYOZm8PB8",
        "outputId": "16d35973-a687-47c8-a19f-c756de04cc3e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-104-c1a084ec7cd6>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"BIO_tags_question\"] = df_train_english.apply(lambda row: [\"O\"] * max_len_questions, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[\"merged_BIO\"] = df_train_english.apply(lambda row: row[\"BIO_tags_text\"] + row[\"BIO_tags_question\"], axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUest-IQ8kVy",
        "outputId": "0e3efb6d-11ff-4055-fd54-5e153c6f30e6"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-105-ca49f9b253d9>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"merged_BIO\"] = df_train_english.apply(lambda row: row[\"BIO_tags_text\"] + row[\"BIO_tags_question\"], axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[\"merged_tokens\"] = df_train_english.apply(lambda row: row[\"before_tokens\"] + row[\"answer_tokens\"] + row[\"after_tokens\"] + row[\"text_pad\"] + row[\"question_tokens\"] + row[\"question_pad\"], axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QmQ5MkQ829I",
        "outputId": "b7494975-ffb7-4e16-801a-5d19516b2837"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-864bc71f6821>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"merged_tokens\"] = df_train_english.apply(lambda row: row[\"before_tokens\"] + row[\"answer_tokens\"] + row[\"after_tokens\"] + row[\"text_pad\"] + row[\"question_tokens\"] + row[\"question_pad\"], axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_lengts_not_eq = df_train_english[df_train_english.apply(lambda row: not (len(row[\"merged_BIO\"])) == len(row[\"merged_tokens\"]), axis=1)]\n",
        "#df_lengts_not_eq"
      ],
      "metadata": {
        "id": "-Jt4enh79nUb"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BIO_tags_to_labels(row):\n",
        "  # Create a dictionary to map the strings to numbers\n",
        "  mapping = {\"B\": 2, \"O\": 0, \"I\": 1}\n",
        "\n",
        "  # Use a list comprehension to convert the strings to numbers\n",
        "  return [mapping[char] for char in row[\"merged_BIO\"]]"
      ],
      "metadata": {
        "id": "xyO55f60UyZC"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_english[\"BIO_labels\"] = df_train_english.apply(BIO_tags_to_labels, axis=1)\n",
        "#df_train_english.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40lW_pHKVBhX",
        "outputId": "361d89fd-9d02-4dd1-82f1-d9d8323c07f3"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-109-bd3e41d9a529>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_train_english[\"BIO_labels\"] = df_train_english.apply(BIO_tags_to_labels, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format of merged vector:\n",
        "[before] + [answer] + [after] + [padding] + [question] + [padding]"
      ],
      "metadata": {
        "id": "UlEWdylt-VIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bpemb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTuk-zk4OD7n",
        "outputId": "1d0d784e-9d76-4387-a86b-fd7f3be91de5"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bpemb import BPEmb\n",
        "\n",
        "# Load English model with 25k word-pieces\n",
        "bpemb_id = BPEmb(lang='eng', dim=100, vs=25000)\n",
        "\n",
        "# Extract the embeddings\n",
        "pretrained_embeddings = bpemb_id.emb.vectors"
      ],
      "metadata": {
        "id": "355PFmoxOATk"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Following code is taken from lab 4"
      ],
      "metadata": {
        "id": "lrfwIPDeNkqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import torch\n",
        "import random\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
        "from typing import List, Tuple, AnyStr\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from datasets import load_dataset, load_metric\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch.nn.functional as F\n",
        "import heapq"
      ],
      "metadata": {
        "id": "RhWbOsUhMt5V"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class BiLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic BiLSTM-CRF network\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            pretrained_embeddings: torch.tensor,\n",
        "            lstm_dim: int,\n",
        "            dropout_prob: float = 0.1,\n",
        "            n_classes: int = 2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializer for basic BiLSTM network\n",
        "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
        "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
        "        :param dropout_prob: Dropout probability\n",
        "        :param n_classes: The number of output classes\n",
        "        \"\"\"\n",
        "\n",
        "        # First thing is to call the superclass initializer\n",
        "        super(BiLSTM, self).__init__()\n",
        "\n",
        "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
        "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
        "        self.model = nn.ModuleDict({\n",
        "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
        "            'bilstm': nn.LSTM(\n",
        "                pretrained_embeddings.shape[1],  # input size\n",
        "                lstm_dim,  # hidden size\n",
        "                2,  # number of layers\n",
        "                batch_first=True,\n",
        "                dropout=dropout_prob,\n",
        "                bidirectional=True),\n",
        "            'ff': nn.Linear(2*lstm_dim, n_classes),\n",
        "        })\n",
        "        self.n_classes = n_classes\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        # Initialize the weights of the model\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
        "                     list(self.model['ff'].named_parameters())\n",
        "        for n,p in all_params:\n",
        "            if 'weight' in n:\n",
        "                nn.init.xavier_normal_(p)\n",
        "            elif 'bias' in n:\n",
        "                nn.init.zeros_(p)\n",
        "\n",
        "    def forward(self, inputs, input_lens, hidden_states = None, labels = None):\n",
        "        \"\"\"\n",
        "        Defines how tensors flow through the model\n",
        "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
        "        :param input_lens: (b) The length of each input sequence\n",
        "        :param labels: (b) The label of each sample\n",
        "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
        "        \"\"\"\n",
        "\n",
        "        # Get embeddings (b x sl x edim)\n",
        "        embeds = self.model['embeddings'](inputs)\n",
        "\n",
        "        # Pack padded: This is necessary for padded batches input to an RNN - https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
        "            embeds,\n",
        "            input_lens.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Pass the packed sequence through the BiLSTM\n",
        "        if hidden_states:\n",
        "            lstm_out, hidden = self.model['bilstm'](lstm_in, hidden_states)\n",
        "        else:\n",
        "            lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
        "\n",
        "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
        "        lstm_out, lengths = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
        "\n",
        "        # Get logits (b x seq_len x n_classes)\n",
        "        logits = self.model['ff'](lstm_out)\n",
        "        outputs = (logits, lengths)\n",
        "        if labels is not None:\n",
        "            loss = self.loss(logits.reshape(-1, self.n_classes), labels.reshape(-1))\n",
        "            outputs =  outputs + (loss,)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "BmV4glRF-out"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model: nn.Module,\n",
        "    train_dl: DataLoader,\n",
        "    valid_dl: DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    n_epochs: int,\n",
        "    device: torch.device,\n",
        "    scheduler=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    The main training loop which will optimize a given model on a given dataset\n",
        "    :param model: The model being optimized\n",
        "    :param train_dl: The training dataset\n",
        "    :param valid_dl: A validation dataset\n",
        "    :param optimizer: The optimizer used to update the model parameters\n",
        "    :param n_epochs: Number of epochs to train for\n",
        "    :param device: The device to train on\n",
        "    :return: (model, losses) The best model and the losses per iteration\n",
        "    \"\"\"\n",
        "\n",
        "  # Keep track of the loss and best accuracy\n",
        "    losses = []\n",
        "    learning_rates = []\n",
        "    best_f1 = 0.0\n",
        "\n",
        "    # Iterate through epochs\n",
        "    for ep in range(n_epochs):\n",
        "\n",
        "        loss_epoch = []\n",
        "\n",
        "        #Iterate through each batch in the dataloader\n",
        "        for batch in tqdm(train_dl):\n",
        "            # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n",
        "            # things like dropout and layer normalization\n",
        "            model.train()\n",
        "\n",
        "            # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
        "            # keeps track of these dynamically in its computation graph so you need to explicitly\n",
        "            # zero them out\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Place each tensor on the GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids = batch[0]\n",
        "            seq_lens = batch[1]\n",
        "            labels = batch[2]\n",
        "\n",
        "            # Pass the inputs through the model, get the current loss and logits\n",
        "            logits, lengths, loss = model(input_ids, seq_lens, labels=labels)\n",
        "            losses.append(loss.item())\n",
        "            loss_epoch.append(loss.item())\n",
        "\n",
        "            # Calculate all of the gradients and weight updates for the model\n",
        "            loss.backward()\n",
        "\n",
        "            # Optional: clip gradients\n",
        "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Finally, update the weights of the model\n",
        "            optimizer.step()\n",
        "            if scheduler != None:\n",
        "                scheduler.step()\n",
        "                learning_rates.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "        # Perform inline evaluation at the end of the epoch\n",
        "        f1 = evaluate(model, valid_dl)\n",
        "        print(f'Validation F1: {f1}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
        "\n",
        "        # Keep track of the best model based on the accuracy\n",
        "        if f1 > best_f1:\n",
        "            torch.save(model.state_dict(), 'best_model')\n",
        "            best_f1 = f1\n",
        "\n",
        "    return losses, learning_rates"
      ],
      "metadata": {
        "id": "bbA2efabMhtm"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given dataset\n",
        "    :param model: The model under evaluation\n",
        "    :param valid_dl: A `DataLoader` reading validation data\n",
        "    :return: The accuracy of the model on the dataset\n",
        "    \"\"\"\n",
        "    # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n",
        "    # layer normalization and dropout\n",
        "    model.eval()\n",
        "    labels_all = []\n",
        "    preds_all = []\n",
        "\n",
        "    # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_dl, desc='Evaluation'):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids = batch[0]\n",
        "            seq_lens = batch[1]\n",
        "            labels = batch[2]\n",
        "            hidden_states = None\n",
        "\n",
        "            logits, _, _ = model(input_ids, seq_lens, hidden_states=hidden_states, labels=labels)\n",
        "            preds_all.extend(torch.argmax(logits, dim=-1).reshape(-1).detach().cpu().numpy())\n",
        "            labels_all.extend(labels.reshape(-1).detach().cpu().numpy())\n",
        "\n",
        "    P, R, F1, _ = precision_recall_fscore_support(labels_all, preds_all, average='macro')\n",
        "    print(confusion_matrix(labels_all, preds_all))\n",
        "    return F1\n",
        "\n"
      ],
      "metadata": {
        "id": "x5lp1LigNtEd"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_dim = 128\n",
        "dropout_prob = 0.1\n",
        "batch_size = 8\n",
        "lr = 1e-2\n",
        "n_epochs = 10\n",
        "n_workers = 0  # set to a larger number if you run your code in colab\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "# Create the model\n",
        "model = BiLSTM(\n",
        "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings),\n",
        "    lstm_dim=lstm_dim,\n",
        "    dropout_prob=dropout_prob,\n",
        "    n_classes=3\n",
        "  ).to(device)"
      ],
      "metadata": {
        "id": "OumBhBN3NvLf"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Combines multiple data samples into a single batch\n",
        "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
        "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
        "    \"\"\"\n",
        "    input_ids = [i[0][0] for i in input_data]\n",
        "    seq_lens = [len(i) for i in input_data]\n",
        "    labels = [i[2] for i in input_data]\n",
        "\n",
        "    max_length = max([len(i) for i in input_ids])\n",
        "\n",
        "    # Pad all of the input samples to the max length (25000 is the ID of the [PAD] token)\n",
        "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
        "\n",
        "    # Make sure each sample is max_length long\n",
        "    assert (all(len(i) == max_length for i in input_ids))\n",
        "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
      ],
      "metadata": {
        "id": "OOJiSlm1OqVJ"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    input_ids = [tokenizer.encode_ids_with_eos(i[0][0]) for i in input_data]\n",
        "    seq_lens = [len(i) for i in input_ids]\n",
        "    labels = [i[2] for i in input_data]\n",
        "\n",
        "    max_length = max([len(i) for i in input_ids])\n",
        "\n",
        "    input_ids = [(i + [0] * (max_length - len(i))) for i in input_ids]\n",
        "    labels = [(i + [0] * (max_length - len(i))) for i in labels] # 0 is the id of the O tag\n",
        "\n",
        "    assert (all(len(i) == max_length for i in input_ids))\n",
        "    assert (all(len(i) == max_length for i in labels))\n",
        "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
      ],
      "metadata": {
        "id": "spAX4fRUayHT"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_batch_bilstm(text: List, tokenizer, max_len=512) -> Tuple[List, List]:\n",
        "    \"\"\"\n",
        "    Creates a tokenized batch for input to a bilstm model\n",
        "    :param text: A list of sentences to tokenize\n",
        "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
        "    :return: Tokenized text as well as the length of the input sequence\n",
        "    \"\"\"\n",
        "    # Some light preprocessing\n",
        "    input_ids = [tokenizer.encode_ids_with_eos(t)[:max_len] for t in text]\n",
        "\n",
        "    return input_ids, [len(ids) for ids in input_ids]"
      ],
      "metadata": {
        "id": "3LmTjqCyYJiq"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will load the dataset and process it lazily in the __getitem__ function\n",
        "class ClassificationDatasetReader(Dataset):\n",
        "  def __init__(self, df, tokenizer):\n",
        "    self.df = df\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df.values[idx]\n",
        "    # Calls the text_to_batch function\n",
        "    input_ids,seq_lens = text_to_batch_bilstm([row[0]], self.tokenizer)\n",
        "    label = row[1]\n",
        "    return input_ids, seq_lens, label"
      ],
      "metadata": {
        "id": "0TRIcTQEYHR1"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_df(df, tokenizer):\n",
        "  df[[\"before\", \"answer\", \"after\"]] = df.apply(split_text_with_answer, axis=1)\n",
        "  df[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
        "  max_len = df.apply(lambda row: len(row[\"before_tokens\"]) + len(row[\"answer\"]) + len(row[\"after_tokens\"]), axis=1).max()\n",
        "  df[\"text_pad\"] = df.apply(lambda row: [\"[PAD]\"] * (max_len - (len(row[\"before_tokens\"]) + len(row[\"answer\"]) + len(row[\"after_tokens\"]))), axis=1)\n",
        "  df[\"BIO_tags_text\"] = df.apply(create_BIO_tags_text, axis=1)\n",
        "  df[\"question_tokens\"] = df.apply(lambda row: tokenizer.encode(row[\"question_text\"]), axis=1)\n",
        "  max_len_questions = df.apply(lambda row: len(row[\"question_tokens\"]), axis=1).max()\n",
        "  df[\"question_pad\"] = df.apply(lambda row: [\"[PAD]\"] * (max_len_questions - len(row[\"question_tokens\"])), axis=1)\n",
        "  df[\"BIO_tags_question\"] = df.apply(lambda row: [\"O\"] * max_len_questions, axis=1)\n",
        "  df[\"merged_BIO\"] = df.apply(lambda row: row[\"BIO_tags_text\"] + row[\"BIO_tags_question\"], axis=1)\n",
        "  df[\"merged_tokens\"] = df.apply(lambda row: row[\"before_tokens\"] + row[\"answer_tokens\"] + row[\"after_tokens\"] + row[\"text_pad\"] + row[\"question_tokens\"] + row[\"question_pad\"], axis=1)\n",
        "  df[\"BIO_labels\"] = df.apply(BIO_tags_to_labels, axis=1)\n",
        "  res_df = pd.DataFrame({\n",
        "    'text':(df[\"merged_tokens\"]),\n",
        "    'answerable':(df[\"BIO_labels\"])\n",
        "    })\n",
        "  return df, res_df"
      ],
      "metadata": {
        "id": "wrgtb3X0QvHp"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_train, english_train_merged = pre_process_df(df_train_english, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeHErObSSDSe",
        "outputId": "d5219b1f-0d22-4f52-de5f-a7ef398e829f"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-121-6dd39949d74d>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before\", \"answer\", \"after\"]] = df.apply(split_text_with_answer, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"text_pad\"] = df.apply(lambda row: [\"[PAD]\"] * (max_len - (len(row[\"before_tokens\"]) + len(row[\"answer\"]) + len(row[\"after_tokens\"]))), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"BIO_tags_text\"] = df.apply(create_BIO_tags_text, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"question_tokens\"] = df.apply(lambda row: tokenizer.encode(row[\"question_text\"]), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"question_pad\"] = df.apply(lambda row: [\"[PAD]\"] * (max_len_questions - len(row[\"question_tokens\"])), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"BIO_tags_question\"] = df.apply(lambda row: [\"O\"] * max_len_questions, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"merged_BIO\"] = df.apply(lambda row: row[\"BIO_tags_text\"] + row[\"BIO_tags_question\"], axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"merged_tokens\"] = df.apply(lambda row: row[\"before_tokens\"] + row[\"answer_tokens\"] + row[\"after_tokens\"] + row[\"text_pad\"] + row[\"question_tokens\"] + row[\"question_pad\"], axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"BIO_labels\"] = df.apply(BIO_tags_to_labels, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_train_merged"
      ],
      "metadata": {
        "id": "BLJUolC0XlsO",
        "outputId": "c7e561b4-a6bc-459a-d2e9-0e0c17d6b636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text  \\\n",
              "26      [▁quantum, ▁field, ▁theory, ▁naturally, ▁began...   \n",
              "43      [▁the, ▁nobel, ▁prize, ▁in, ▁literature, ▁(, s...   \n",
              "112     [▁dialect, ic, ▁or, ▁dialect, ics, ▁(, greek, ...   \n",
              "123     [▁hang, ul, ▁was, ▁personally, ▁created, ▁and,...   \n",
              "125     [▁grass, hop, pers, ▁are, ▁plant, -, e, aters,...   \n",
              "...                                                   ...   \n",
              "116000  [▁the, ▁medley, ▁relay, ▁was, ▁scheduled, ▁in,...   \n",
              "116012  [▁s, ā, m, kh, ya, ▁is, ▁a, ▁dual, ist, ▁philo...   \n",
              "116027  [▁m, ollo, ▁was, ▁surprised, ▁by, ▁the, ▁succe...   \n",
              "116055  [▁in, ▁the, ▁end, ,, ▁president, ▁truman, ▁mad...   \n",
              "116066  [▁the, ▁previous, ▁mayor, ,, ▁bill, ▁la, fore,...   \n",
              "\n",
              "                                               answerable  \n",
              "26      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "43      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "112     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "123     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "125     [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "...                                                   ...  \n",
              "116000  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "116012  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "116027  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "116055  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "116066  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "\n",
              "[7389 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-51698b87-836e-49a3-859c-2f4ace45ff3a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>answerable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>[▁quantum, ▁field, ▁theory, ▁naturally, ▁began...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>[▁the, ▁nobel, ▁prize, ▁in, ▁literature, ▁(, s...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>[▁dialect, ic, ▁or, ▁dialect, ics, ▁(, greek, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>[▁hang, ul, ▁was, ▁personally, ▁created, ▁and,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>[▁grass, hop, pers, ▁are, ▁plant, -, e, aters,...</td>\n",
              "      <td>[2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116000</th>\n",
              "      <td>[▁the, ▁medley, ▁relay, ▁was, ▁scheduled, ▁in,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116012</th>\n",
              "      <td>[▁s, ā, m, kh, ya, ▁is, ▁a, ▁dual, ist, ▁philo...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116027</th>\n",
              "      <td>[▁m, ollo, ▁was, ▁surprised, ▁by, ▁the, ▁succe...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116055</th>\n",
              "      <td>[▁in, ▁the, ▁end, ,, ▁president, ▁truman, ▁mad...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116066</th>\n",
              "      <td>[▁the, ▁previous, ▁mayor, ,, ▁bill, ▁la, fore,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7389 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51698b87-836e-49a3-859c-2f4ace45ff3a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-51698b87-836e-49a3-859c-2f4ace45ff3a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-51698b87-836e-49a3-859c-2f4ace45ff3a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c2d87769-6667-434a-9bb6-825b34ef9de7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c2d87769-6667-434a-9bb6-825b34ef9de7')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c2d87769-6667-434a-9bb6-825b34ef9de7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = bpemb_id\n",
        "english_val, english_val_merged = pre_process_df(df_val_english, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sozfR2m4SoUr",
        "outputId": "eb66b4a6-e453-45c6-9375-017497e372e4"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-121-6dd39949d74d>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before\", \"answer\", \"after\"]] = df.apply(split_text_with_answer, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before\", \"answer\", \"after\"]] = df.apply(split_text_with_answer, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before\", \"answer\", \"after\"]] = df.apply(split_text_with_answer, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[[\"before_tokens\", \"answer_tokens\", \"after_tokens\"]] = df.apply(lambda row: BPEmb_tokenize_before_after(row, tokenizer), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"text_pad\"] = df.apply(lambda row: [\"[PAD]\"] * (max_len - (len(row[\"before_tokens\"]) + len(row[\"answer\"]) + len(row[\"after_tokens\"]))), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"BIO_tags_text\"] = df.apply(create_BIO_tags_text, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"question_tokens\"] = df.apply(lambda row: tokenizer.encode(row[\"question_text\"]), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"question_pad\"] = df.apply(lambda row: [\"[PAD]\"] * (max_len_questions - len(row[\"question_tokens\"])), axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"BIO_tags_question\"] = df.apply(lambda row: [\"O\"] * max_len_questions, axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"merged_BIO\"] = df.apply(lambda row: row[\"BIO_tags_text\"] + row[\"BIO_tags_question\"], axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"merged_tokens\"] = df.apply(lambda row: row[\"before_tokens\"] + row[\"answer_tokens\"] + row[\"after_tokens\"] + row[\"text_pad\"] + row[\"question_tokens\"] + row[\"question_pad\"], axis=1)\n",
            "<ipython-input-121-6dd39949d74d>:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"BIO_labels\"] = df.apply(BIO_tags_to_labels, axis=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ClassificationDatasetReader(english_train_merged, bpemb_id)\n",
        "valid_dataset = ClassificationDatasetReader(english_val_merged, bpemb_id)"
      ],
      "metadata": {
        "id": "YlUEf-VDYT-n"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "zHhSBy5FYskb",
        "outputId": "917c64ad-9683-4a45-d33f-0c9220f41d4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[[24912, 8062, 2],\n",
              "   [24912, 1316, 2],\n",
              "   [24912, 2355, 2],\n",
              "   [24912, 10284, 2],\n",
              "   [24912, 1046, 2],\n",
              "   [24912, 97, 2],\n",
              "   [24912, 7, 2],\n",
              "   [24912, 2147, 2],\n",
              "   [24912, 27, 2],\n",
              "   [24912, 15236, 2],\n",
              "   [24912, 11715, 2],\n",
              "   [1749, 2],\n",
              "   [24912, 79, 2],\n",
              "   [24912, 7, 2],\n",
              "   [24912, 15236, 2],\n",
              "   [24912, 1316, 2],\n",
              "   [24912, 73, 2],\n",
              "   [24912, 7, 2],\n",
              "   [24912, 491, 2],\n",
              "   [24912, 646, 2],\n",
              "   [24912, 4219, 2],\n",
              "   [24912, 1316, 2],\n",
              "   [24912, 79, 2],\n",
              "   [24912, 27, 2],\n",
              "   [24912, 7, 2],\n",
              "   [24912, 52, 2],\n",
              "   [10, 2],\n",
              "   [24912, 896, 2],\n",
              "   [24912, 0, 2],\n",
              "   [121, 2],\n",
              "   [24912, 0, 2],\n",
              "   [20804, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2],\n",
              "   [24912, 0, 15173, 0, 2]]],\n",
              " [512],\n",
              " [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  2,\n",
              "  1,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  ...])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm, num_workers=n_workers)\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=len(valid_dataset), collate_fn=collate_batch_bilstm, num_workers=n_workers)\n",
        "\n",
        "# Create the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "scheduler = CyclicLR(optimizer, base_lr=0., max_lr=lr, step_size_up=1, step_size_down=len(train_dl)*n_epochs, cycle_momentum=False)\n",
        "\n",
        "# Train\n",
        "losses, learning_rates = train(model, train_dl, valid_dl, optimizer, n_epochs, device, scheduler)\n",
        "model.load_state_dict(torch.load('best_model'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378,
          "referenced_widgets": [
            "1d5ac87780f74cccb41586a36a4d236e",
            "5ac0b3cac8314a09b3eb55d594f63435",
            "58acea9497f84f9e8d4eed7b4ef4a193",
            "b1354b420d1c442d94aefafaa5f7e998",
            "286082eb6c2942e981a3fbf2f0aa180e",
            "bd4bc6c49e6f4c068477c60cdfa6353e",
            "50e95382d8a94ce28e92a4f916793783",
            "5d2f7c0de13247cf8d05c7dc1f0bcb5f",
            "948ead86ab054f8393e7be06206ec584",
            "d6caeb43068e4925bd2f2f77b4640fea",
            "c2c5e2a55ada4d6cb1cc2d59d4ec6753"
          ]
        },
        "id": "jGum441qNys9",
        "outputId": "b21a25bc-8bae-4aa9-e8d7-5a388c5bcfee"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/924 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d5ac87780f74cccb41586a36a4d236e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-2ad693c95fcb>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-114-566c1086cee8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, optimizer, n_epochs, device, scheduler)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#Iterate through each batch in the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;31m# VERY IMPORTANT: Make sure the model is in training mode, which turns on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# things like dropout and layer normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-8864c5f598cd>\u001b[0m in \u001b[0;36mcollate_batch_bilstm\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Make sure each sample is max_length long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 2 (got 2)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d5ac87780f74cccb41586a36a4d236e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ac0b3cac8314a09b3eb55d594f63435",
              "IPY_MODEL_58acea9497f84f9e8d4eed7b4ef4a193",
              "IPY_MODEL_b1354b420d1c442d94aefafaa5f7e998"
            ],
            "layout": "IPY_MODEL_286082eb6c2942e981a3fbf2f0aa180e"
          }
        },
        "5ac0b3cac8314a09b3eb55d594f63435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd4bc6c49e6f4c068477c60cdfa6353e",
            "placeholder": "​",
            "style": "IPY_MODEL_50e95382d8a94ce28e92a4f916793783",
            "value": "  0%"
          }
        },
        "58acea9497f84f9e8d4eed7b4ef4a193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d2f7c0de13247cf8d05c7dc1f0bcb5f",
            "max": 924,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_948ead86ab054f8393e7be06206ec584",
            "value": 0
          }
        },
        "b1354b420d1c442d94aefafaa5f7e998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6caeb43068e4925bd2f2f77b4640fea",
            "placeholder": "​",
            "style": "IPY_MODEL_c2c5e2a55ada4d6cb1cc2d59d4ec6753",
            "value": " 0/924 [00:00&lt;?, ?it/s]"
          }
        },
        "286082eb6c2942e981a3fbf2f0aa180e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd4bc6c49e6f4c068477c60cdfa6353e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50e95382d8a94ce28e92a4f916793783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d2f7c0de13247cf8d05c7dc1f0bcb5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "948ead86ab054f8393e7be06206ec584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6caeb43068e4925bd2f2f77b4640fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2c5e2a55ada4d6cb1cc2d59d4ec6753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
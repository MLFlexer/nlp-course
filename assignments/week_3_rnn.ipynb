{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/emmastoklundlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "# enabling inline plots in Jupyter\n",
    "%matplotlib inline\n",
    "datasets.logging.set_verbosity_error()\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (4.33.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: filelock in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: requests in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915faf1b0d814c25abd458e1c74ce89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language   \n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish  \\\n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations   \n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...  \\\n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext   \n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...  \\\n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary column where if the question is answered it is equal to 1, and if not answerable it is 0\n",
    "def check_annotations(annotation):\n",
    "    return annotation == {'answer_start': [-1], 'answer_text': ['']}\n",
    "\n",
    "df_train['correct_answer'] = df_train['annotations'].apply(check_annotations)\n",
    "df_train['correct_answer'] = (~df_train['correct_answer']).astype(int)\n",
    "\n",
    "df_val['correct_answer'] = df_val['annotations'].apply(check_annotations)\n",
    "df_val['correct_answer'] = (~df_val['correct_answer']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bengali_document = df_train[df_train['language'] == 'bengali'][\"document_plaintext\"]\n",
    "df_train_arab_document = df_train[df_train['language'] == 'arabic'][\"document_plaintext\"]\n",
    "df_train_indonesian_document = df_train[df_train['language'] == 'indonesian'][\"document_plaintext\"]\n",
    "df_train_indonesian_document.head()\n",
    "\n",
    "df_train_english_document = df_train[df_train['language'] == 'english'][\"document_plaintext\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the documents\n",
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
    "\n",
    "# Tokinize train document_plaintext\n",
    "tokenize(df_train_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "# Tokinize validation document_plaintext\n",
    "tokenize(df_val_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "\n",
    "# For testing\n",
    "tokenize(df_train_english, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_english, \"document_plaintext\", mbert_tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# added in tokenization of the questions\n",
    "# Tokinize train question_text\n",
    "tokenize(df_train_bengali, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_train_arabic, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_train_indonesian, \"question_text\", mbert_tokeniser)\n",
    "\n",
    "# Tokinize validation question_text\n",
    "tokenize(df_val_bengali, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_val_arabic, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_val_indonesian, \"question_text\", mbert_tokeniser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/179117980.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# For testing tokenize in english\n",
    "tokenize(df_train_english, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_val_english, \"question_text\", mbert_tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data document_plaintext tokenized\n",
    "document_plaintext_tokenized_bengali = list(df_train_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_arabic = list(df_train_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_indonesian = list(df_train_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "# Validation data document_plaintext tokenized\n",
    "document_plaintext_tokenized_val_bengali = list(df_val_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_arabic = list(df_val_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_indonesian = list(df_val_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "# Training data question_text tokenized\n",
    "question_text_tokenized_bengali = list(df_train_bengali[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_arabic = list(df_train_arabic[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_indonesian = list(df_train_indonesian[\"question_text_tokenized\"].explode())\n",
    "\n",
    "# Validation data question_text tokenized\n",
    "question_text_tokenized_val_bengali = list(df_val_bengali[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_val_arabic = list(df_val_arabic[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_val_indonesian = list(df_val_indonesian[\"question_text_tokenized\"].explode())\n",
    "\n",
    "# for testing in english\n",
    "document_plaintext_tokenized_english = list(df_train_english[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_english = list(df_val_english[\"document_plaintext_tokenized\"].explode())\n",
    "question_text_tokenized_english = list(df_train_english[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_val_english = list(df_val_english[\"question_text_tokenized\"].explode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_34212/528443990.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_english['text_tokenized'] = df_train_english['document_plaintext_tokenized'] + df_train_english['question_text_tokenized']\n"
     ]
    }
   ],
   "source": [
    "df_train_english['text_tokenized'] = df_train_english['document_plaintext_tokenized'] + df_train_english['question_text_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenized_text = list(df_train_english['text_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "      <th>correct_answer</th>\n",
       "      <th>document_plaintext_tokenized</th>\n",
       "      <th>question_text_tokenized</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>When was quantum field theory developed?</td>\n",
       "      <td>Quantum field theory</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [159], 'answer_text': ['1920s']}</td>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Quantum%20field%...</td>\n",
       "      <td>1</td>\n",
       "      <td>[quantum, field, theory, naturally, began, wit...</td>\n",
       "      <td>[when, was, quantum, field, theory, developed, ?]</td>\n",
       "      <td>[quantum, field, theory, naturally, began, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Who was the first Nobel prize winner for Liter...</td>\n",
       "      <td>List of Nobel laureates in Literature</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [610], 'answer_text': ['Sully...</td>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20Nobe...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, nobel, prize, in, literature, (, swedish...</td>\n",
       "      <td>[who, was, the, first, nobel, prize, winner, f...</td>\n",
       "      <td>[the, nobel, prize, in, literature, (, swedish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>When is the dialectical method used?</td>\n",
       "      <td>Dialectic</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [129], 'answer_text': ['disco...</td>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dialectic</td>\n",
       "      <td>1</td>\n",
       "      <td>[dialect, ##ic, or, dialect, ##ics, (, greek, ...</td>\n",
       "      <td>[when, is, the, dialect, ##ical, method, used, ?]</td>\n",
       "      <td>[dialect, ##ic, or, dialect, ##ics, (, greek, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Who invented Hangul?</td>\n",
       "      <td>Origin of Hangul</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [88], 'answer_text': ['Sejong...</td>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Origin%20of%20Ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hangul, was, personally, created, and, promu,...</td>\n",
       "      <td>[who, invented, hangul, ?]</td>\n",
       "      <td>[hangul, was, personally, created, and, promu,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>What do Grasshoppers eat?</td>\n",
       "      <td>Grasshopper</td>\n",
       "      <td>english</td>\n",
       "      <td>{'answer_start': [0], 'answer_text': ['Grassho...</td>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Grasshopper</td>\n",
       "      <td>1</td>\n",
       "      <td>[grasshoppers, are, plant, -, eat, ##ers, ,, w...</td>\n",
       "      <td>[what, do, grasshoppers, eat, ?]</td>\n",
       "      <td>[grasshoppers, are, plant, -, eat, ##ers, ,, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         question_text   \n",
       "26            When was quantum field theory developed?  \\\n",
       "43   Who was the first Nobel prize winner for Liter...   \n",
       "112               When is the dialectical method used?   \n",
       "123                               Who invented Hangul?   \n",
       "125                          What do Grasshoppers eat?   \n",
       "\n",
       "                            document_title language   \n",
       "26                    Quantum field theory  english  \\\n",
       "43   List of Nobel laureates in Literature  english   \n",
       "112                              Dialectic  english   \n",
       "123                       Origin of Hangul  english   \n",
       "125                            Grasshopper  english   \n",
       "\n",
       "                                           annotations   \n",
       "26   {'answer_start': [159], 'answer_text': ['1920s']}  \\\n",
       "43   {'answer_start': [610], 'answer_text': ['Sully...   \n",
       "112  {'answer_start': [129], 'answer_text': ['disco...   \n",
       "123  {'answer_start': [88], 'answer_text': ['Sejong...   \n",
       "125  {'answer_start': [0], 'answer_text': ['Grassho...   \n",
       "\n",
       "                                    document_plaintext   \n",
       "26   Quantum field theory naturally began with the ...  \\\n",
       "43   The Nobel Prize in Literature (Swedish: Nobelp...   \n",
       "112  Dialectic or dialectics (Greek: διαλεκτική, di...   \n",
       "123  Hangul was personally created and promulgated ...   \n",
       "125  Grasshoppers are plant-eaters, with a few spec...   \n",
       "\n",
       "                                          document_url  correct_answer   \n",
       "26   https://en.wikipedia.org/wiki/Quantum%20field%...               1  \\\n",
       "43   https://en.wikipedia.org/wiki/List%20of%20Nobe...               1   \n",
       "112            https://en.wikipedia.org/wiki/Dialectic               1   \n",
       "123  https://en.wikipedia.org/wiki/Origin%20of%20Ha...               1   \n",
       "125          https://en.wikipedia.org/wiki/Grasshopper               1   \n",
       "\n",
       "                          document_plaintext_tokenized   \n",
       "26   [quantum, field, theory, naturally, began, wit...  \\\n",
       "43   [the, nobel, prize, in, literature, (, swedish...   \n",
       "112  [dialect, ##ic, or, dialect, ##ics, (, greek, ...   \n",
       "123  [hangul, was, personally, created, and, promu,...   \n",
       "125  [grasshoppers, are, plant, -, eat, ##ers, ,, w...   \n",
       "\n",
       "                               question_text_tokenized   \n",
       "26   [when, was, quantum, field, theory, developed, ?]  \\\n",
       "43   [who, was, the, first, nobel, prize, winner, f...   \n",
       "112  [when, is, the, dialect, ##ical, method, used, ?]   \n",
       "123                         [who, invented, hangul, ?]   \n",
       "125                   [what, do, grasshoppers, eat, ?]   \n",
       "\n",
       "                                        text_tokenized  \n",
       "26   [quantum, field, theory, naturally, began, wit...  \n",
       "43   [the, nobel, prize, in, literature, (, swedish...  \n",
       "112  [dialect, ##ic, or, dialect, ##ics, (, greek, ...  \n",
       "123  [hangul, was, personally, created, and, promu,...  \n",
       "125  [grasshoppers, are, plant, -, eat, ##ers, ,, w...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_english.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of correct answers in df_train_english\n",
    "# labels for whether the question could be answered or not\n",
    "correct_answer_train_english = list(df_train_english[\"correct_answer\"])\n",
    "correct_answer_val_english = list(df_val_english[\"correct_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating the full list of vocabulary in the tweet_eval data\n",
    "total_vocabulary = set()\n",
    "for document in document_plaintext_tokenized_english + document_plaintext_tokenized_val_english + question_text_tokenized_english + question_text_tokenized_val_english:\n",
    "    total_vocabulary.add(document.lower())\n",
    "total_vocabulary = sorted(list(total_vocabulary))\n",
    "\n",
    "# appending an empty padding token at the beginning of the vocabulary\n",
    "total_vocabulary = [\"\"]+total_vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.577384266993352 % of tokens are out of vocabulary\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_embedding_matrix(tokens, embedding):\n",
    "    \"\"\"creates an embedding matrix from pre-trained embeddings for a new vocabulary. It also adds an extra vector\n",
    "    vector of zeroes in row 0 to embed the padding token, and initializes missing tokens as vectors of 0s\"\"\"\n",
    "    oov = set()\n",
    "    size = embedding.vector_size\n",
    "    # note the extra zero vector that will used for padding\n",
    "    embedding_matrix=np.zeros((len(tokens),size))\n",
    "    c = 0\n",
    "    for i in range(1,len(tokens)):\n",
    "        try:\n",
    "            embedding_matrix[i]=embedding[tokens[i]]\n",
    "        except KeyError: #to catch the words missing in the embeddings\n",
    "            try:\n",
    "                embedding_matrix[i]=embedding[tokens[i].lower()]\n",
    "            except KeyError:\n",
    "                #if the token does not have an embedding, we initialize it as a vector of 0s\n",
    "                embedding_matrix[i] = np.zeros(size)\n",
    "                #we keep track of the out of vocabulary tokens\n",
    "                oov.add(tokens[i])\n",
    "                c +=1\n",
    "    print(f'{c/len(tokens)*100} % of tokens are out of vocabulary')\n",
    "    return embedding_matrix, oov\n",
    "\n",
    "# load the pretrained embeddings (these can be used as the embedding argument in create_embedding_matrix)\n",
    "# look into other gloves - is glove_twitter_25 the best?\n",
    "glove = gensim.downloader.load('glove-wiki-gigaword-100') \n",
    "\n",
    "#get the embedding matrix and out of vocabulary words for our tweet_eval vocabulary\n",
    "embedding_matrix, oov = create_embedding_matrix(total_vocabulary, glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(text, total_vocabulary):\n",
    "    \"\"\"Turns the input text (one tweet) into a vector of indices in total_vocabulary that corresponds to the tokenized words in the input text\"\"\"\n",
    "    vocab_dict = {word: index for index, word in enumerate(total_vocabulary)}\n",
    "\n",
    "    # Initialize a list to store the encoded text\n",
    "    encoded_text = []\n",
    "\n",
    "    for t in text:\n",
    "        # Convert the token to lowercase to match the vocabulary\n",
    "        t_lower = t.lower()\n",
    "        if t_lower in vocab_dict:\n",
    "            # Use the dictionary to quickly find the index\n",
    "            encoded_text.append(vocab_dict[t_lower])\n",
    "\n",
    "    return encoded_text\n",
    "\n",
    "def add_padding(vector, max_length, padding_index):\n",
    "    \"\"\"adds copies of the padding token to make the input vector the max_length size, so that all inputs are the same length (the length of tweet with most words)\"\"\"\n",
    "    if len(vector) < max_length:\n",
    "        vector = [padding_index for _ in range(max_length-len(vector))] + vector\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DELETE AS DOES NOT WORK\n",
    "# train_english_features = [[text_to_indices(x, total_vocabulary) for x in document] for document in english_tokenized_text[0]]\n",
    "# val_english_features = [[text_to_indices(x, total_vocabulary) for x in document] for document in english_tokenized_text[0]]\n",
    "\n",
    "# longest_document = max(train_english_features+val_english_features, key=len)\n",
    "# max_length = len(longest_document)\n",
    "# padding_index = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feature vectors by applying the text_to_indices function to each\n",
    "train_english_features = [text_to_indices(x, total_vocabulary) for x in english_tokenized_text]\n",
    "val_english_features = [text_to_indices(x, total_vocabulary) for x in english_tokenized_text]\n",
    "\n",
    "longest_document = max(train_english_features+val_english_features, key=len)\n",
    "max_length = len(longest_document)\n",
    "padding_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the feature vectors by applying the add_padding function to each\n",
    "train_english_features = [add_padding(x, max_length, padding_index) for x in train_english_features]\n",
    "val_english_features = [add_padding(x, max_length, padding_index) for x in val_english_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuestionClassifierTrain(torch.utils.data.Dataset):\n",
    "    # defining the sources of the data\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.from_numpy(np.array(features))\n",
    "        self.y = torch.from_numpy(np.array(labels))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index].unsqueeze(0)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "\n",
    "data_train_english = QuestionClassifierTrain(train_english_features, correct_answer_train_english)\n",
    "data_val_english = QuestionClassifierTrain(val_english_features, correct_answer_val_english)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train_english, batch_size=64)\n",
    "val_loader = torch.utils.data.DataLoader(data_val_english, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the embedding step and RNN model\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1] #this will be the size of the input for the RNN\n",
    "        \n",
    "        #define the RNN itself \n",
    "        self.rnn = torch.nn.RNN(emb_dim, rnn_size, batch_first=True)\n",
    "        #set batch_first=True for your RNN layer\n",
    "        \n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.outputs = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        # The RNN returns two tensors: one representing the hidden states at all positions,\n",
    "        # and another representing only the final hidden states.\n",
    "        # In this many-to-one model, we only need the final hidden states.\n",
    "        all_states, final_state = self.rnn(encoded_inputs)\n",
    "        final_state = final_state.squeeze() #flatten to make sure it has the right dimensions for the next linear step\n",
    "        \n",
    "        # run the final state through the output layer\n",
    "        outputs = self.outputs(final_state)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(26937, 100, padding_idx=0)\n",
      "  (rnn): RNN(100, 100, batch_first=True)\n",
      "  (outputs): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = SimpleRNN(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "print(myRNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def training_loop(model, num_epochs):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            targets = targets.squeeze() #dependending on your torch version you might have to use targets = targets.squeeze().long()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad(): # for evaluation we don't backpropagate and update weights anymore\n",
    "        for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "            outputs = torch.softmax(model(inputs), 1 ) # apply softmax to get probabilities/logits\n",
    "            # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            # accumulating the predictions\n",
    "            predictions += indices.tolist()\n",
    "            # accumulating the true labels\n",
    "            labels += targets.tolist()\n",
    "    \n",
    "    acc = accuracy_score(predictions, labels)\n",
    "    print(f'Model accuracy: {acc}')\n",
    "    return acc, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.5597439453413244\n",
      "Epoch 2: loss 0.8771149357379382\n",
      "Epoch 3: loss 0.8947227676639912\n",
      "Model accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = SimpleRNN(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "myRNN = training_loop(myRNN, 3)\n",
    "acc, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced version supporting multiple types of RNN layers\n",
    "\n",
    "class RNN_or_LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix, type=\"RNN\"):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1]\n",
    "        \n",
    "        #remember the batch_first=True argument\n",
    "        if type == \"RNN\":\n",
    "            self.rnn = torch.nn.RNN(emb_dim, rnn_size, batch_first=True)\n",
    "        elif type == \"LSTM\":\n",
    "            self.rnn = torch.nn.LSTM(emb_dim, rnn_size, batch_first=True)   \n",
    "        else:\n",
    "            raise LookupError(\"Only RNN and LSTM are supported.\")\n",
    "        self.output = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "        #apply the RNN or LSTM\n",
    "        if type == \"RNN\":\n",
    "            all_states, final_state = self.rnn(encoded_inputs)\n",
    "        else:\n",
    "            # LSTM's output is different and needs to be treated differently, see documentation for details\n",
    "            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "        \n",
    "        # run the final states through the output layer\n",
    "        outputs = self.output(final_state.squeeze())\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.8187243733524971\n",
      "Epoch 2: loss 1.063502600416541\n",
      "Epoch 3: loss 0.8644104276237816\n",
      "Model accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "myLSTM = RNN_or_LSTM(rnn_size=100, n_classes=3, type='LSTM', embedding_matrix=embedding_matrix)\n",
    "\n",
    "\n",
    "\n",
    "myLSTM = training_loop(myLSTM, 3)\n",
    "acc, preds = evaluate(myLSTM, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bidirectional_RNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)\n",
    "        emb_dim = embedding_matrix.shape[1] #this will be the size of the input for the RNN\n",
    "        \n",
    "        #define the RNN itself \n",
    "        self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        #set batch_first=True for your RNN layer\n",
    "        \n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.fc_logits = torch.nn.Linear(2*rnn_size, n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        # NB: for a bidirectional RNN, the final state corresponds to the *last* token\n",
    "        # in the forward direction and the *first* token in the backward direction.\n",
    "        #Notice that we use torch.concat to concatenate the final states from the forward and backward directions\n",
    "        rnn_out, final_state = self.rnn(encoded_inputs)\n",
    "        final_states_combined = torch.cat([final_state[-2,:,:], final_state[-1,:,:]], dim=1)\n",
    "\n",
    "        # run the output through the final linear layer\n",
    "        outputslinear = self.fc_logits(final_states_combined)\n",
    "        return outputslinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.5578079082447873\n",
      "Epoch 2: loss 0.8844208673429514\n",
      "Epoch 3: loss 0.8676807398201319\n",
      "Model accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "biRNN = Bidirectional_RNN(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "biRNN = training_loop(biRNN, 3)\n",
    "acc, preds = evaluate(biRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look into ways of optimisation - will this fix the accuracy issue & the below does not work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "biRNN = Bidirectional_RNN(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "optimizer = optim.Adam(biRNN.parameters(), lr=0.001)  # You can adjust the learning rate (lr) as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_inputs, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}: loss {avg_loss:.4f}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m biRNN \u001b[39m=\u001b[39m training_loop(biRNN, \u001b[39m3\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m acc, preds \u001b[39m=\u001b[39m evaluate(biRNN, val_loader)\n",
      "Cell \u001b[0;32mIn[83], line 9\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients\u001b[39;00m\n\u001b[1;32m      8\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_inputs)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "biRNN = training_loop(biRNN, 3)\n",
    "acc, preds = evaluate(biRNN, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class BiLSTMNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic BiLSTM network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic BiLSTM network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
    "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(BiLSTMNetwork, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'bilstm': nn.LSTM(\n",
    "                pretrained_embeddings.shape[1],\n",
    "                lstm_dim,\n",
    "                1,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'cls': nn.Linear(2*lstm_dim, n_classes)\n",
    "        })\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
    "                     list(self.model['cls'].named_parameters())\n",
    "        for n,p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
    "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # Max pool along the last dimension\n",
    "        ff_in = self.dropout(torch.max(lstm_out, 1)[0])\n",
    "        # Some magic to get the last output of the BiLSTM for classification (b x 2*lstm_dim)\n",
    "        #ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
    "\n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.model['cls'](ff_in).view(-1, self.n_classes)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Xentropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'rnn_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m biLSTM \u001b[39m=\u001b[39m BiLSTMNetwork(rnn_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, n_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, embedding_matrix\u001b[39m=\u001b[39;49membedding_matrix)\n\u001b[1;32m      3\u001b[0m biLSTM \u001b[39m=\u001b[39m training_loop(biLSTM, \u001b[39m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m acc, preds \u001b[39m=\u001b[39m evaluate(biLSTM, val_loader)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'rnn_size'"
     ]
    }
   ],
   "source": [
    "biLSTM = BiLSTMNetwork(rnn_size=100, n_classes=3, embedding_matrix=embedding_matrix)\n",
    "\n",
    "biLSTM = training_loop(biLSTM, 3)\n",
    "acc, preds = evaluate(biLSTM, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Initialize BiLSTMNetwork\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m biLSTM \u001b[39m=\u001b[39m BiLSTMNetwork(embedding_matrix, lstm_dim \u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, dropout_prob\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m1\u001b[39;49m, n_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m biLSTM \u001b[39m=\u001b[39m training_loop(biLSTM, \u001b[39m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m acc, preds \u001b[39m=\u001b[39m evaluate(biLSTM, val_loader)\n",
      "Cell \u001b[0;32mIn[85], line 27\u001b[0m, in \u001b[0;36mBiLSTMNetwork.__init__\u001b[0;34m(self, pretrained_embeddings, lstm_dim, dropout_prob, n_classes)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39msuper\u001b[39m(BiLSTMNetwork, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     24\u001b[0m \u001b[39m# We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleDict({\n\u001b[0;32m---> 27\u001b[0m     \u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m: nn\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_embeddings, padding_idx\u001b[39m=\u001b[39;49mpretrained_embeddings\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m),\n\u001b[1;32m     28\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mbilstm\u001b[39m\u001b[39m'\u001b[39m: nn\u001b[39m.\u001b[39mLSTM(\n\u001b[1;32m     29\u001b[0m         pretrained_embeddings\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m     30\u001b[0m         lstm_dim,\n\u001b[1;32m     31\u001b[0m         \u001b[39m1\u001b[39m,\n\u001b[1;32m     32\u001b[0m         batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m         dropout\u001b[39m=\u001b[39mdropout_prob,\n\u001b[1;32m     34\u001b[0m         bidirectional\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     35\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m'\u001b[39m: nn\u001b[39m.\u001b[39mLinear(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mlstm_dim, n_classes)\n\u001b[1;32m     36\u001b[0m })\n\u001b[1;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes \u001b[39m=\u001b[39m n_classes\n\u001b[1;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39mdropout_prob)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/sparse.py:210\u001b[0m, in \u001b[0;36mEmbedding.from_pretrained\u001b[0;34m(cls, embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    181\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, embeddings, freeze\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding_idx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    182\u001b[0m                     max_norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm_type\u001b[39m=\u001b[39m\u001b[39m2.\u001b[39m, scale_grad_by_freq\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    183\u001b[0m                     sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Creates Embedding instance from given 2-dimensional FloatTensor.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \n\u001b[1;32m    186\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39m        tensor([[ 4.0000,  5.1000,  6.3000]])\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[39massert\u001b[39;00m embeddings\u001b[39m.\u001b[39;49mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \\\n\u001b[1;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEmbeddings parameter is expected to be 2-dimensional\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    212\u001b[0m     rows, cols \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39mshape\n\u001b[1;32m    213\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    214\u001b[0m         num_embeddings\u001b[39m=\u001b[39mrows,\n\u001b[1;32m    215\u001b[0m         embedding_dim\u001b[39m=\u001b[39mcols,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m         scale_grad_by_freq\u001b[39m=\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    222\u001b[0m         sparse\u001b[39m=\u001b[39msparse)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize BiLSTMNetwork\n",
    "biLSTM = BiLSTMNetwork(embedding_matrix, lstm_dim =100, dropout_prob= 0/1, n_classes=3)\n",
    "\n",
    "biLSTM = training_loop(biLSTM, 3)\n",
    "acc, preds = evaluate(biLSTM, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Experiment with a second recurrent layer to implement a deep (or stacked) RNN. This can be done using the parameters of [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

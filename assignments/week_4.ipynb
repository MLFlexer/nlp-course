{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Week 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move from binary classification to span-based QA, i.e., identifying the span in the document that answers the question, when it is answerable.\n",
    "Let k be the number of members in your group. Using the training data, implement k different sequence labellers for each of the three languages, which predict which tokens in a document are part of the answer to the correspond- ing question. Evaluate the sequence labellers on the respective validation sets, report and analyse the performance for each language and compare the scores across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bpemb in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: tqdm in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (4.65.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (0.1.99)\n",
      "Requirement already satisfied: requests in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (2.29.0)\n",
      "Requirement already satisfied: gensim in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (4.3.1)\n",
      "Requirement already satisfied: numpy in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim->bpemb) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim->bpemb) (6.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (2022.12.7)\n",
      "Requirement already satisfied: gensim in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (1.10.1)\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: setuptools in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (66.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.29.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: jinja2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.10.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: fasttext in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from fasttext) (2.11.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from fasttext) (66.0.0)\n",
      "Requirement already satisfied: numpy in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from fasttext) (1.24.3)\n",
      "Requirement already satisfied: datasets in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: multiprocess in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.17.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (2.29.0)\n",
      "Requirement already satisfied: xxhash in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: filelock in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sklearn in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (0.0.post5)\n"
     ]
    }
   ],
   "source": [
    "!pip install bpemb\n",
    "!pip install gensim\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install fasttext\n",
    "!pip install datasets\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import torch\n",
    "import random\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR, CyclicLR\n",
    "from typing import List, Tuple, AnyStr\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from datasets import load_dataset, load_metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn.functional as F\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_reproducibility(seed=42):\n",
    "    # Sets seed manually for both CPU and CUDA\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For atomic operations there is currently \n",
    "    # no simple way to enforce determinism, as\n",
    "    # the order of parallel operations is not known.\n",
    "    # CUDNN\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # System based\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "enforce_reproducibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/emmastoklundlee/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ab49a150b84ef6bf1d64289f971863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116067\n",
      "13325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language   \n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish  \\\n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations   \n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...  \\\n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext   \n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...  \\\n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
    "\n",
    "\n",
    "def answer_text(df):\n",
    "    # create new column with 1 if answerable, 0 if not answerable\n",
    "    df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
    "    # drop all rows with answerable = 0\n",
    "    df = df[df['answerable'] == 1]\n",
    "    # return answer_text from annotations\n",
    "    df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
    "    # create new column with answer_start converted to int\n",
    "    df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_7924/4111912176.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_7924/4111912176.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_7924/4111912176.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_7924/4111912176.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answerable'] = df['annotations'].apply(lambda x: 0 if x['answer_start'] == [-1] else 1)\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_7924/4111912176.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_text'] = df['annotations'].apply(lambda x: x['answer_text'][0])\n",
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_7924/4111912176.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['answer_start_int'] = df['annotations'].apply(lambda x: int(x['answer_start'][0]))\n"
     ]
    }
   ],
   "source": [
    "df_train_english = answer_text(df_train_english)\n",
    "df_val_english = answer_text(df_val_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_tokenize(df):\n",
    "    tokenize(df, 'answer_text', mbert_tokeniser)\n",
    "    tokenize(df, 'document_plaintext', mbert_tokeniser)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words_to_characters(df):\n",
    "    \"\"\"\n",
    "    Split tokenized words in a DataFrame into individual characters and save them in a new column.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing tokenized words.\n",
    "        word_column_name (str): The name of the column containing tokenized words.\n",
    "        new_column_name (str): The name of the new column to store individual characters.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    answer_text_char = []\n",
    "    document_text_char = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        answer_text = row['answer_text']\n",
    "        document_text = row['document_plaintext']\n",
    "        chars_ans = []\n",
    "        chars_doc = []\n",
    "\n",
    "        for word in answer_text:\n",
    "            chars_ans.extend(list(word))  # Split word into individual characters and extend the list\n",
    "        for word in document_text:\n",
    "            chars_doc.extend(list(word))\n",
    "        \n",
    "        answer_text_char.append(chars_ans)\n",
    "        document_text_char.append(chars_doc)\n",
    "\n",
    "    df['answer_text_char'] = answer_text_char\n",
    "    df['document_text_char'] = document_text_char\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_english = split_words_to_characters(df_train_english)\n",
    "df_val_english = split_words_to_characters(df_val_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return length of answer_text_tokenized\n",
    "def answer_length(df):\n",
    "    df['answer_length'] = df['answer_text_char'].apply(lambda x: len(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_english = answer_length(df_train_english)\n",
    "df_val_english = answer_length(df_val_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bio tags for document_plaintext_tokenized where B is index of answer_start_int and I is index of answer_start_int + answer_length, and other are 0\n",
    "def iob_tags(df):\n",
    "    df['iob_tags'] = df.apply(lambda x: ['0' if i < x['answer_start_int'] or i >= x['answer_start_int'] + x['answer_length'] else '1' if i == x['answer_start_int'] else '1' for i in range(len(x['document_text_char']))], axis=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_english = iob_tags(df_train_english)\n",
    "df_val_english = iob_tags(df_val_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display first row\n",
    "df_train_english['iob_tags'][26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bpemb import BPEmb\n",
    "\n",
    "# Load english model with 25k word-pieces\n",
    "bpemb_id= BPEmb(lang='eng', dim=100, vs=25000)\n",
    "\n",
    "# Assuming bpemb_id is your pre-trained word embeddings (e.g., fastText, Word2Vec, or GloVe)\n",
    "\n",
    "# Extract the embeddings and add a randomly initialized embedding for our extra [PAD] token\n",
    "pretrained_embeddings = np.concatenate([bpemb_id.emb.vectors, np.zeros(shape=(1, 100))], axis=0)\n",
    "\n",
    "# Extract the vocab and add an extra [PAD] token\n",
    "vocabulary = bpemb_id.emb.index_to_key + ['[PAD]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from the embeddings\n",
    "embedding_dict = {token: embedding for token, embedding in zip(vocabulary, pretrained_embeddings)}\n",
    "\n",
    "# Define a function to tokenize and embed text\n",
    "def tokenize_and_embed_text(df, embedding_dict):\n",
    "    tokenized_text_list = []\n",
    "\n",
    "    for document_text in df['document_text_char']:\n",
    "        # Tokenize the document text\n",
    "        tokens = [token for token in document_text]\n",
    "\n",
    "        # Map tokens to embeddings using the dictionary\n",
    "        token_embeddings = [embedding_dict.get(token, embedding_dict['[PAD]']) for token in tokens]\n",
    "\n",
    "        # Append the token embeddings to the list\n",
    "        tokenized_text_list.extend(token_embeddings)\n",
    "\n",
    "    # Return the sequence embeddings as a NumPy array\n",
    "    sequence_embedding = np.array(tokenized_text_list)\n",
    "    return sequence_embedding\n",
    "\n",
    "# Usage:\n",
    "sequence_embedding = tokenize_and_embed_text(df_train_english, embedding_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMCharacterLabeler(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            sequence_embedding: torch.tensor,  # Replace with sequence_embedding\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        super(BiLSTMCharacterLabeler, self).__init__()\n",
    "\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(sequence_embedding, padding_idx=sequence_embedding.shape[0] - 1),\n",
    "            'bilstm': nn.LSTM(\n",
    "                sequence_embedding.shape[1],\n",
    "                lstm_dim,\n",
    "                1,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'cls': nn.Linear(2 * lstm_dim, n_classes)\n",
    "        })\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + list(self.model['cls'].named_parameters())\n",
    "        for n, p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels=None):\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        lstm_out, _ = self.model['bilstm'](lstm_in)\n",
    "\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        ff_in = self.dropout(torch.max(lstm_out, 1)[0])\n",
    "\n",
    "        logits = self.model['cls'](ff_in).view(-1, self.n_classes)\n",
    "\n",
    "        outputs = (logits,)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"cuda available\")\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = BiLSTMCharacterLabeler(\n",
    "    sequence_embedding=torch.FloatTensor(sequence_embedding),  # Use sequence_embedding instead of pretrained_embeddings\n",
    "    lstm_dim=100,\n",
    "    dropout_prob=0.1,\n",
    "    n_classes=2\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "  logits = np.asarray(logits).reshape(-1, len(logits[0]))\n",
    "  labels = np.asarray(labels).reshape(-1)\n",
    "  return np.sum(np.argmax(logits, axis=-1) == labels).astype(np.float32) / float(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
    "  \"\"\"\n",
    "  Evaluates the model on the given dataset\n",
    "  :param model: The model under evaluation\n",
    "  :param valid_dl: A `DataLoader` reading validation data\n",
    "  :return: The accuracy of the model on the dataset\n",
    "  \"\"\"\n",
    "  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n",
    "  # layer normalization and dropout\n",
    "  model.eval()\n",
    "  labels_all = []\n",
    "  logits_all = []\n",
    "\n",
    "  # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      seq_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      _, logits = model(input_ids, seq_lens, labels=labels)\n",
    "      labels_all.extend(list(labels.detach().cpu().numpy()))\n",
    "      logits_all.extend(list(logits.detach().cpu().numpy()))\n",
    "    acc = accuracy(logits_all, labels_all)\n",
    "\n",
    "    return acc,labels_all,logits_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dl: DataLoader,\n",
    "    valid_dl: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    n_epochs: int,\n",
    "    device: torch.device,\n",
    "    patience: int = 10\n",
    "):\n",
    "  \"\"\"\n",
    "  The main training loop which will optimize a given model on a given dataset\n",
    "  :param model: The model being optimized\n",
    "  :param train_dl: The training dataset\n",
    "  :param valid_dl: A validation dataset\n",
    "  :param optimizer: The optimizer used to update the model parameters\n",
    "  :param n_epochs: Number of epochs to train for\n",
    "  :param device: The device to train on\n",
    "  :return: (model, losses) The best model and the losses per iteration\n",
    "  \"\"\"\n",
    "\n",
    "  # Keep track of the loss and best accuracy\n",
    "  losses = []\n",
    "  best_acc = 0.0\n",
    "  pcounter = 0\n",
    "\n",
    "  # Iterate through epochs\n",
    "  for ep in range(n_epochs):\n",
    "\n",
    "    loss_epoch = []\n",
    "\n",
    "    #Iterate through each batch in the dataloader\n",
    "    for batch in tqdm(train_dl):\n",
    "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n",
    "      # things like dropout and layer normalization\n",
    "      model.train()\n",
    "\n",
    "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
    "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
    "      # zero them out\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Place each tensor on the GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      seq_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      # Pass the inputs through the model, get the current loss and logits\n",
    "      loss, logits = model(input_ids, seq_lens, labels=labels)\n",
    "      losses.append(loss.item())\n",
    "      loss_epoch.append(loss.item())\n",
    "\n",
    "      # Calculate all of the gradients and weight updates for the model\n",
    "      loss.backward()\n",
    "\n",
    "      # Optional: clip gradients\n",
    "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      # Finally, update the weights of the model\n",
    "      optimizer.step()\n",
    "      #gc.collect()\n",
    "\n",
    "    # Perform inline evaluation at the end of the epoch\n",
    "    acc,_,_ = evaluate(model, valid_dl)\n",
    "    print(f'Validation accuracy: {acc}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
    "\n",
    "    # Keep track of the best model based on the accuracy\n",
    "    if acc > best_acc:\n",
    "      torch.save(model.state_dict(), 'best_model')\n",
    "      best_acc = acc\n",
    "      pcounter = 0\n",
    "    else:\n",
    "      pcounter += 1\n",
    "      if pcounter == patience:\n",
    "        break\n",
    "        #gc.collect()\n",
    "\n",
    "  model.load_state_dict(torch.load('best_model'))\n",
    "  return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyperparameters\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_to_batch_bilstm(text: List, tokenizer, max_len=512) -> Tuple[List, List]:\n",
    "#     \"\"\"\n",
    "#     Creates a tokenized batch for input to a bilstm model\n",
    "#     :param text: A list of sentences to tokenize\n",
    "#     :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
    "#     :return: Tokenized text as well as the length of the input sequence\n",
    "#     \"\"\"\n",
    "#     # Some light preprocessing\n",
    "#     input_ids = [tokenizer.encode_ids_with_eos(t)[:max_len] for t in text]\n",
    "\n",
    "#     return input_ids, [len(ids) for ids in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_batch_bilstm(batch):\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param batch: List of data samples, where each sample is a dictionary with 'input' and 'label' keys\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    input_data = [sample['input'] for sample in batch]\n",
    "    labels = [sample['label'] for sample in batch]\n",
    "\n",
    "    max_length = max([len(seq) for seq in input_data])\n",
    "\n",
    "    # Pad all of the input samples to the max length (25000 is the ID of the [PAD] token)\n",
    "    input_ids = [seq + [25000] * (max_length - len(seq)) for seq in input_data]\n",
    "\n",
    "    # Make sure each sample is max_length long\n",
    "    assert all(len(seq) == max_length for seq in input_ids)\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # This will load the dataset and process it lazily in the __getitem__ function\n",
    "# class ClassificationDatasetReader(Dataset):\n",
    "#   def __init__(self, df, tokenizer):\n",
    "#     self.df = df\n",
    "#     self.tokenizer = tokenizer\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.df)\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     row = self.df.values[idx]\n",
    "#     # Calls the text_to_batch function\n",
    "#     input_ids,seq_lens = text_to_batch_bilstm([row[0]], self.tokenizer)\n",
    "#     label = row[1]\n",
    "#     return input_ids, seq_lens, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, text_column_name, label_column_name):\n",
    "        self.data = df[text_column_name].tolist()\n",
    "        self.labels = df[label_column_name].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'input': self.data[idx],  # Replace with your input data (sequence_embeddings)\n",
    "            'label': self.labels[idx]  # Replace with your labels\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_7924/3271786820.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_dl):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8330a32e981540179e3d884eb8769bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[239], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m valid_dl \u001b[39m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, collate_fn\u001b[39m=\u001b[39mcollate_batch_bilstm)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m model, losses \u001b[39m=\u001b[39m train(model, train_dl, valid_dl, optimizer, n_epochs, device)\n",
      "Cell \u001b[0;32mIn[228], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dl, valid_dl, optimizer, n_epochs, device, patience)\u001b[0m\n\u001b[1;32m     29\u001b[0m loss_epoch \u001b[39m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[39m#Iterate through each batch in the dataloader\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_dl):\n\u001b[1;32m     33\u001b[0m   \u001b[39m# VERY IMPORTANT: Make sure the model is in training mode, which turns on\u001b[39;00m\n\u001b[1;32m     34\u001b[0m   \u001b[39m# things like dropout and layer normalization\u001b[39;00m\n\u001b[1;32m     35\u001b[0m   model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     37\u001b[0m   \u001b[39m# VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\u001b[39;00m\n\u001b[1;32m     38\u001b[0m   \u001b[39m# keeps track of these dynamically in its computation graph so you need to explicitly\u001b[39;00m\n\u001b[1;32m     39\u001b[0m   \u001b[39m# zero them out\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "Cell \u001b[0;32mIn[238], line 20\u001b[0m, in \u001b[0;36mcollate_batch_bilstm\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Make sure each sample is max_length long\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39mlen\u001b[39m(seq) \u001b[39m==\u001b[39m max_length \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m input_ids)\n\u001b[0;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mtensor(input_ids), torch\u001b[39m.\u001b[39mtensor(labels)\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Define your model, optimizer, and other hyperparameters\n",
    "# Create the model\n",
    "model = BiLSTMCharacterLabeler(\n",
    "    sequence_embedding=torch.FloatTensor(pretrained_embeddings),  # Use sequence_embedding instead of pretrained_embeddings\n",
    "    lstm_dim=100,\n",
    "    dropout_prob=0.1,\n",
    "    n_classes=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "n_epochs = 10  # Define the number of training epochs\n",
    "batch_size = 32  # Define your batch size\n",
    "\n",
    "# Create the dataset readers\n",
    "# Assuming 'document_text_char' and 'iob_tags' are columns in your DataFrame\n",
    "# Assuming 'document_text_char' is your text data and 'iob_tags' is your label data\n",
    "train_dataset = CustomDataset(df_train_english[:1000], 'document_text_char', 'iob_tags')\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm)\n",
    "\n",
    "# Assuming 'document_text_char' and 'iob_tags' are columns in your DataFrame\n",
    "val_dataset = CustomDataset(df_val_english[:1000], 'document_text_char', 'iob_tags')\n",
    "valid_dl = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_batch_bilstm)\n",
    "\n",
    "# Train\n",
    "model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m Adam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model, losses \u001b[39m=\u001b[39m train(model, train_dl, valid_dl, optimizer, n_epochs, device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split document_plaintext and answer text into words \n",
    "def split_words(df):\n",
    "    df['document_plaintext'] = df['document_plaintext'].apply(lambda x: x.split())\n",
    "    df['answer_text'] = df['answer_text'].apply(lambda x: x.split())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate through each row of df and create a dictionary for each row with document_text_char as key and iob_tags as value\n",
    "# def char_to_iob(df):\n",
    "#     df['char_to_iob'] = df.apply(lambda x: dict(zip(x['document_text_char'], x['iob_tags'])), axis=1)\n",
    "#     return df\n",
    "\n",
    "# convert iob_tags list to string\n",
    "def iob_tags_to_string(df):\n",
    "    df['iob_tags'] = df['iob_tags'].apply(lambda x: ' '.join(x))\n",
    "    return df\n",
    "\n",
    "def iob_to_char(df):\n",
    "    # Initialize an empty dictionary\n",
    "    result_dict = {}\n",
    "    \n",
    "    # Iterate through each row of the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        iob_tags = row['iob_tags']\n",
    "        char = row['document_text_char']\n",
    "        \n",
    "        # Check if iob_tags is already a key in the dictionary\n",
    "        if iob_tags not in result_dict:\n",
    "            result_dict[iob_tags] = []\n",
    "        \n",
    "        # Append the character to the list associated with the iob_tags key\n",
    "        result_dict[iob_tags].append(char)\n",
    "    \n",
    "    # Add the dictionary as a new column in the DataFrame\n",
    "    df['iob_to_char'] = df['iob_tags'].map(result_dict)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# def char_to_iob(df):\n",
    "#     # Initialize an empty dictionary\n",
    "#     result_dict = {}\n",
    "    \n",
    "#     # Iterate through each row of the DataFrame\n",
    "#     for _, row in df.iterrows():\n",
    "#         iob_tags = row['iob_tags']\n",
    "#         char = row['document_text_char']\n",
    "        \n",
    "#         # Check if iob_tags is already a key in the dictionary\n",
    "#         if char not in result_dict:\n",
    "#             result_dict[char] = []\n",
    "        \n",
    "#         # Append the character to the list associated with the iob_tags key\n",
    "#         result_dict[char].append(iob_tags)\n",
    "    \n",
    "#     # Add the dictionary as a new column in the DataFrame\n",
    "#     df['char_to_iob'] = df['document_text_char'].map(result_dict)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate through each row of df and create a dictionary for each row with iob_tags as key and document_text_char as value\n",
    "# def iob_to_char(df):\n",
    "#     df['iob_to_char'] = df.apply(lambda x: dict(zip(x['iob_tags'], x['document_text_char'])), axis=1)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# df_train_english = char_to_iob(df_train_english)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_train_english \u001b[39m=\u001b[39m iob_to_char(df_train_english)\n",
      "Cell \u001b[0;32mIn[122], line 21\u001b[0m, in \u001b[0;36miob_to_char\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     18\u001b[0m char \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mdocument_text_char\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[39m# Check if iob_tags is already a key in the dictionary\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mif\u001b[39;00m iob_tags \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m result_dict:\n\u001b[1;32m     22\u001b[0m     result_dict[iob_tags] \u001b[39m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[39m# Append the character to the list associated with the iob_tags key\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# df_train_english = char_to_iob(df_train_english)\n",
    "df_train_english = iob_to_char(df_train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q': 'O',\n",
       " 'u': 'O',\n",
       " 'a': 'O',\n",
       " 'n': 'O',\n",
       " 't': 'O',\n",
       " 'm': 'O',\n",
       " ' ': 'O',\n",
       " 'f': 'O',\n",
       " 'i': 'O',\n",
       " 'e': 'O',\n",
       " 'l': 'O',\n",
       " 'd': 'O',\n",
       " 'h': 'O',\n",
       " 'o': 'O',\n",
       " 'r': 'O',\n",
       " 'y': 'O',\n",
       " 'b': 'O',\n",
       " 'g': 'O',\n",
       " 'w': 'O',\n",
       " 's': 'I',\n",
       " 'c': 'O',\n",
       " ',': 'O',\n",
       " 'k': 'O',\n",
       " '1': 'O',\n",
       " '9': 'I',\n",
       " '2': 'I',\n",
       " '0': 'I',\n",
       " '.': 'O',\n",
       " '[': 'O',\n",
       " '8': 'O',\n",
       " ']': 'O',\n",
       " ':': 'O'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_english['char_to_iob'][26]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModelForSequenceClassification, TrainingArguments, Trainer\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m----> 4\u001b[0m      \u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, id2label\u001b[39m=\u001b[39mid2label, label2id\u001b[39m=\u001b[39mlabel2id\n\u001b[1;32m      5\u001b[0m  )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2label' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "     \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30d75c84a6b4a4291d8a9304913af92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ef6af4474e4b3b981b1a81d4d9ae67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4185638a9ea04ac390711fa6d879c24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801c68bfdaa448e695d3327a906e5d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2ee7407e7f4d129800a386d44a787d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e1447429ff4b4184ba581d159f7ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in df applying nlp to document_plaintext_tokenized\n",
    "def ner(df):\n",
    "    df['ner'] = df['document_plaintext'].apply(lambda x: nlp(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_english = split_words(df_train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train_english \u001b[39m=\u001b[39m ner(df_train_english)\n",
      "Cell \u001b[0;32mIn[110], line 3\u001b[0m, in \u001b[0;36mner\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mner\u001b[39m(df):\n\u001b[0;32m----> 3\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mner\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mdocument_plaintext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: nlp(x))\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/pandas/core/series.py:4626\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4517\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4518\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4521\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4522\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4523\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4524\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4525\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4624\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4625\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4626\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[110], line 3\u001b[0m, in \u001b[0;36mner.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mner\u001b[39m(df):\n\u001b[0;32m----> 3\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mner\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mdocument_plaintext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: nlp(x))\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:192\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    190\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39moffset_mapping\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m offset_mapping\n\u001b[0;32m--> 192\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[39m=\u001b[39m [output \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:111\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    110\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    112\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:112\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    111\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 112\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pipelines/base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    989\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 990\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    991\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    992\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pipelines/token_classification.py:218\u001b[0m, in \u001b[0;36mTokenClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(model_inputs\u001b[39m.\u001b[39mdata)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    221\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m: logits,\n\u001b[1;32m    222\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mspecial_tokens_mask\u001b[39m\u001b[39m\"\u001b[39m: special_tokens_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[1;32m    226\u001b[0m }\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1759\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1753\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[39m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1759\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1760\u001b[0m     input_ids,\n\u001b[1;32m   1761\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1762\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1763\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1764\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1765\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1766\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1767\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1768\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1769\u001b[0m )\n\u001b[1;32m   1771\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1773\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    612\u001b[0m         hidden_states,\n\u001b[1;32m    613\u001b[0m         attention_mask,\n\u001b[1;32m    614\u001b[0m         layer_head_mask,\n\u001b[1;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    617\u001b[0m         past_key_value,\n\u001b[1;32m    618\u001b[0m         output_attentions,\n\u001b[1;32m    619\u001b[0m     )\n\u001b[1;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    540\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    541\u001b[0m )\n\u001b[1;32m    542\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/pytorch_utils.py:247\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    465\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_train_english = ner(df_train_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

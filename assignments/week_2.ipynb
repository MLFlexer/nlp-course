{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 2\n",
    "\n",
    "Let k be the number of members in your group (k ∈ {1,2,3}). Implement k different7 language models for each of the three languages, separately for the questions and the documents (total k×3×2 language models), using the training data. Evaluate each of them on the validation data, report their performance and discuss the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language  \\\n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish   \n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext  \\\n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4779\n",
      "29598\n",
      "11394\n"
     ]
    }
   ],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "print(len(df_train_bengali))\n",
    "print(len(df_train_arabic))\n",
    "print(len(df_train_indonesian))\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bengali_document = df_train[df_train['language'] == 'bengali'][\"document_plaintext\"]\n",
    "df_train_arab_document = df_train[df_train['language'] == 'arabic'][\"document_plaintext\"]\n",
    "df_train_indonesian_document = df_train[df_train['language'] == 'indonesian'][\"document_plaintext\"]\n",
    "df_train_indonesian_document.head()\n",
    "\n",
    "df_train_english_document = df_train[df_train['language'] == 'english'][\"document_plaintext\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/ipykernel_9135/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the documents\n",
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
    "\n",
    "# Tokinize train document_plaintext\n",
    "tokenize(df_train_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "# Tokinize validation document_plaintext\n",
    "tokenize(df_val_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "\n",
    "# For testing\n",
    "tokenize(df_train_english, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_english, \"document_plaintext\", mbert_tokeniser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity UniformLM: 3623.9999999845454\n",
      "Arabic perplexity UniformLM: 11174.000000245973\n",
      "Indonesian perplexity UniformLM: 18974.000000269545\n"
     ]
    }
   ],
   "source": [
    "from statnlpbook.lm import NGramLM, UniformLM, inject_OOVs, perplexity, replace_OOVs\n",
    "\n",
    "# Training data document_plaintext tokenized\n",
    "document_plaintext_tokenized_bengali = list(df_train_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_arabic = list(df_train_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_indonesian = list(df_train_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "# Validation data document_plaintext tokenized\n",
    "document_plaintext_tokenized_val_bengali = list(df_val_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_arabic = list(df_val_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_indonesian = list(df_val_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "def perplexity_uniform(train, test):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = UniformLM(oov_vocab)\n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "print(\"Bengali perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali))\n",
    "print(\"Arabic perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic))\n",
    "print(\"Indonesian perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Unigram: 371.19288552880323\n",
      "Arabic perplexity Unigram: 751.8094261847106\n",
      "Indonesian perplexity Unigram: 1735.9898871981518\n"
     ]
    }
   ],
   "source": [
    "def perplexity_ngram(train, test, n):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = NGramLM(oov_train, n)\n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "print(\"Bengali perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Bigram: inf\n",
      "Arabic perplexity Bigram: inf\n",
      "Indonesian perplexity Bigram: inf\n"
     ]
    }
   ],
   "source": [
    "# BiGram TODO: BUG \n",
    "print(\"Bengali perplexity Bigram:\",perplexity_ngram(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 2))\n",
    "print(\"Arabic perplexity Bigram:\",perplexity_ngram(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 2))\n",
    "print(\"Indonesian perplexity Bigram:\",perplexity_ngram(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

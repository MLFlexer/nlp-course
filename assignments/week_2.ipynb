{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have editted the week 2 document that you had started and added in some things.\n",
    "\n",
    "First things are needs for all weeks exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.9/site-packages (4.33.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in ./.venv/lib/python3.9/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language  \\\n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish   \n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations  \\\n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext  \\\n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4779\n",
      "29598\n",
      "11394\n"
     ]
    }
   ],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "print(len(df_train_bengali))\n",
    "print(len(df_train_arabic))\n",
    "print(len(df_train_indonesian))\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bengali_document = df_train[df_train['language'] == 'bengali'][\"document_plaintext\"]\n",
    "df_train_arab_document = df_train[df_train['language'] == 'arabic'][\"document_plaintext\"]\n",
    "df_train_indonesian_document = df_train[df_train['language'] == 'indonesian'][\"document_plaintext\"]\n",
    "df_train_indonesian_document.head()\n",
    "\n",
    "df_train_english_document = df_train[df_train['language'] == 'english'][\"document_plaintext\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the documents\n",
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
    "\n",
    "# Tokinize train document_plaintext\n",
    "tokenize(df_train_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_train_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "# Tokinize validation document_plaintext\n",
    "tokenize(df_val_bengali, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_arabic, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_indonesian, \"document_plaintext\", mbert_tokeniser)\n",
    "\n",
    "\n",
    "# For testing\n",
    "tokenize(df_train_english, \"document_plaintext\", mbert_tokeniser)\n",
    "tokenize(df_val_english, \"document_plaintext\", mbert_tokeniser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have moved tokenization to here, and # it out where it was before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n",
      "/var/folders/96/51nz4vj53xj0q5n07407wbrr0000gq/T/com.apple.shortcuts.mac-helper/ipykernel_27449/3821323326.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, f'{key}_tokenized'] = [transformer_model.tokenize(row) for row in df[key]]\n"
     ]
    }
   ],
   "source": [
    "# added in tokenization of the questions\n",
    "# Tokinize train question_text\n",
    "tokenize(df_train_bengali, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_train_arabic, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_train_indonesian, \"question_text\", mbert_tokeniser)\n",
    "\n",
    "# Tokinize validation question_text\n",
    "tokenize(df_val_bengali, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_val_arabic, \"question_text\", mbert_tokeniser)\n",
    "tokenize(df_val_indonesian, \"question_text\", mbert_tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data document_plaintext tokenized\n",
    "document_plaintext_tokenized_bengali = list(df_train_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_arabic = list(df_train_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_indonesian = list(df_train_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "# Validation data document_plaintext tokenized\n",
    "document_plaintext_tokenized_val_bengali = list(df_val_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_arabic = list(df_val_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "document_plaintext_tokenized_val_indonesian = list(df_val_indonesian[\"document_plaintext_tokenized\"].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data question_text tokenized\n",
    "question_text_tokenized_bengali = list(df_train_bengali[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_arabic = list(df_train_arabic[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_indonesian = list(df_train_indonesian[\"question_text_tokenized\"].explode())\n",
    "\n",
    "# Validation data question_text tokenized\n",
    "question_text_tokenized_val_bengali = list(df_val_bengali[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_val_arabic = list(df_val_arabic[\"question_text_tokenized\"].explode())\n",
    "question_text_tokenized_val_indonesian = list(df_val_indonesian[\"question_text_tokenized\"].explode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 2\n",
    "\n",
    "Let k be the number of members in your group (k ∈ {1,2,3}). Implement k different7 language models for each of the three languages, separately for the questions and the documents (total k×3×2 language models), using the training data. Evaluate each of them on the validation data, report their performance and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, models created for the document texts. This includes: UniformLM, Unigram, Unigram with Laplace smoothing, Bigram with Laplace smoothing and Trigram with Laplace smoothing.\n",
    "\n",
    "Laplace smoothing was added to N-gram models where N>1 as an error occurred meaning that perplexity was infinite. **add explanation of why this was**\n",
    "\n",
    "**We only need to keep 3 of the following 5 models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity UniformLM: 3623.9999999845454\n",
      "Arabic perplexity UniformLM: 11174.000000245973\n",
      "Indonesian perplexity UniformLM: 18974.000000269545\n"
     ]
    }
   ],
   "source": [
    "from statnlpbook.lm import NGramLM, UniformLM, inject_OOVs, perplexity, replace_OOVs, LaplaceLM\n",
    "\n",
    "# DELETE - moved to above\n",
    "# # Training data document_plaintext tokenized\n",
    "# document_plaintext_tokenized_bengali = list(df_train_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_arabic = list(df_train_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_indonesian = list(df_train_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "# # Validation data document_plaintext tokenized\n",
    "# document_plaintext_tokenized_val_bengali = list(df_val_bengali[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_val_arabic = list(df_val_arabic[\"document_plaintext_tokenized\"].explode())\n",
    "# document_plaintext_tokenized_val_indonesian = list(df_val_indonesian[\"document_plaintext_tokenized\"].explode())\n",
    "\n",
    "def perplexity_uniform(train, test):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = UniformLM(oov_vocab)\n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "print(\"Bengali perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali))\n",
    "print(\"Arabic perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic))\n",
    "print(\"Indonesian perplexity UniformLM:\",perplexity_uniform(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Unigram: 371.19288552880323\n",
      "Arabic perplexity Unigram: 751.8094261847106\n",
      "Indonesian perplexity Unigram: 1735.9898871981518\n"
     ]
    }
   ],
   "source": [
    "def perplexity_ngram(train, test, n):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = NGramLM(oov_train, n)\n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "print(\"Bengali perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian perplexity Unigram:\",perplexity_ngram(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Unigram with Laplace smoothing: 371.6411272453043\n",
      "Arabic perplexity Unigram with Laplace smoothing: 751.5783613512319\n",
      "Indonesian perplexity Unigram with Laplace smoothing: 1729.8410811023289\n"
     ]
    }
   ],
   "source": [
    "# creating ngram model with laplace smoothing\n",
    "def perplexity_ngram_laplace(train, test, n):\n",
    "    oov_train = inject_OOVs(train)\n",
    "    oov_vocab = set(oov_train)\n",
    "    oov_test = replace_OOVs(oov_vocab, test)\n",
    "    oov_baseline = LaplaceLM(NGramLM(oov_train, n), alpha=1) # alpha should be set to 1 as we are adding 1 word? \n",
    "    return perplexity(oov_baseline,oov_test)\n",
    "\n",
    "# first comparing difference with adding laplace smoothing to unigram model\n",
    "print(\"Bengali perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Bigram with Laplace smoothing: 161.7756039883504\n",
      "Arabic perplexity Bigram with Laplace smoothing: 366.7034046492581\n",
      "Indonesian perplexity Bigram with Laplace smoothing: 2021.0868388849685\n"
     ]
    }
   ],
   "source": [
    "# BiGram with laplace smoothing, no longer has 'inf' bug/error \n",
    "print(\"Bengali perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 2))\n",
    "print(\"Arabic perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 2))\n",
    "print(\"Indonesian perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali perplexity Trigram with Laplace smoothing: 683.8866567672605\n",
      "Arabic perplexity Trigram with Laplace smoothing: 2098.1074792562567\n",
      "Indonesian perplexity Trigram with Laplace smoothing: 8404.6716095129\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_bengali, document_plaintext_tokenized_val_bengali, 3))\n",
    "print(\"Arabic perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_arabic, document_plaintext_tokenized_val_arabic, 3))\n",
    "print(\"Indonesian perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(document_plaintext_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, models are created for the question texts for all 5 models as outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali question perplexity UniformLM: 694.9999999976678\n",
      "Arabic question perplexity UniformLM: 2578.0000000642076\n",
      "Indonesian question perplexity UniformLM: 5437.999999995055\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali question perplexity UniformLM:\",perplexity_uniform(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali))\n",
    "print(\"Arabic question perplexity UniformLM:\",perplexity_uniform(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic))\n",
    "print(\"Indonesian question perplexity UniformLM:\",perplexity_uniform(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Unigram: 425.35365857669797\n",
      "Arabic questions perplexity Unigram: 1133.505444711856\n",
      "Indonesian questions perplexity Unigram: 990.4566121951657\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Unigram:\",perplexity_ngram(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic questions perplexity Unigram:\",perplexity_ngram(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian questions perplexity Unigram:\",perplexity_ngram(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Unigram with Laplace smoothing: 419.05258362912895\n",
      "Arabic questions perplexity Unigram with Laplace smoothing: 1103.461062556527\n",
      "Indonesian questions perplexity Unigram with Laplace smoothing: 945.9552934166811\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 1))\n",
    "print(\"Arabic questions perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 1))\n",
    "print(\"Indonesian questions perplexity Unigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Bigram with Laplace smoothing: 263.6057476078464\n",
      "Arabic questions perplexity Bigram with Laplace smoothing: 963.6267293858549\n",
      "Indonesian questions perplexity Bigram with Laplace smoothing: 2012.680873575219\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 2))\n",
    "print(\"Arabic questions perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 2))\n",
    "print(\"Indonesian questions perplexity Bigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bengali questions perplexity Trigram with Laplace smoothing: 454.84625413299256\n",
      "Arabic questions perplexity Trigram with Laplace smoothing: 1954.7181123047\n",
      "Indonesian questions perplexity Trigram with Laplace smoothing: 4219.730994551042\n"
     ]
    }
   ],
   "source": [
    "print(\"Bengali questions perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_bengali, document_plaintext_tokenized_val_bengali, 3))\n",
    "print(\"Arabic questions perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_arabic, document_plaintext_tokenized_val_arabic, 3))\n",
    "print(\"Indonesian questions perplexity Trigram with Laplace smoothing:\",perplexity_ngram_laplace(question_text_tokenized_indonesian, document_plaintext_tokenized_val_indonesian, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

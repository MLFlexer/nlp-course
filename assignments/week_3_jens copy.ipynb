{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Week 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let k be the number of members in your group. Implement and train k different supervised classifiers for each of the three languages separately, using the training data for that language. The classifiers must only use the document and question as input. Evaluate the classifiers on the respective validation sets, report and analyse the performance for each language and compare the scores across languages.\n",
    "\n",
    "The classifiers can use linguistic/lexical features, e.g., bag-of-words, n-gram counts, overlaps of words between question and document, etc.; word embed- dings, or word/sentence representations from neural language models. You can, for example, find pretrained Transformer language models for different languages, trained with different language objectives, and fine-tuned for differ- entdownstreamtasks,fromHuggingFace.9 Youcanalsotrainorfine-tuneyour own neural language models on the dataset. Motivate your choice of features and classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: bpemb in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: requests in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (2.29.0)\n",
      "Requirement already satisfied: tqdm in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (4.65.0)\n",
      "Requirement already satisfied: gensim in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (4.3.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (0.1.99)\n",
      "Requirement already satisfied: numpy in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from bpemb) (1.24.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim->bpemb) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim->bpemb) (1.10.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from requests->bpemb) (2022.12.7)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: gensim in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages (from gensim) (1.10.1)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install bpemb\n",
    "!pip install gensim\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "import sys \n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/Users/emmastoklundlee/.cache/huggingface/datasets/copenlu___parquet/copenlu--nlp_course_tydiqa-cceecfb5416d988a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3197ffe32674d7ca74a6d9f7ae37052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116067\n",
      "13325\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>language</th>\n",
       "      <th>annotations</th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Milloin Charles Fort syntyi?</td>\n",
       "      <td>Charles Fort</td>\n",
       "      <td>finnish</td>\n",
       "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
       "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
       "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
       "      <td>ダニエル・J・キャラハン</td>\n",
       "      <td>japanese</td>\n",
       "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
       "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
       "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
       "      <td>వేప</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
       "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
       "      <td>চেঙ্গিজ খান</td>\n",
       "      <td>bengali</td>\n",
       "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
       "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
       "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
       "      <td>రెయ్యలగడ్ద</td>\n",
       "      <td>telugu</td>\n",
       "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
       "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
       "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            question_text document_title  language   \n",
       "0            Milloin Charles Fort syntyi?   Charles Fort   finnish  \\\n",
       "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
       "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
       "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
       "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
       "\n",
       "                                         annotations   \n",
       "0  {'answer_start': [18], 'answer_text': ['6. elo...  \\\n",
       "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
       "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
       "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
       "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
       "\n",
       "                                  document_plaintext   \n",
       "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...  \\\n",
       "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
       "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
       "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
       "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
       "\n",
       "                                        document_url  \n",
       "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
       "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
       "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
       "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
       "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
    "\n",
    "train_set = dataset[\"train\"]\n",
    "validation_set = dataset[\"validation\"]\n",
    "\n",
    "df_train = train_set.to_pandas()\n",
    "df_val = validation_set.to_pandas()\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(df_val))\n",
    "\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation data for each language\n",
    "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
    "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
    "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
    "\n",
    "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
    "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
    "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
    "\n",
    "\n",
    "# For testing\n",
    "df_val_english = df_val[df_val['language'] == 'english']\n",
    "df_train_english = df_train[df_train['language'] == 'english']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>answerable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Wound care encourages and speeds wound healing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Brothers Amos and Wilfrid Ayre founded Burntis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>For species of mammals, larger brains (in abso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>As from 31 March 1989, fishing vessel registra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>When Quezon City was created in 1939, the foll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  answerable\n",
       "30   Wound care encourages and speeds wound healing...           1\n",
       "47   Brothers Amos and Wilfrid Ayre founded Burntis...           1\n",
       "59   For species of mammals, larger brains (in abso...           1\n",
       "77   As from 31 March 1989, fishing vessel registra...           1\n",
       "106  When Quezon City was created in 1939, the foll...           1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with the combined documents and questions and add if they are answerable\n",
    "df_train_bengali_merged = pd.DataFrame({\n",
    "    'text':(df_train_bengali[\"document_plaintext\"] + df_train_bengali[\"question_text\"]),\n",
    "    'answerable':(df_train_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_train_arabic_merged = pd.DataFrame({\n",
    "    'text': (df_train_arabic[\"document_plaintext\"] + df_train_arabic[\"question_text\"]),\n",
    "    'answerable': (df_train_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "                                    })\n",
    "df_train_indonesian_merged = pd.DataFrame({\n",
    "    'text':(df_train_indonesian[\"document_plaintext\"] + df_train_indonesian[\"question_text\"]),\n",
    "    'answerable':(df_train_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_train_english_merged = pd.DataFrame({\n",
    "    'text':(df_train_english[\"document_plaintext\"] + df_train_english[\"question_text\"]),\n",
    "    'answerable':(df_train_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "\n",
    "\n",
    "## Same for validation data\n",
    "df_val_bengali_merged = pd.DataFrame({\n",
    "    'text':(df_val_bengali[\"document_plaintext\"] + df_val_bengali[\"question_text\"]),\n",
    "    'answerable':(df_val_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_val_arabic_merged = pd.DataFrame({\n",
    "    'text': (df_val_arabic[\"document_plaintext\"] + df_val_arabic[\"question_text\"]),\n",
    "    'answerable': (df_val_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "                                    })\n",
    "df_val_indonesian_merged = pd.DataFrame({\n",
    "    'text':(df_val_indonesian[\"document_plaintext\"] + df_val_indonesian[\"question_text\"]),\n",
    "    'answerable':(df_val_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "df_val_english_merged = pd.DataFrame({\n",
    "    'text':(df_val_english[\"document_plaintext\"] + df_val_english[\"question_text\"]),\n",
    "    'answerable':(df_val_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
    "    })\n",
    "\n",
    "df_val_english_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the documents\n",
    "from transformers import AutoTokenizer\n",
    "mbert_tokeniser = AutoTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize(df, key, transformer_model):\n",
    "    # Create a new column to store the tokenized data\n",
    "    df[f'{key}_tokenized'] = df[key].apply(lambda row: transformer_model.tokenize(row))\n",
    "\n",
    "tokenize(df_train_bengali_merged, 'text', mbert_tokeniser)\n",
    "tokenize(df_train_arabic_merged, 'text', mbert_tokeniser)\n",
    "tokenize(df_train_indonesian_merged, 'text', mbert_tokeniser)\n",
    "tokenize(df_train_english_merged, 'text', mbert_tokeniser)\n",
    "\n",
    "tokenize(df_val_bengali_merged, 'text', mbert_tokeniser)\n",
    "tokenize(df_val_arabic_merged, 'text', mbert_tokeniser)\n",
    "tokenize(df_val_indonesian_merged, 'text', mbert_tokeniser)\n",
    "tokenize(df_val_english_merged, 'text', mbert_tokeniser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>answerable</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Quantum field theory naturally began with the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[quantum, field, theory, naturally, began, wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>The Nobel Prize in Literature (Swedish: Nobelp...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, nobel, prize, in, literature, (, swedish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Dialectic or dialectics (Greek: διαλεκτική, di...</td>\n",
       "      <td>1</td>\n",
       "      <td>[dialect, ##ic, or, dialect, ##ics, (, greek, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Hangul was personally created and promulgated ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hangul, was, personally, created, and, promu,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Grasshoppers are plant-eaters, with a few spec...</td>\n",
       "      <td>1</td>\n",
       "      <td>[grasshoppers, are, plant, -, eat, ##ers, ,, w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  answerable   \n",
       "26   Quantum field theory naturally began with the ...           1  \\\n",
       "43   The Nobel Prize in Literature (Swedish: Nobelp...           1   \n",
       "112  Dialectic or dialectics (Greek: διαλεκτική, di...           1   \n",
       "123  Hangul was personally created and promulgated ...           1   \n",
       "125  Grasshoppers are plant-eaters, with a few spec...           1   \n",
       "\n",
       "                                        text_tokenized  \n",
       "26   [quantum, field, theory, naturally, began, wit...  \n",
       "43   [the, nobel, prize, in, literature, (, swedish...  \n",
       "112  [dialect, ##ic, or, dialect, ##ics, (, greek, ...  \n",
       "123  [hangul, was, personally, created, and, promu,...  \n",
       "125  [grasshoppers, are, plant, -, eat, ##ers, ,, w...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_english_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INDONESIAN - Logistic Regression\n",
      "Accuracy: 0.7422334172963896\n",
      "Precision: 0.7799227799227799\n",
      "Recall: 0.6767169179229481\n",
      "F1: 0.7246636771300449\n",
      "\n",
      "BENGALI - Logistic Regression\n",
      "Accuracy: 0.7098214285714286\n",
      "Precision: 0.688\n",
      "Recall: 0.7678571428571429\n",
      "F1: 0.7257383966244725\n",
      "\n",
      "ARABIC - Logistic Regression\n",
      "Accuracy: 0.7923238696109358\n",
      "Precision: 0.8054945054945055\n",
      "Recall: 0.7707676130389064\n",
      "F1: 0.7877485222998388\n",
      "\n",
      "ENGLISH - Logistic Regression\n",
      "Accuracy: 0.7151515151515152\n",
      "Precision: 0.7186858316221766\n",
      "Recall: 0.7070707070707071\n",
      "F1: 0.7128309572301426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "## Train the model on the Bengali training data\n",
    "# select the input and label columns\n",
    "# Indonesian\n",
    "X_train_indonesian = df_train_indonesian_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_indonesian = df_train_indonesian_merged.iloc[:, 1].values\n",
    "\n",
    "# Bengali\n",
    "X_train_bengali = df_train_bengali_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_bengali = df_train_bengali_merged.iloc[:, 1].values\n",
    "\n",
    "#Arabic\n",
    "X_train_arabic = df_train_arabic_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_arabic = df_train_arabic_merged.iloc[:, 1].values\n",
    "\n",
    "# English\n",
    "X_train_english = df_train_english_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_train_english = df_train_english_merged.iloc[:, 1].values\n",
    "\n",
    "# Validation data\n",
    "# Indonesian\n",
    "X_val_indonesian = df_val_indonesian_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_indosnesian = df_val_indonesian_merged.iloc[:, 1].values\n",
    "\n",
    "# Bengali\n",
    "X_val_bengali = df_val_bengali_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_bengali = df_val_bengali_merged.iloc[:, 1].values\n",
    "\n",
    "#Arabic\n",
    "X_val_arabic = df_val_arabic_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_arabic = df_val_arabic_merged.iloc[:, 1].values\n",
    "\n",
    "# English\n",
    "X_val_english = df_val_english_merged.iloc[:, 0].values.reshape(-1, 1)\n",
    "y_val_english = df_val_english_merged.iloc[:, 1].values\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Indonesian\n",
    "X_train_indonesian_tokenized = vectorizer.fit_transform(X_train_indonesian.ravel())\n",
    "X_val_tokenized_indonesian = vectorizer.transform(X_val_indonesian.ravel())\n",
    "\n",
    "# Bengali\n",
    "X_train_bengali_tokenized = vectorizer.fit_transform(X_train_bengali.ravel())\n",
    "X_val_tokenized_bengali = vectorizer.transform(X_val_bengali.ravel())\n",
    "\n",
    "# Arabic\n",
    "X_train_arabic_tokenized = vectorizer.fit_transform(X_train_arabic.ravel())\n",
    "X_val_tokenized_arabic = vectorizer.transform(X_val_arabic.ravel())\n",
    "\n",
    "# English\n",
    "X_train_english_tokenized = vectorizer.fit_transform(X_train_english.ravel())\n",
    "X_val_tokenized_english = vectorizer.transform(X_val_english.ravel())\n",
    "\n",
    "# Create a logistic regression model\n",
    "model_indonesian = LogisticRegression()\n",
    "model_bengali = LogisticRegression()\n",
    "model_arabic = LogisticRegression()\n",
    "model_english = LogisticRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_indonesian.fit(X_train_indonesian_tokenized, y_train_indonesian)\n",
    "model_bengali.fit(X_train_bengali_tokenized, y_train_bengali)\n",
    "model_arabic.fit(X_train_arabic_tokenized, y_train_arabic)\n",
    "model_english.fit(X_train_english_tokenized, y_train_english)\n",
    "\n",
    "\n",
    "## Test the model on the validation data\n",
    "\n",
    "# Indonesian\n",
    "y_pred_indonesian = model_indonesian.predict(X_val_tokenized_indonesian)\n",
    "print()\n",
    "print(\"INDONESIAN - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Precision:\", precision_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Recall:\", recall_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"F1:\", f1_score(y_val_indosnesian, y_pred_indonesian))\n",
    "\n",
    "# Bengali\n",
    "y_pred_bengali = model_bengali.predict(X_val_tokenized_bengali)\n",
    "print()\n",
    "print(\"BENGALI - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Precision:\", precision_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Recall:\", recall_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"F1:\", f1_score(y_val_bengali, y_pred_bengali))\n",
    "\n",
    "# Arabic\n",
    "y_pred_arabic = model_arabic.predict(X_val_tokenized_arabic)\n",
    "print()\n",
    "print(\"ARABIC - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Precision:\", precision_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Recall:\", recall_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"F1:\", f1_score(y_val_arabic, y_pred_arabic))\n",
    "\n",
    "# English\n",
    "y_pred_english = model_english.predict(X_val_tokenized_english)\n",
    "print()\n",
    "print(\"ENGLISH - Logistic Regression\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_english, y_pred_english))\n",
    "print(\"Precision:\", precision_score(y_val_english, y_pred_english))\n",
    "print(\"Recall:\", recall_score(y_val_english, y_pred_english))\n",
    "print(\"F1:\", f1_score(y_val_english, y_pred_english))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INDONESIAN - Random Forest\n",
      "Accuracy: 0.7590260285474392\n",
      "Precision: 0.7913533834586466\n",
      "Recall: 0.7051926298157454\n",
      "F1: 0.745792736935341\n",
      "\n",
      "BENGALI - Random Forest\n",
      "Accuracy: 0.7053571428571429\n",
      "Precision: 0.7129629629629629\n",
      "Recall: 0.6875\n",
      "F1: 0.7\n",
      "\n",
      "ARABIC - Random Forest\n",
      "Accuracy: 0.7839116719242902\n",
      "Precision: 0.8096330275229358\n",
      "Recall: 0.7423764458464774\n",
      "F1: 0.7745474492594625\n",
      "\n",
      "ENGLISH - Random Forest\n",
      "Accuracy: 0.697979797979798\n",
      "Precision: 0.696\n",
      "Recall: 0.703030303030303\n",
      "F1: 0.699497487437186\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "model_indonesian = RandomForestClassifier()\n",
    "model_bengali = RandomForestClassifier()\n",
    "model_arabic = RandomForestClassifier()\n",
    "model_english = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to the data\n",
    "model_indonesian.fit(X_train_indonesian_tokenized, y_train_indonesian)\n",
    "model_bengali.fit(X_train_bengali_tokenized, y_train_bengali)\n",
    "model_arabic.fit(X_train_arabic_tokenized, y_train_arabic)\n",
    "model_english.fit(X_train_english_tokenized, y_train_english)\n",
    "\n",
    "# Evaluate the model\n",
    "# Indonesian\n",
    "y_pred_indonesian = model_indonesian.predict(X_val_tokenized_indonesian)\n",
    "print()\n",
    "print(\"INDONESIAN - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Precision:\", precision_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"Recall:\", recall_score(y_val_indosnesian, y_pred_indonesian))\n",
    "print(\"F1:\", f1_score(y_val_indosnesian, y_pred_indonesian))\n",
    "\n",
    "# Bengali\n",
    "y_pred_bengali = model_bengali.predict(X_val_tokenized_bengali)\n",
    "print()\n",
    "print(\"BENGALI - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Precision:\", precision_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"Recall:\", recall_score(y_val_bengali, y_pred_bengali))\n",
    "print(\"F1:\", f1_score(y_val_bengali, y_pred_bengali))\n",
    "\n",
    "# Arabic\n",
    "y_pred_arabic = model_arabic.predict(X_val_tokenized_arabic)\n",
    "print()\n",
    "print(\"ARABIC - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Precision:\", precision_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"Recall:\", recall_score(y_val_arabic, y_pred_arabic))\n",
    "print(\"F1:\", f1_score(y_val_arabic, y_pred_arabic))\n",
    "\n",
    "# English\n",
    "y_pred_english = model_english.predict(X_val_tokenized_english)\n",
    "print()\n",
    "print(\"ENGLISH - Random Forest\")\n",
    "print(\"Accuracy:\", accuracy_score(y_val_english, y_pred_english))\n",
    "print(\"Precision:\", precision_score(y_val_english, y_pred_english))\n",
    "print(\"Recall:\", recall_score(y_val_english, y_pred_english))\n",
    "print(\"F1:\", f1_score(y_val_english, y_pred_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocabulary_eng = set()\n",
    "\n",
    "# Iterate through the tokenized data and add valid strings to the vocabulary\n",
    "for document_tokens in df_train_english_merged['text_tokenized'] + df_val_english_merged['text_tokenized']:\n",
    "    if isinstance(document_tokens, list):  # Check if it's a list\n",
    "        for token in document_tokens:\n",
    "            if isinstance(token, str):  # Check if the token is a string\n",
    "                total_vocabulary_eng.add(token.lower())\n",
    "\n",
    "total_vocabulary_eng = sorted(list(total_vocabulary_eng))\n",
    "\n",
    "# Appending an empty padding token at the beginning of the vocabulary\n",
    "total_vocabulary_eng = [\"\"] + total_vocabulary_eng\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_vocabulary_eng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m glove \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mdownloader\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mglove-wiki-gigaword-100\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[1;32m     31\u001b[0m \u001b[39m#get the embedding matrix and out of vocabulary words for our tweet_eval vocabulary\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m embedding_matrix_eng, oov \u001b[39m=\u001b[39m create_embedding_matrix(total_vocabulary_eng, glove)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_vocabulary_eng' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(tokens, embedding):\n",
    "    \"\"\"creates an embedding matrix from pre-trained embeddings for a new vocabulary. It also adds an extra vector\n",
    "    vector of zeroes in row 0 to embed the padding token, and initializes missing tokens as vectors of 0s\"\"\"\n",
    "    oov = set()\n",
    "    size = embedding.vector_size\n",
    "    # note the extra zero vector that will used for padding\n",
    "    embedding_matrix=np.zeros((len(tokens),size))\n",
    "    c = 0\n",
    "    for i in range(1,len(tokens)):\n",
    "        try:\n",
    "            embedding_matrix[i]=embedding[tokens[i]]\n",
    "        except KeyError: #to catch the words missing in the embeddings\n",
    "            try:\n",
    "                embedding_matrix[i]=embedding[tokens[i].lower()]\n",
    "            except KeyError:\n",
    "                #if the token does not have an embedding, we initialize it as a vector of 0s\n",
    "                embedding_matrix[i] = np.zeros(size)\n",
    "                #we keep track of the out of vocabulary tokens\n",
    "                oov.add(tokens[i])\n",
    "                c +=1\n",
    "    print(f'{c/len(tokens)*100} % of tokens are out of vocabulary')\n",
    "    return embedding_matrix, oov\n",
    "\n",
    "# load the pretrained embeddings (these can be used as the embedding argument in create_embedding_matrix)\n",
    "# look into other gloves - is glove_twitter_25 the best?\n",
    "glove = gensim.downloader.load('glove-wiki-gigaword-100') \n",
    "\n",
    "#get the embedding matrix and out of vocabulary words for our tweet_eval vocabulary\n",
    "embedding_matrix_eng, oov = create_embedding_matrix(total_vocabulary_eng, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(text, total_vocabulary):\n",
    "    \"\"\"Turns the input text (one tweet) into a vector of indices in total_vocabulary that corresponds to the tokenized words in the input text\"\"\"\n",
    "    vocab_dict = {word: index for index, word in enumerate(total_vocabulary)}\n",
    "\n",
    "    # Initialize a list to store the encoded text\n",
    "    encoded_text = []\n",
    "\n",
    "    for t in text:\n",
    "        # Convert the token to lowercase to match the vocabulary\n",
    "        t_lower = t.lower()\n",
    "        if t_lower in vocab_dict:\n",
    "            # Use the dictionary to quickly find the index\n",
    "            encoded_text.append(vocab_dict[t_lower])\n",
    "\n",
    "    return encoded_text\n",
    "\n",
    "def add_padding(vector, max_length, padding_index):\n",
    "    \"\"\"adds copies of the padding token to make the input vector the max_length size, so that all inputs are the same length (the length of tweet with most words)\"\"\"\n",
    "    if len(vector) < max_length:\n",
    "        vector = [padding_index for _ in range(max_length-len(vector))] + vector\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the feature vectors by applying the text_to_indices function to each\n",
    "train_english_features = [text_to_indices(x, total_vocabulary_eng) for x in df_train_english_merged['text_tokenized']]\n",
    "val_english_features = [text_to_indices(x, total_vocabulary_eng) for x in df_val_english_merged['text_tokenized']]\n",
    "\n",
    "longest_document = max(train_english_features+val_english_features, key=len)\n",
    "max_length = len(longest_document)\n",
    "padding_index = 0\n",
    "\n",
    "# padding the feature vectors by applying the add_padding function to each\n",
    "train_english_features = [add_padding(x, max_length, padding_index) for x in train_english_features]\n",
    "val_english_features = [add_padding(x, max_length, padding_index) for x in val_english_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.[8]:1When was quantum field theory developed?'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     15\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my)\n\u001b[0;32m---> 18\u001b[0m data_train_english \u001b[39m=\u001b[39m QuestionClassifierTrain(X_train_english, y_train_english)\n\u001b[1;32m     19\u001b[0m data_val_english \u001b[39m=\u001b[39m QuestionClassifierTrain(X_val_english, y_val_english)\n\u001b[1;32m     21\u001b[0m train_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(data_train_english, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n",
      "Cell \u001b[0;32mIn[48], line 6\u001b[0m, in \u001b[0;36mQuestionClassifierTrain.__init__\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, features, labels):\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39;49marray(features, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m      7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39marray(labels, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32))\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Quantum field theory naturally began with the study of electromagnetic interactions, as the electromagnetic field was the only known classical field as of the 1920s.[8]:1When was quantum field theory developed?'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class QuestionClassifierTrain(torch.utils.data.Dataset):\n",
    "    # defining the sources of the data\n",
    "    def __init__(self, features, labels):\n",
    "        self.X = torch.from_numpy(np.array(features, dtype=np.float32))\n",
    "        self.y = torch.from_numpy(np.array(labels, dtype=np.float32))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index].unsqueeze(0)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "\n",
    "data_train_english = QuestionClassifierTrain(X_train_english, y_train_english)\n",
    "data_val_english = QuestionClassifierTrain(X_val_english, y_val_english)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train_english, batch_size=64)\n",
    "val_loader = torch.utils.data.DataLoader(data_val_english, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_batch_bilstm(text: List, tokenizer, max_len=512) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Creates a tokenized batch for input to a bilstm model\n",
    "    :param text: A list of sentences to tokenize\n",
    "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
    "    :return: Tokenized text as well as the length of the input sequence\n",
    "    \"\"\"\n",
    "    # Some light preprocessing\n",
    "    input_ids = [tokenizer.encode_ids_with_eos(t)[:max_len] for t in text]\n",
    "\n",
    "    return input_ids, [len(ids) for ids in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    input_ids = [i[0][0] for i in input_data]\n",
    "    seq_lens = [i[1][0] for i in input_data]\n",
    "    labels = [i[2] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    # Pad all of the input samples to the max length (25000 is the ID of the [PAD] token)\n",
    "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
    "\n",
    "    # Make sure each sample is max_length long\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will load the dataset and process it lazily in the __getitem__ function\n",
    "class ClassificationDatasetReader(Dataset):\n",
    "  def __init__(self, df, tokenizer):\n",
    "    self.df = df\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.df.values[idx]\n",
    "    # Calls the text_to_batch function\n",
    "    input_ids,seq_lens = text_to_batch_bilstm([row[0]], self.tokenizer)\n",
    "    label = row[1]\n",
    "    return input_ids, seq_lens, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "# Load english model with 25k word-pieces\n",
    "bpemb_id= BPEmb(lang='id', dim=100, vs=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings and add a randomly initialized embedding for our extra [PAD] token\n",
    "pretrained_embeddings = np.concatenate([bpemb_id.emb.vectors, np.zeros(shape=(1,100))], axis=0)\n",
    "# Extract the vocab and add an extra [PAD] token\n",
    "vocabulary = bpemb_id.emb.index_to_key + ['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Assuming each element in the batch is a tuple (inputs, targets)\n",
    "    inputs, targets = zip(*batch)\n",
    "    \n",
    "    # Perform padding or truncation here to make inputs of equal size\n",
    "    \n",
    "    return padded_inputs, targets  # Return padded inputs and targets\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=64, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# defining the embedding step and RNN model\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, embedding_matrix):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings), padding_idx=0, freeze=True)\n",
    "        emb_dim = pretrained_embeddings.shape[1] #this will be the size of the input for the RNN\n",
    "        \n",
    "        #define the RNN itself \n",
    "        self.rnn = torch.nn.RNN(emb_dim, rnn_size, batch_first=True)\n",
    "        #set batch_first=True for your RNN layer\n",
    "        \n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.outputs = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        # The RNN returns two tensors: one representing the hidden states at all positions,\n",
    "        # and another representing only the final hidden states.\n",
    "        # In this many-to-one model, we only need the final hidden states.\n",
    "        all_states, final_state = self.rnn(encoded_inputs)\n",
    "        final_state = final_state.squeeze() #flatten to make sure it has the right dimensions for the next linear step\n",
    "        \n",
    "        # run the final state through the output layer\n",
    "        outputs = self.outputs(final_state)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def training_loop(model, num_epochs):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            targets = targets.squeeze() #dependending on your torch version you might have to use targets = targets.squeeze().long()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad(): # for evaluation we don't backpropagate and update weights anymore\n",
    "        for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "            outputs = torch.softmax(model(inputs), 1 ) # apply softmax to get probabilities/logits\n",
    "            # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            # accumulating the predictions\n",
    "            predictions += indices.tolist()\n",
    "            # accumulating the true labels\n",
    "            labels += targets.tolist()\n",
    "    \n",
    "    acc = accuracy_score(predictions, labels)\n",
    "    f1 = f1_score(predictions, labels)\n",
    "    print(f'Model accuracy: {acc}'\n",
    "          f'F1 score: {f1}')\n",
    "    return acc, f1, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train', 'validation']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# initializing and training the model:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m myRNN \u001b[39m=\u001b[39m SimpleRNN(rnn_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_classes\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, embedding_matrix\u001b[39m=\u001b[39mpretrained_embeddings)\n\u001b[0;32m----> 4\u001b[0m myRNN \u001b[39m=\u001b[39m training_loop(myRNN, \u001b[39m3\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m acc, f1, preds \u001b[39m=\u001b[39m evaluate(myRNN, val_loader)\n",
      "Cell \u001b[0;32mIn[64], line 8\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, num_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      7\u001b[0m     losses \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mfor\u001b[39;00m batch_index, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     10\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m         outputs \u001b[39m=\u001b[39m model(inputs)\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/datasets/dataset_dict.py:63\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     59\u001b[0m available_suggested_splits \u001b[39m=\u001b[39m [\n\u001b[1;32m     60\u001b[0m     split \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m (Split\u001b[39m.\u001b[39mTRAIN, Split\u001b[39m.\u001b[39mTEST, Split\u001b[39m.\u001b[39mVALIDATION) \u001b[39mif\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m     61\u001b[0m ]\n\u001b[1;32m     62\u001b[0m suggested_split \u001b[39m=\u001b[39m available_suggested_splits[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m available_suggested_splits \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 63\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m     64\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid key: \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m. Please first select a split. For example: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`my_dataset_dictionary[\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msuggested_split\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m][\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m]`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAvailable splits: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train', 'validation']\""
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = SimpleRNN(rnn_size=10, n_classes=3, embedding_matrix=pretrained_embeddings)\n",
    "\n",
    "myRNN = training_loop(myRNN, 3)\n",
    "acc, f1, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on word counts vs GloVe embeddings and BPEMB embeddings (remember for report)\n",
    "Question: Just as a note, you can actually get much better performance using simple word counts -- why do you think this is?\n",
    "\n",
    "Possible answer:\n",
    "The reason simple word counts can sometimes outperform more complex models like GloVe or BPEmb embeddings in certain tasks is due to the nature of the data and the task itself.\n",
    "\n",
    "**In some tasks, the presence or absence of specific words can be highly indicative of the class or category. For example, in sentiment analysis, words like 'good', 'awesome', 'bad', 'terrible' etc. can be strong indicators of the sentiment. A simple word count vectorizer captures this information effectively.**\n",
    "\n",
    "On the other hand, word embeddings like GloVe or BPEmb capture semantic and syntactic relationships between words, which can be very useful for tasks that require understanding of context or when dealing with words not present in the training set. However, these embeddings might introduce noise for tasks that can be solved based on simple word occurrence statistics.\n",
    "\n",
    "In summary, the effectiveness of a method depends on the specific task and the nature of the data. It's always a good idea to start with simpler methods and then move to more complex ones if necessary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is taken from lab_2.ipynb and modified to fit the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data into a model\n",
    "\n",
    "A simple and common way that data is read in PyTorch is to use the two following classes: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "The `Dataset` class can be extended to read in and store the data you are using for your experiment. The only requirements are to implement the `__len__` and `__getitem__` methods. `__len__` simply returns the size of your dataset and `__getitem__` takes an index and returns that sample from your dataset, processed in whatever way is necessary to be input to your model.\n",
    "\n",
    "The `DataLoader` class determines how to iterate through your `Dataset`, including how to shuffle and batch your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_batch_bilstm(text: List, tokenizer, max_len=512) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Creates a tokenized batch for input to a bilstm model\n",
    "    :param text: A list of sentences to tokenize\n",
    "    :param tokenizer: A tokenization function to use (i.e. fasttext)\n",
    "    :return: Tokenized text as well as the length of the input sequence\n",
    "    \"\"\"\n",
    "    # Some light preprocessing\n",
    "    input_ids = [tokenizer.encode_ids_with_eos(t)[:max_len] for t in text]\n",
    "\n",
    "    return input_ids, [len(ids) for ids in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_bilstm(input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Combines multiple data samples into a single batch\n",
    "    :param input_data: The combined input_ids, seq_lens, and labels for the batch\n",
    "    :return: A tuple of tensors (input_ids, seq_lens, labels)\n",
    "    \"\"\"\n",
    "    input_ids = [i[0][0] for i in input_data]\n",
    "    seq_lens = [i[1][0] for i in input_data]\n",
    "    labels = [i[2] for i in input_data]\n",
    "\n",
    "    max_length = max([len(i) for i in input_ids])\n",
    "\n",
    "    # Pad all of the input samples to the max length (25000 is the ID of the [PAD] token)\n",
    "    input_ids = [(i + [25000] * (max_length - len(i))) for i in input_ids]\n",
    "\n",
    "    # Make sure each sample is max_length long\n",
    "    assert (all(len(i) == max_length for i in input_ids))\n",
    "    return torch.tensor(input_ids), torch.tensor(seq_lens), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will load the dataset and process it lazily in the __getitem__ function\n",
    "class ClassificationDatasetReader(Dataset):\n",
    "  def __init__(self, df, tokenizer):\n",
    "    self.df = df\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.df.values[idx]\n",
    "    # Calls the text_to_batch function\n",
    "    input_ids,seq_lens = text_to_batch_bilstm([row[0]], self.tokenizer)\n",
    "    label = row[1]\n",
    "    return input_ids, seq_lens, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpemb import BPEmb\n",
    "\n",
    "# Load english model with 25k word-pieces\n",
    "bpemb_id= BPEmb(lang='id', dim=100, vs=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings and add a randomly initialized embedding for our extra [PAD] token\n",
    "pretrained_embeddings = np.concatenate([bpemb_id.emb.vectors, np.zeros(shape=(1,100))], axis=0)\n",
    "# Extract the vocab and add an extra [PAD] token\n",
    "vocabulary = bpemb_id.emb.index_to_key + ['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_trained = ClassificationDatasetReader(df_train_english_merged, bpemb_id)\n",
    "reader_val = ClassificationDatasetReader(df_val_english_merged, bpemb_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model\n",
    "\n",
    "Next we will create a BiLSTM model with BPE word-piece embeddings. In this case we will extend the PyTorch class `torch.nn.Module`. To create your own module, you need only define your model architecture in the `__init__` function, and define how tensors are processed by your model in the `__forward__` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Define a default lstm_dim\n",
    "lstm_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiRNNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic BiRNN network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            rnn_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic BiRNN network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
    "        :param rnn_dim: The dimensionality of the BiRNN network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(BiRNNNetwork, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a 2 layer BiRNN, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'birnn': nn.RNN(\n",
    "                pretrained_embeddings.shape[1],\n",
    "                rnn_dim,\n",
    "                num_layers=1,  # You can adjust the number of layers as needed\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'cls': nn.Linear(2 * rnn_dim, n_classes)\n",
    "        })\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['birnn'].named_parameters()) + \\\n",
    "                     list(self.model['cls'].named_parameters())\n",
    "        for n, p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels=None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        rnn_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass the packed sequence through the BiRNN\n",
    "        rnn_out, _ = self.model['birnn'](rnn_in)\n",
    "\n",
    "        # Unpack the packed sequence --> (b x sl x 2*rnn_dim)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "\n",
    "        # Max pool along the last dimension\n",
    "        ff_in = self.dropout(torch.max(rnn_out, 1)[0])\n",
    "\n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.model['cls'](ff_in).view(-1, self.n_classes)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Cross-entropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model\n",
    "class BiLSTMNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic BiLSTM network\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            pretrained_embeddings: torch.tensor,\n",
    "            lstm_dim: int,\n",
    "            dropout_prob: float = 0.1,\n",
    "            n_classes: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializer for basic BiLSTM network\n",
    "        :param pretrained_embeddings: A tensor containing the pretrained BPE embeddings\n",
    "        :param lstm_dim: The dimensionality of the BiLSTM network\n",
    "        :param dropout_prob: Dropout probability\n",
    "        :param n_classes: The number of output classes\n",
    "        \"\"\"\n",
    "\n",
    "        # First thing is to call the superclass initializer\n",
    "        super(BiLSTMNetwork, self).__init__()\n",
    "\n",
    "        # We'll define the network in a ModuleDict, which makes organizing the model a bit nicer\n",
    "        # The components are an embedding layer, a 2 layer BiLSTM, and a feed-forward output layer\n",
    "        self.model = nn.ModuleDict({\n",
    "            'embeddings': nn.Embedding.from_pretrained(pretrained_embeddings, padding_idx=pretrained_embeddings.shape[0] - 1),\n",
    "            'bilstm': nn.LSTM(\n",
    "                pretrained_embeddings.shape[1],\n",
    "                lstm_dim,\n",
    "                1,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_prob,\n",
    "                bidirectional=True),\n",
    "            'cls': nn.Linear(2*lstm_dim, n_classes)\n",
    "        })\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        # Initialize the weights of the model\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        all_params = list(self.model['bilstm'].named_parameters()) + \\\n",
    "                     list(self.model['cls'].named_parameters())\n",
    "        for n,p in all_params:\n",
    "            if 'weight' in n:\n",
    "                nn.init.xavier_normal_(p)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        \"\"\"\n",
    "        Defines how tensors flow through the model\n",
    "        :param inputs: (b x sl) The IDs into the vocabulary of the input samples\n",
    "        :param input_lens: (b) The length of each input sequence\n",
    "        :param labels: (b) The label of each sample\n",
    "        :return: (loss, logits) if `labels` is not None, otherwise just (logits,)\n",
    "        \"\"\"\n",
    "\n",
    "        # Get embeddings (b x sl x edim)\n",
    "        embeds = self.model['embeddings'](inputs)\n",
    "\n",
    "        # Pack padded: This is necessary for padded batches input to an RNN\n",
    "        lstm_in = nn.utils.rnn.pack_padded_sequence(\n",
    "            embeds,\n",
    "            input_lens.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        # Pass the packed sequence through the BiLSTM\n",
    "        lstm_out, hidden = self.model['bilstm'](lstm_in)\n",
    "\n",
    "        # Unpack the packed sequence --> (b x sl x 2*lstm_dim)\n",
    "        lstm_out,_ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        # Max pool along the last dimension\n",
    "        ff_in = self.dropout(torch.max(lstm_out, 1)[0])\n",
    "        # Some magic to get the last output of the BiLSTM for classification (b x 2*lstm_dim)\n",
    "        #ff_in = lstm_out.gather(1, input_lens.view(-1,1,1).expand(lstm_out.size(0), 1, lstm_out.size(2)) - 1).squeeze()\n",
    "\n",
    "        # Get logits (b x n_classes)\n",
    "        logits = self.model['cls'](ff_in).view(-1, self.n_classes)\n",
    "        outputs = (logits,)\n",
    "        if labels is not None:\n",
    "            # Xentropy loss\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"cuda available\")\n",
    "  device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emmastoklundlee/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = BiLSTMNetwork(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings),\n",
    "    lstm_dim=lstm_dim,\n",
    "    dropout_prob=0.1,\n",
    "    n_classes=2\n",
    "  ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = BiRNNNetwork(\n",
    "    pretrained_embeddings=torch.FloatTensor(pretrained_embeddings),\n",
    "    rnn_dim=100,\n",
    "    dropout_prob=0.1,\n",
    "    n_classes=2\n",
    "  ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "  logits = np.asarray(logits).reshape(-1, len(logits[0]))\n",
    "  labels = np.asarray(labels).reshape(-1)\n",
    "  return np.sum(np.argmax(logits, axis=-1) == labels).astype(np.float32) / float(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, valid_dl: DataLoader):\n",
    "  \"\"\"\n",
    "  Evaluates the model on the given dataset\n",
    "  :param model: The model under evaluation\n",
    "  :param valid_dl: A `DataLoader` reading validation data\n",
    "  :return: The accuracy of the model on the dataset\n",
    "  \"\"\"\n",
    "  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n",
    "  # layer normalization and dropout\n",
    "  model.eval()\n",
    "  labels_all = []\n",
    "  logits_all = []\n",
    "\n",
    "  # ALSO IMPORTANT: Don't accumulate gradients during this process\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(valid_dl, desc='Evaluation'):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      seq_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      _, logits = model(input_ids, seq_lens, labels=labels)\n",
    "      labels_all.extend(list(labels.detach().cpu().numpy()))\n",
    "      logits_all.extend(list(logits.detach().cpu().numpy()))\n",
    "    acc = accuracy(logits_all, labels_all)\n",
    "\n",
    "    return acc,labels_all,logits_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dl: DataLoader,\n",
    "    valid_dl: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    n_epochs: int,\n",
    "    device: torch.device,\n",
    "    patience: int = 10\n",
    "):\n",
    "  \"\"\"\n",
    "  The main training loop which will optimize a given model on a given dataset\n",
    "  :param model: The model being optimized\n",
    "  :param train_dl: The training dataset\n",
    "  :param valid_dl: A validation dataset\n",
    "  :param optimizer: The optimizer used to update the model parameters\n",
    "  :param n_epochs: Number of epochs to train for\n",
    "  :param device: The device to train on\n",
    "  :return: (model, losses) The best model and the losses per iteration\n",
    "  \"\"\"\n",
    "\n",
    "  # Keep track of the loss and best accuracy\n",
    "  losses = []\n",
    "  best_acc = 0.0\n",
    "  pcounter = 0\n",
    "\n",
    "  # Iterate through epochs\n",
    "  for ep in range(n_epochs):\n",
    "\n",
    "    loss_epoch = []\n",
    "\n",
    "    #Iterate through each batch in the dataloader\n",
    "    for batch in tqdm(train_dl):\n",
    "      # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n",
    "      # things like dropout and layer normalization\n",
    "      model.train()\n",
    "\n",
    "      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n",
    "      # keeps track of these dynamically in its computation graph so you need to explicitly\n",
    "      # zero them out\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Place each tensor on the GPU\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      input_ids = batch[0]\n",
    "      seq_lens = batch[1]\n",
    "      labels = batch[2]\n",
    "\n",
    "      # Pass the inputs through the model, get the current loss and logits\n",
    "      loss, logits = model(input_ids, seq_lens, labels=labels)\n",
    "      losses.append(loss.item())\n",
    "      loss_epoch.append(loss.item())\n",
    "\n",
    "      # Calculate all of the gradients and weight updates for the model\n",
    "      loss.backward()\n",
    "\n",
    "      # Optional: clip gradients\n",
    "      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "      # Finally, update the weights of the model\n",
    "      optimizer.step()\n",
    "      #gc.collect()\n",
    "\n",
    "    # Perform inline evaluation at the end of the epoch\n",
    "    acc,_,_ = evaluate(model, valid_dl)\n",
    "    print(f'Validation accuracy: {acc}, train loss: {sum(loss_epoch) / len(loss_epoch)}')\n",
    "\n",
    "    # Keep track of the best model based on the accuracy\n",
    "    if acc > best_acc:\n",
    "      torch.save(model.state_dict(), 'best_model')\n",
    "      best_acc = acc\n",
    "      pcounter = 0\n",
    "    else:\n",
    "      pcounter += 1\n",
    "      if pcounter == patience:\n",
    "        break\n",
    "        #gc.collect()\n",
    "\n",
    "  model.load_state_dict(torch.load('best_model'))\n",
    "  return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyperparameters\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'pretrained_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Create the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m myRNN(\n\u001b[1;32m      3\u001b[0m     pretrained_embeddings\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mFloatTensor(pretrained_embeddings),\n\u001b[1;32m      4\u001b[0m     lstm_dim\u001b[39m=\u001b[39;49mlstm_dim,\n\u001b[1;32m      5\u001b[0m     dropout_prob\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     n_classes\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m\n\u001b[1;32m      7\u001b[0m   )\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gensim_update/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'pretrained_embeddings'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_56188/3271786820.py:32: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_dl):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a61b0195de425cb73349e18d3d7e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/k0nn68mj0ylfjxzs4mcl9_f00000gn/T/ipykernel_56188/1187767117.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(valid_dl, desc='Evaluation'):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b73b312dacb47f9b09a9098f3ecedfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6875, train loss: 0.664477046529452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb20bd65eaee4139b12baa12e2eb4af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2085a397a8a048fab6c46d577a8b106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6875, train loss: 0.620822739203771\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3056c47c026746ff9ba7c8773377fa6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3e5902bd304b60bd90aabfa67feb7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7053571428571429, train loss: 0.5988541092475256\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b89946c55348719af5eae0a4b95380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f2ff5a277b4e7abf3821c5a9ca5f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7142857142857143, train loss: 0.5860849579175313\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5950f1eca6d94168a4910561bf41414c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee853a792c13479bac7e0252afb33009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6919642857142857, train loss: 0.5726226300001145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9389d1e5ec9c4506bd5c63c2a85015dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77363d2319d04989ac5525276f7e759d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7098214285714286, train loss: 0.5675098170836766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdeff826d02c4bc2a4be643694130da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5368251954241649d0547b673434853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7008928571428571, train loss: 0.5630445984999338\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c50b8a75a74050b122bcbe9faebc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92abb7a9d15d447bb1fb1c87e2ff0432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.7142857142857143, train loss: 0.5497639417648316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1304a7d8c94e1ba92b5e1f1a0e1a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef9f7d045fa94eb288df04709f69ba64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6205357142857143, train loss: 0.5489549688498179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec1a4f448ee4c76acc43013228c2a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7faabace44841f9bcdde0fbbcf3b5ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6919642857142857, train loss: 0.5407154452800751\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset readers\n",
    "train_dataset = ClassificationDatasetReader(df_train_bengali_merged[:5000], bpemb_id)\n",
    "# dataset loaded lazily with N workers in parallel\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_bilstm)\n",
    "\n",
    "valid_dataset = ClassificationDatasetReader(df_val_bengali_merged[:1000], bpemb_id)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=len(df_val_bengali_merged[:1000]), collate_fn=collate_batch_bilstm)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Train\n",
    "model, losses = train(model, train_dl, valid_dl, optimizer, n_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# defining the embedding step and RNN model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSimpleRNN\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, rnn_size, n_classes, pretrained_embeddings):\n\u001b[1;32m      5\u001b[0m         \u001b[39m# initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         \u001b[39m# and a certain number of output classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# defining the embedding step and RNN model\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, pretrained_embeddings):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings), padding_idx=0, freeze=True)\n",
    "        emb_dim = pretrained_embeddings.shape[1] #this will be the size of the input for the RNN\n",
    "        \n",
    "        #define the RNN itself \n",
    "        self.rnn = torch.nn.RNN(emb_dim, rnn_size, batch_first=True)\n",
    "        #set batch_first=True for your RNN layer\n",
    "        \n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.outputs = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, input_lens, labels = None):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        # The RNN returns two tensors: one representing the hidden states at all positions,\n",
    "        # and another representing only the final hidden states.\n",
    "        # In this many-to-one model, we only need the final hidden states.\n",
    "        all_states, final_state = self.rnn(encoded_inputs)\n",
    "        final_state = final_state.squeeze() #flatten to make sure it has the right dimensions for the next linear step\n",
    "        \n",
    "        # run the final state through the output layer\n",
    "        outputs = self.outputs(final_state)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def training_loop(model, num_epochs):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch_index, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            targets = targets.squeeze() #dependending on your torch version you might have to use targets = targets.squeeze().long()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f'Epoch {epoch+1}: loss {np.mean(losses)}')\n",
    "    return model\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad(): # for evaluation we don't backpropagate and update weights anymore\n",
    "        for batch_index, (inputs, targets) in enumerate(val_loader):\n",
    "            outputs = torch.softmax(model(inputs), 1 ) # apply softmax to get probabilities/logits\n",
    "            # getting the indices of the logit with the highest value, which corresponds to the predicted class (as labels 0, 1, 2)\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            # accumulating the predictions\n",
    "            predictions += indices.tolist()\n",
    "            # accumulating the true labels\n",
    "            labels += targets.tolist()\n",
    "    \n",
    "    acc = accuracy_score(predictions, labels)\n",
    "    f1 = f1_score(predictions, labels)\n",
    "    print(f'Model accuracy: {acc}'\n",
    "          f'F1 score: {f1}')\n",
    "    return acc, f1, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (numpy.ndarray, padding_idx=int), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: padding_idx\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# initializing and training the model:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m myRNN \u001b[39m=\u001b[39m SimpleRNN(rnn_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, n_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, pretrained_embeddings\u001b[39m=\u001b[39;49mpretrained_embeddings)\n\u001b[1;32m      4\u001b[0m myRNN \u001b[39m=\u001b[39m training_loop(myRNN, \u001b[39m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m acc, f1, preds \u001b[39m=\u001b[39m evaluate(myRNN, val_loader)\n",
      "Cell \u001b[0;32mIn[74], line 11\u001b[0m, in \u001b[0;36mSimpleRNN.__init__\u001b[0;34m(self, rnn_size, n_classes, pretrained_embeddings)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     10\u001b[0m \u001b[39m#applying the embeddings to the inputs\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mEmbedding\u001b[39m.\u001b[39mfrom_pretrained(torch\u001b[39m.\u001b[39;49mFloatTensor(pretrained_embeddings, padding_idx\u001b[39m=\u001b[39;49mpretrained_embeddings\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m] \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m))\n\u001b[1;32m     12\u001b[0m emb_dim \u001b[39m=\u001b[39m pretrained_embeddings\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m#this will be the size of the input for the RNN\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m#define the RNN itself \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (numpy.ndarray, padding_idx=int), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: padding_idx\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "# initializing and training the model:\n",
    "myRNN = SimpleRNN(rnn_size=10, n_classes=3, pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "myRNN = training_loop(myRNN, 3)\n",
    "acc, f1, preds = evaluate(myRNN, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced version supporting multiple types of RNN layers\n",
    "\n",
    "class RNN_or_LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, pretrained_embeddings, type=\"RNN\"):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings), padding_idx=0, freeze=True)\n",
    "        emb_dim = pretrained_embeddings.shape[1]\n",
    "        \n",
    "        #remember the batch_first=True argument\n",
    "        if type == \"RNN\":\n",
    "            self.rnn = torch.nn.RNN(emb_dim, rnn_size, batch_first=True)\n",
    "        elif type == \"LSTM\":\n",
    "            self.rnn = torch.nn.LSTM(emb_dim, rnn_size, batch_first=True)   \n",
    "        else:\n",
    "            raise LookupError(\"Only RNN and LSTM are supported.\")\n",
    "        self.output = torch.nn.Linear(rnn_size, n_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "\n",
    "        #apply the RNN or LSTM\n",
    "        if type == \"RNN\":\n",
    "            all_states, final_state = self.rnn(encoded_inputs)\n",
    "        else:\n",
    "            # LSTM's output is different and needs to be treated differently, see documentation for details\n",
    "            all_states, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "        \n",
    "        # run the final states through the output layer\n",
    "        outputs = self.output(final_state.squeeze())\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 0.9892009298863083\n",
      "Epoch 2: loss 0.8243393404730435\n",
      "Epoch 3: loss 0.7881016083832445\n",
      "Model accuracy: 0.5F1 score: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m myLSTM \u001b[39m=\u001b[39m RNN_or_LSTM(rnn_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_classes\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLSTM\u001b[39m\u001b[39m'\u001b[39m, pretrained_embeddings\u001b[39m=\u001b[39mpretrained_embeddings)\n\u001b[1;32m      5\u001b[0m myLSTM \u001b[39m=\u001b[39m training_loop(myLSTM, \u001b[39m3\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m acc, preds \u001b[39m=\u001b[39m evaluate(myLSTM, val_loader)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "myLSTM = RNN_or_LSTM(rnn_size=10, n_classes=3, type='LSTM', pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "myLSTM = training_loop(myLSTM, 3)\n",
    "acc, f1, preds = evaluate(myLSTM, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bidirectional LSTM\n",
    "class Bidirectional_LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnn_size, n_classes, pretrained_embeddings):\n",
    "        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)\n",
    "        # and a certain number of output classes\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #applying the embeddings to the inputs\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings), padding_idx=0, freeze=True)\n",
    "        emb_dim = pretrained_embeddings.shape[1] #this will be the size of the input for the RNN\n",
    "        \n",
    "        #define the RNN itself \n",
    "        self.rnn = torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        #set batch_first=True for your RNN layer\n",
    "        \n",
    "        #define the output layer (no softmax needed here; we will apply softmax as part of the loss calculation)\n",
    "        #applies a linear transformation to the RNN\n",
    "        #final layer state and outputs scores for the n classes\n",
    "        self.fc_logits = torch.nn.Linear(2*rnn_size, n_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # encode the input vectors\n",
    "        encoded_inputs = self.embedding(inputs)\n",
    "        \n",
    "        # NB: for a bidirectional RNN, the final state corresponds to the *last* token\n",
    "        # in the forward direction and the *first* token in the backward direction.\n",
    "        #Notice that we use torch.concat to concatenate the final states from the forward and backward directions\n",
    "        rnn_out, (final_state, c_n) = self.rnn(encoded_inputs)\n",
    "        final_states_combined = torch.cat([final_state[-2,:,:], final_state[-1,:,:]], dim=1)\n",
    "\n",
    "        # run the output through the final linear layer\n",
    "        outputslinear = self.fc_logits(final_states_combined)\n",
    "        return outputslinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss 1.0771430430227313\n",
      "Epoch 2: loss 0.8312811345375818\n",
      "Epoch 3: loss 0.7989947962863692\n",
      "Model accuracy: 0.5F1 score: 0.0\n"
     ]
    }
   ],
   "source": [
    "biLSTM = Bidirectional_LSTM(rnn_size=10, n_classes=3, pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "biLSTM = training_loop(biLSTM, 3)\n",
    "acc, f1, preds = evaluate(biLSTM, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install update transformers\n!pip install datasets\n!pip install evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-29T12:25:20.629263Z","iopub.execute_input":"2023-10-29T12:25:20.630065Z","iopub.status.idle":"2023-10-29T12:25:55.292772Z","shell.execute_reply.started":"2023-10-29T12:25:20.630031Z","shell.execute_reply":"2023-10-29T12:25:55.291532Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: update in /opt/conda/lib/python3.10/site-packages (0.0.1)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: style==1.1.0 in /opt/conda/lib/python3.10/site-packages (from update) (1.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import load_metric\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForQuestionAnswering\nfrom transformers import AutoConfig\nfrom functools import partial\nimport torch\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch import nn\nfrom collections import defaultdict, OrderedDict\n# MODEL_NAME = 'xlm-roberta-base'\nMODEL_NAME = 'bert-base-multilingual-uncased'\n# NUM_SUBSAMPLES = 11394\n#bengali: 4779\n#Arabic: 29598\n#Indonesian: 11394\nLANGUAGE = \"indonesian\" # \"bengali\" \"arabic\"","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:25:55.296371Z","iopub.execute_input":"2023-10-29T12:25:55.297169Z","iopub.status.idle":"2023-10-29T12:25:55.304224Z","shell.execute_reply.started":"2023-10-29T12:25:55.297138Z","shell.execute_reply":"2023-10-29T12:25:55.303299Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def enforce_reproducibility(seed=42):\n    # Sets seed manually for both CPU and CUDA\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # For atomic operations there is currently\n    # no simple way to enforce determinism, as\n    # the order of parallel operations is not known.\n    # CUDNN\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # System based\n    random.seed(seed)\n    np.random.seed(seed)\n\ndevice = torch.device(\"cpu\")\nif torch.cuda.is_available():\n  device = torch.device(\"cuda\")\n\nenforce_reproducibility()","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:25:55.305509Z","iopub.execute_input":"2023-10-29T12:25:55.305837Z","iopub.status.idle":"2023-10-29T12:25:55.322660Z","shell.execute_reply.started":"2023-10-29T12:25:55.305806Z","shell.execute_reply":"2023-10-29T12:25:55.321869Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import Dataset, DatasetDict\n\ndataset = load_dataset(\"copenlu/answerable_tydiqa\")\n\nfiltered_dataset = dataset.filter(lambda entry: entry[\"language\"] in [LANGUAGE])#, \"arabic\", \"bengali\"])\n#filtered_dataset = filtered_dataset.filter(lambda entry: entry[\"annotations\"][\"answer_start\"][0] == -1)\n\ntrain_set = filtered_dataset[\"train\"]\nvalidation_set = filtered_dataset[\"validation\"]\n\ntrain_set_df = train_set.to_pandas()\ntrain_set_df['id'] = range(len(train_set_df))\nvalidation_set_df = validation_set.to_pandas()\nvalidation_set_df['id'] = range(len(validation_set_df))\n\ntrain_set = Dataset.from_pandas(train_set_df)\nvalidation_set = Dataset.from_pandas(validation_set_df)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:25:55.323599Z","iopub.execute_input":"2023-10-29T12:25:55.323835Z","iopub.status.idle":"2023-10-29T12:25:56.519059Z","shell.execute_reply.started":"2023-10-29T12:25:55.323814Z","shell.execute_reply":"2023-10-29T12:25:56.518272Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf2f058e1c334396a0d176487118ae3e"}},"metadata":{}}]},{"cell_type":"code","source":"tk = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:25:56.521437Z","iopub.execute_input":"2023-10-29T12:25:56.521717Z","iopub.status.idle":"2023-10-29T12:25:56.731473Z","shell.execute_reply.started":"2023-10-29T12:25:56.521692Z","shell.execute_reply":"2023-10-29T12:25:56.730658Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForQuestionAnswering.from_pretrained(\"/kaggle/input/arabic-model\").to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:25:56.732571Z","iopub.execute_input":"2023-10-29T12:25:56.732860Z","iopub.status.idle":"2023-10-29T12:25:58.596887Z","shell.execute_reply.started":"2023-10-29T12:25:56.732836Z","shell.execute_reply":"2023-10-29T12:25:58.596054Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def get_validation_features(tk, samples):\n  # First, tokenize the text. We get the offsets and return overflowing sequences in\n  # order to break up long sequences into multiple inputs. The offsets will help us\n  # determine the original answer text\n  batch = tk.batch_encode_plus(\n        [[q,c] for q,c in zip(samples['question_text'], samples['document_plaintext'])],\n        padding='max_length',\n        truncation='only_second',\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True\n    )\n\n  # We'll store the ID of the samples to calculate squad score\n  batch['example_id'] = []\n  # The overflow sample map tells us which input each sample corresponds to\n  sample_map = batch.pop('overflow_to_sample_mapping')\n\n  for i in range(len(batch['input_ids'])):\n    # The sample index tells us which of the values in \"samples\" these features belong to\n    sample_idx = sample_map[i]\n    sequence_ids = batch.sequence_ids(i)\n\n    # Add the ID to map these features back to the correct sample\n    batch['example_id'].append(samples['id'][sample_idx])\n\n    #Set offsets for non-context words to be None for ease of processing\n    batch['offset_mapping'][i] = [o if sequence_ids[k] == 1 else None for k,o in enumerate(batch['offset_mapping'][i])]\n\n  return batch\n\ndef val_collate_fn(inputs):\n  input_ids = torch.tensor([i['input_ids'] for i in inputs])\n  attention_mask = torch.tensor([i['attention_mask'] for i in inputs])\n\n  # Truncate to max length\n  max_len = max(attention_mask.sum(-1))\n  input_ids = input_ids[:,:max_len]\n  attention_mask = attention_mask[:,:max_len]\n\n  return {'input_ids': input_ids, 'attention_mask': attention_mask}","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:25:58.598053Z","iopub.execute_input":"2023-10-29T12:25:58.598335Z","iopub.status.idle":"2023-10-29T12:25:58.608075Z","shell.execute_reply.started":"2023-10-29T12:25:58.598310Z","shell.execute_reply":"2023-10-29T12:25:58.607132Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"validation_dataset = validation_set.map(partial(get_validation_features, tk), batched=True, remove_columns=validation_set.column_names)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:25:58.609211Z","iopub.execute_input":"2023-10-29T12:25:58.609733Z","iopub.status.idle":"2023-10-29T12:26:08.560294Z","shell.execute_reply.started":"2023-10-29T12:25:58.609700Z","shell.execute_reply":"2023-10-29T12:26:08.559273Z"},"trusted":true},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d17900918c4ebfb0d63bdf00360bf5"}},"metadata":{}}]},{"cell_type":"code","source":"def predict(model: nn.Module, valid_dl: DataLoader):\n  \"\"\"\n  Evaluates the model on the given dataset\n  :param model: The model under evaluation\n  :param valid_dl: A `DataLoader` reading validation data\n  :return: The accuracy of the model on the dataset\n  \"\"\"\n  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n  # layer normalization and dropout\n  model.eval()\n  start_logits_all = []\n  end_logits_all = []\n\n  # ALSO IMPORTANT: Don't accumulate gradients during this process\n  with torch.no_grad():\n    for batch in tqdm(valid_dl, desc='Evaluation'):\n      batch = {b: batch[b].to(device) for b in batch}\n\n      # Pass the inputs through the model, get the current loss and logits\n      outputs = model(\n          input_ids=batch['input_ids'],\n          attention_mask=batch['attention_mask']\n      )\n      # Store the \"start\" class logits and \"end\" class logits for every token in the input\n      start_logits_all.extend(list(outputs['start_logits'].detach().cpu().numpy()))\n      end_logits_all.extend(list(outputs['end_logits'].detach().cpu().numpy()))\n\n\n    return start_logits_all,end_logits_all\n\ndef post_process_predictions(examples, dataset, logits, tokenizer, num_possible_answers = 20, max_answer_length = 30):\n  all_start_logits, all_end_logits = logits\n  # Build a map from example to its corresponding features. This will allow us to index from\n  # sample ID to all of the features for that sample (in case they were split up due to long input)\n  example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n  features_per_example = defaultdict(list)\n  for i, feature in enumerate(dataset):\n      features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n  # Create somewhere to store our predictions\n  predictions = OrderedDict()\n\n  # Iterate through each sample in the dataset\n  for j, sample in enumerate(tqdm(examples)):\n\n    # Get the feature indices (all of the features split across the batch)\n    feature_indices = features_per_example[j]\n    # Get the original context which predumably has the answer text\n    context = sample['document_plaintext']\n\n    preds = []\n\n    min_score_threshold = None\n\n    # Iterate through all of the features\n    for ft_idx in feature_indices:\n\n      # Get the start and end answer logits for this input\n      start_logits = all_start_logits[ft_idx]\n      end_logits = all_end_logits[ft_idx]\n\n      # Get the offsets to map token indices to character indices\n      offset_mapping = dataset[ft_idx]['offset_mapping']\n\n\n      # Update minimum null prediction.\n      cls_index = dataset[ft_idx][\"input_ids\"].index(tokenizer.cls_token_id)\n      feature_min_score_threshold = start_logits[cls_index] + end_logits[cls_index]\n      if min_score_threshold is None or min_score_threshold < feature_min_score_threshold:\n          min_score_threshold = feature_min_score_threshold\n\n      # Sort the logits and take the top N\n      start_indices = np.argsort(start_logits)[::-1][:num_possible_answers]\n      end_indices = np.argsort(end_logits)[::-1][:num_possible_answers]\n\n      # Iterate through start and end indices\n      for start_index in start_indices:\n        for end_index in end_indices:\n\n          # Ignore this combination if either the indices are not in the context\n          if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n            continue\n\n          # Also ignore if the start index is greater than the end index of the number of tokens\n          # is greater than some specified threshold\n          if start_index > end_index or end_index - start_index + 1 > max_answer_length:\n            continue\n          try:\n              ans_text = context[offset_mapping[start_index][0]:offset_mapping[end_index][1]]\n              preds.append({\n                  'score': start_logits[start_index] + end_logits[end_index],\n                  'text': ans_text\n              })\n          except Exception as e:\n              #print(start_index)\n              #print(offset_mapping[start_index])\n              #print(offset_mapping[1])\n              continue\n              #print(offset_mapping[start_index][0])\n\n    if len(preds) > 0:\n      # Sort by score to get the top answer\n      best_answer = sorted(preds, key=lambda x: x['score'], reverse=True)[0]\n    else:\n      best_answer = {'score': 0.0, 'text': \"\"}\n\n    # if the best answer is below the threshold for lowest score, give it the empty string\n\n    answer = best_answer[\"text\"] if best_answer[\"score\"] > min_score_threshold else \"\"\n    predictions[sample['id']] = answer\n  return predictions","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:08.561578Z","iopub.execute_input":"2023-10-29T12:26:08.561894Z","iopub.status.idle":"2023-10-29T12:26:08.579627Z","shell.execute_reply.started":"2023-10-29T12:26:08.561869Z","shell.execute_reply":"2023-10-29T12:26:08.578648Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"val_dl = DataLoader(validation_dataset, collate_fn=val_collate_fn, batch_size=32)\nlogits = predict(model, val_dl)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:08.581066Z","iopub.execute_input":"2023-10-29T12:26:08.581503Z","iopub.status.idle":"2023-10-29T12:26:27.043222Z","shell.execute_reply.started":"2023-10-29T12:26:08.581472Z","shell.execute_reply":"2023-10-29T12:26:27.042370Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Evaluation: 100%|██████████| 38/38 [00:18<00:00,  2.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_dataset","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:27.044431Z","iopub.execute_input":"2023-10-29T12:26:27.044703Z","iopub.status.idle":"2023-10-29T12:26:27.051381Z","shell.execute_reply.started":"2023-10-29T12:26:27.044680Z","shell.execute_reply":"2023-10-29T12:26:27.050404Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n    num_rows: 1210\n})"},"metadata":{}}]},{"cell_type":"code","source":"predictions = post_process_predictions(validation_set, validation_dataset, logits, tk)\nformatted_predictions = [{'id': k, 'prediction_text': v} for k,v in predictions.items()]\ngold = [{'id': example['id'], 'answers': example['annotations'][\"answer_text\"][0]} for example in validation_set]","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:27.052516Z","iopub.execute_input":"2023-10-29T12:26:27.052802Z","iopub.status.idle":"2023-10-29T12:26:36.143264Z","shell.execute_reply.started":"2023-10-29T12:26:27.052770Z","shell.execute_reply":"2023-10-29T12:26:36.142462Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"100%|██████████| 1191/1191 [00:06<00:00, 188.80it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\" Official evaluation script for v1.1 of the SQuAD dataset. \"\"\"\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(s):\n    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n\n    def white_space_fix(text):\n        return ' '.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    if len(prediction_tokens) == 0 and len(ground_truth_tokens) == 0:\n      return 1\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    #print(prediction)\n    #print(ground_truth)\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    #print(metric_fn)\n    #print(prediction)\n    #print(ground_truths)\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    #if len(scores_for_ground_truths) == 0:\n    #  if len(prediction) == 0:\n    #    return 1 # FIX: skal ændres, så at hvis der ikke er noget svar og prediction også siger det, så skal den have god score\n     # else:\n     #   return 0\n    return max(scores_for_ground_truths)\n\n\ndef evaluate_squad(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        # print(article)\n        for paragraph in article['paragraphs']:\n            for qa in paragraph['qas']:\n                #print(qa)\n                #print(predictions[qa['id']])\n                total += 1\n                if qa['id'] not in predictions:\n                    message = 'Unanswered question ' + qa['id'] + \\\n                              ' will receive score 0.'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n                prediction = predictions[qa['id']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {'exact_match': exact_match, 'f1': f1}\n\ndef compute_squad(predictions, references):\n  pred_dict = {prediction[\"id\"]: prediction[\"prediction_text\"] for prediction in predictions}\n  dataset = [\n      {\n          \"paragraphs\": [\n              {\n                  \"qas\": [\n                      {\n                          \"answers\": [{\"text\": ref[\"answers\"]} ],\n                          \"id\": ref[\"id\"],\n                      }\n                      for ref in references\n                  ]\n              }\n          ]\n      }\n  ]\n  score = evaluate_squad(dataset=dataset, predictions=pred_dict)\n  return score","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:36.144820Z","iopub.execute_input":"2023-10-29T12:26:36.145172Z","iopub.status.idle":"2023-10-29T12:26:36.162597Z","shell.execute_reply.started":"2023-10-29T12:26:36.145140Z","shell.execute_reply":"2023-10-29T12:26:36.161703Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"compute_squad(references=gold, predictions=formatted_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:36.166368Z","iopub.execute_input":"2023-10-29T12:26:36.166625Z","iopub.status.idle":"2023-10-29T12:26:36.232619Z","shell.execute_reply.started":"2023-10-29T12:26:36.166602Z","shell.execute_reply":"2023-10-29T12:26:36.231781Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 62.04869857262804, 'f1': 69.10885954901626}"},"metadata":{}}]},{"cell_type":"markdown","source":"## All in one function:","metadata":{}},{"cell_type":"code","source":"def zero_shot_eval(model_path, language):\n    filtered_dataset = dataset.filter(lambda entry: entry[\"language\"] in [language])\n\n    train_set = filtered_dataset[\"train\"]\n    validation_set = filtered_dataset[\"validation\"]\n\n    train_set_df = train_set.to_pandas()\n    train_set_df['id'] = range(len(train_set_df))\n    validation_set_df = validation_set.to_pandas()\n    validation_set_df['id'] = range(len(validation_set_df))\n\n    train_set = Dataset.from_pandas(train_set_df)\n    validation_set = Dataset.from_pandas(validation_set_df)\n    \n    model = AutoModelForQuestionAnswering.from_pretrained(model_path).to(device)\n    \n    validation_dataset = validation_set.map(partial(get_validation_features, tk), batched=True, remove_columns=validation_set.column_names)\n    \n    val_dl = DataLoader(validation_dataset, collate_fn=val_collate_fn, batch_size=32)\n    logits = predict(model, val_dl)\n    \n    predictions = post_process_predictions(validation_set, validation_dataset, logits, tk)\n    formatted_predictions = [{'id': k, 'prediction_text': v} for k,v in predictions.items()]\n    gold = [{'id': example['id'], 'answers': example['annotations'][\"answer_text\"][0]} for example in validation_set]\n    \n    return compute_squad(references=gold, predictions=formatted_predictions)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:36.234885Z","iopub.execute_input":"2023-10-29T12:26:36.235597Z","iopub.status.idle":"2023-10-29T12:26:36.244020Z","shell.execute_reply.started":"2023-10-29T12:26:36.235566Z","shell.execute_reply":"2023-10-29T12:26:36.243029Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"## Bengali model, with other languages as input","metadata":{}},{"cell_type":"code","source":"bengali_indonesian_zero_shot = zero_shot_eval(\"/kaggle/input/bengali-bert-2\", \"indonesian\")\nbengali_indonesian_zero_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:26:36.245055Z","iopub.execute_input":"2023-10-29T12:26:36.245358Z","iopub.status.idle":"2023-10-29T12:27:15.615363Z","shell.execute_reply.started":"2023-10-29T12:26:36.245332Z","shell.execute_reply":"2023-10-29T12:27:15.614493Z"},"trusted":true},"execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a63b3a7ee9f444ca898dc0ab1481b58"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 38/38 [00:18<00:00,  2.05it/s]\n100%|██████████| 1191/1191 [00:06<00:00, 192.74it/s]\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 57.59865659109992, 'f1': 60.15850996218458}"},"metadata":{}}]},{"cell_type":"code","source":"bengali_arabic_zero_shot = zero_shot_eval(\"/kaggle/input/bengali-bert-2\", \"arabic\")\nbengali_arabic_zero_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:27:15.616409Z","iopub.execute_input":"2023-10-29T12:27:15.616674Z","iopub.status.idle":"2023-10-29T12:27:26.854839Z","shell.execute_reply.started":"2023-10-29T12:27:15.616651Z","shell.execute_reply":"2023-10-29T12:27:26.853711Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61f8b43f523e4ed58eb81bfc9cb97d2a"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 8/8 [00:05<00:00,  1.59it/s]\n100%|██████████| 224/224 [00:01<00:00, 169.05it/s]\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 65.17857142857143, 'f1': 71.27000231910947}"},"metadata":{}}]},{"cell_type":"code","source":"bengali_bengali_zero_shot = zero_shot_eval(\"/kaggle/input/bengali-bert-2\", \"bengali\")\nbengali_bengali_zero_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:27:26.856283Z","iopub.execute_input":"2023-10-29T12:27:26.856615Z","iopub.status.idle":"2023-10-29T12:28:39.589508Z","shell.execute_reply.started":"2023-10-29T12:27:26.856589Z","shell.execute_reply":"2023-10-29T12:28:39.588610Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff087d4288244b99ba1d5628898b997e"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 62/62 [00:36<00:00,  1.68it/s]\n100%|██████████| 1902/1902 [00:11<00:00, 172.68it/s]\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 56.098843322818084, 'f1': 58.272597262247125}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Indonesian model, with other languages as input","metadata":{}},{"cell_type":"code","source":"indonesian_arabic_zeo_shot = zero_shot_eval(\"/kaggle/input/indonesian-bert-3\", \"arabic\")\nindonesian_arabic_zeo_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:28:39.590833Z","iopub.execute_input":"2023-10-29T12:28:39.591134Z","iopub.status.idle":"2023-10-29T12:28:50.792670Z","shell.execute_reply.started":"2023-10-29T12:28:39.591102Z","shell.execute_reply":"2023-10-29T12:28:50.791708Z"},"trusted":true},"execution_count":54,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bfc6a90ae8744983a7b2b01d52c5586b"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 8/8 [00:04<00:00,  1.67it/s]\n100%|██████████| 224/224 [00:01<00:00, 167.95it/s]\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 56.25, 'f1': 61.72435203685203}"},"metadata":{}}]},{"cell_type":"code","source":"indonesian_bengali_zeo_shot = zero_shot_eval(\"/kaggle/input/indonesian-bert-3\", \"bengali\")\nindonesian_bengali_zeo_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:28:50.793786Z","iopub.execute_input":"2023-10-29T12:28:50.794042Z","iopub.status.idle":"2023-10-29T12:29:30.442628Z","shell.execute_reply.started":"2023-10-29T12:28:50.794021Z","shell.execute_reply":"2023-10-29T12:29:30.441719Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccafde236e1945f8b11ebd5aa0a481cd"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 38/38 [00:18<00:00,  2.04it/s]\n100%|██████████| 1191/1191 [00:06<00:00, 190.93it/s]\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 75.56675062972292, 'f1': 80.50033006143278}"},"metadata":{}}]},{"cell_type":"code","source":"indonesian_indonesian_zeo_shot = zero_shot_eval(\"/kaggle/input/indonesian-bert-3\", \"indonesian\")\nindonesian_indonesian_zeo_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:29:30.443798Z","iopub.execute_input":"2023-10-29T12:29:30.444083Z","iopub.status.idle":"2023-10-29T12:30:43.446646Z","shell.execute_reply.started":"2023-10-29T12:29:30.444058Z","shell.execute_reply":"2023-10-29T12:30:43.445790Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1fd2b66d5d7407d9cac28495751737f"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 62/62 [00:36<00:00,  1.68it/s]\n100%|██████████| 1902/1902 [00:11<00:00, 171.46it/s]\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 66.19348054679286, 'f1': 73.36365789378847}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Arabic model, with other languages as input","metadata":{}},{"cell_type":"code","source":"arabic_ben_zero_shot = zero_shot_eval(\"/kaggle/input/arabic-bert-2\", \"bengali\")\narabic_ben_zero_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:30:43.448128Z","iopub.execute_input":"2023-10-29T12:30:43.448516Z","iopub.status.idle":"2023-10-29T12:31:23.150515Z","shell.execute_reply.started":"2023-10-29T12:30:43.448482Z","shell.execute_reply":"2023-10-29T12:31:23.149560Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4110bbf455448178baf87ee2fc4de1d"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 38/38 [00:18<00:00,  2.07it/s]\n100%|██████████| 1191/1191 [00:06<00:00, 190.53it/s]\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 62.04869857262804, 'f1': 69.10885954901626}"},"metadata":{}}]},{"cell_type":"code","source":"arabic_indo_zero_shot = zero_shot_eval(\"/kaggle/input/arabic-bert-2\", \"indonesian\")\narabic_indo_zero_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:31:23.151653Z","iopub.execute_input":"2023-10-29T12:31:23.151916Z","iopub.status.idle":"2023-10-29T12:31:34.419980Z","shell.execute_reply.started":"2023-10-29T12:31:23.151894Z","shell.execute_reply":"2023-10-29T12:31:34.419060Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8c2601a56141a387c61cf46918b785"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 8/8 [00:04<00:00,  1.66it/s]\n100%|██████████| 224/224 [00:01<00:00, 168.41it/s]\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 51.339285714285715, 'f1': 54.21977328227329}"},"metadata":{}}]},{"cell_type":"code","source":"arabic_arabic_zero_shot = zero_shot_eval(\"/kaggle/input/arabic-bert-2\", \"arabic\")\narabic_arabic_zero_shot","metadata":{"execution":{"iopub.status.busy":"2023-10-29T12:31:34.421417Z","iopub.execute_input":"2023-10-29T12:31:34.421789Z","iopub.status.idle":"2023-10-29T12:32:14.154214Z","shell.execute_reply.started":"2023-10-29T12:31:34.421755Z","shell.execute_reply":"2023-10-29T12:32:14.153273Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1d5067d6cf435188cf3c48e3c23d09"}},"metadata":{}},{"name":"stderr","text":"Evaluation: 100%|██████████| 38/38 [00:18<00:00,  2.03it/s]\n100%|██████████| 1191/1191 [00:06<00:00, 188.80it/s]\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"{'exact_match': 62.04869857262804, 'f1': 69.10885954901626}"},"metadata":{}}]}]}
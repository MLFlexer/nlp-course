{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOMkb87xm7x+t+Acgb8EgbY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLFlexer/nlp-course/blob/Emma/bert_classification_more_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X72usktzzUIF",
        "outputId": "89ef8b94-3764-490e-a778-85bd19ff9b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2023.7.22)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "2023-10-21 15:43:10.656078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-21 15:43:11.725976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-10-21 15:43:13.125777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-10-21 15:43:13.126267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-10-21 15:43:13.126447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install bpemb\n",
        "!pip install gensim\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add-in as occasionally receive an error which requires this to be added\n",
        "# uncomment if the issue arises\n",
        "!pip install transformer[torch]"
      ],
      "metadata": {
        "id": "j1FeucwT_K-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba89ea1e-9945-4643-c29d-a67108bf8e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transformer[torch] (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformer[torch]\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch\n",
        "import datasets\n",
        "datasets.logging.set_verbosity_error()\n",
        "from datasets import load_metric\n",
        "from google.colab import drive\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # uncomment if CAN'T CONNECT TO GPU (it happens...)\n",
        "# import psutil\n",
        "# import platform"
      ],
      "metadata": {
        "id": "erIIBBN-ebGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to save output of models so they can be reloaded\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "output_dir = '/content/drive/My Drive/Colab Notebooks/NLP/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD-nst6u-7jc",
        "outputId": "ed4cfda9-5918-412d-dd10-62790b0b65ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU housekeeping code: you do not need to modify anything, simply\n",
        "# read through it to understand what is going on, and run as is\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# a helper function to format byte counts into KB, MB and so on\n",
        "def bytes_format(b):\n",
        "    if b < 1000:\n",
        "              return f'{b} B'\n",
        "    elif b < 1000000:\n",
        "        return f'{round(float(b/1000),2)} KB'\n",
        "    elif b < 1000000000:\n",
        "        return f'{round(float(b/1000000),2)} MB'\n",
        "    else:\n",
        "        return f'{round(float(b/1000000000),2)} GB'\n",
        "\n",
        "# a helper function to check the amount of available memory\n",
        "def memory_report():\n",
        "  if device!='cpu':\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name()}\")\n",
        "    #print(torch.cuda.memory_summary())\n",
        "    total = torch.cuda.get_device_properties(0).total_memory\n",
        "    reserved = torch.cuda.memory_reserved(0)\n",
        "    allocated = torch.cuda.memory_allocated(0)\n",
        "  #  free = reserved-allocated  # free inside memory_reserved\n",
        "    print(f\"Total cuda memory: {bytes_format(total)}, reserved: {bytes_format(reserved)}, allocated: {bytes_format(allocated)}\")\n",
        "  else:\n",
        "    # Print total memory available on CPU\n",
        "    print(f'Device is CPU {platform.processor()}. GPU is not available rn')\n",
        "    total_memory = psutil.virtual_memory().total\n",
        "    print(f\"Total CPU memory: {bytes_format(total_memory)}\")\n",
        "\n",
        "memory_report()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T11MhtBdv1P",
        "outputId": "4c7ec9fd-107c-4403-fc58-d2f2b6696eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: Tesla T4\n",
            "Total cuda memory: 15.84 GB, reserved: 0 B, allocated: 0 B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AheXHlZxQ38o"
      },
      "outputs": [],
      "source": [
        "# Preamble\n",
        "import sys\n",
        "\n",
        "sys.path.append('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "8Z5S6IwPQ38o",
        "outputId": "13b24088-6e75-4f52-9cb8-eb745a8f0e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "116067\n",
            "13325\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            question_text document_title  language  \\\n",
              "0            Milloin Charles Fort syntyi?   Charles Fort   finnish   \n",
              "1             “ダン” ダニエル・ジャドソン・キャラハンの出身はどこ   ダニエル・J・キャラハン  japanese   \n",
              "2  వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?            వేప    telugu   \n",
              "3      চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?    চেঙ্গিজ খান   bengali   \n",
              "4        రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?     రెయ్యలగడ్ద    telugu   \n",
              "\n",
              "                                         annotations  \\\n",
              "0  {'answer_start': [18], 'answer_text': ['6. elo...   \n",
              "1  {'answer_start': [35], 'answer_text': ['カリフォルニ...   \n",
              "2  {'answer_start': [12], 'answer_text': ['Azadir...   \n",
              "3  {'answer_start': [414], 'answer_text': ['বোরজি...   \n",
              "4  {'answer_start': [259], 'answer_text': ['27 హె...   \n",
              "\n",
              "                                  document_plaintext  \\\n",
              "0  Charles Hoy Fort (6. elokuuta (joidenkin lähte...   \n",
              "1  “ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...   \n",
              "2  వేప (లాటిన్ Azadirachta indica, syn. Melia aza...   \n",
              "3  চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...   \n",
              "4  రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...   \n",
              "\n",
              "                                        document_url  \n",
              "0       https://fi.wikipedia.org/wiki/Charles%20Fort  \n",
              "1  https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...  \n",
              "2  https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...  \n",
              "3  https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...  \n",
              "4  https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5f3b0e80-3d5a-4c74-9a4e-fe94e61dfd95\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_text</th>\n",
              "      <th>document_title</th>\n",
              "      <th>language</th>\n",
              "      <th>annotations</th>\n",
              "      <th>document_plaintext</th>\n",
              "      <th>document_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Milloin Charles Fort syntyi?</td>\n",
              "      <td>Charles Fort</td>\n",
              "      <td>finnish</td>\n",
              "      <td>{'answer_start': [18], 'answer_text': ['6. elo...</td>\n",
              "      <td>Charles Hoy Fort (6. elokuuta (joidenkin lähte...</td>\n",
              "      <td>https://fi.wikipedia.org/wiki/Charles%20Fort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>“ダン” ダニエル・ジャドソン・キャラハンの出身はどこ</td>\n",
              "      <td>ダニエル・J・キャラハン</td>\n",
              "      <td>japanese</td>\n",
              "      <td>{'answer_start': [35], 'answer_text': ['カリフォルニ...</td>\n",
              "      <td>“ダン”こと、ダニエル・ジャドソン・キャラハンは1890年7月26日、カリフォルニア州サンフ...</td>\n",
              "      <td>https://ja.wikipedia.org/wiki/%E3%83%80%E3%83%...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>వేప చెట్టు యొక్క శాస్త్రీయ నామం ఏమిటి?</td>\n",
              "      <td>వేప</td>\n",
              "      <td>telugu</td>\n",
              "      <td>{'answer_start': [12], 'answer_text': ['Azadir...</td>\n",
              "      <td>వేప (లాటిన్ Azadirachta indica, syn. Melia aza...</td>\n",
              "      <td>https://te.wikipedia.org/wiki/%E0%B0%B5%E0%B1%...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>চেঙ্গিস খান কোন বংশের রাজা ছিলেন ?</td>\n",
              "      <td>চেঙ্গিজ খান</td>\n",
              "      <td>bengali</td>\n",
              "      <td>{'answer_start': [414], 'answer_text': ['বোরজি...</td>\n",
              "      <td>চেঙ্গিজ খান (মঙ্গোলীয়: Чингис Хаан  আ-ধ্ব-ব: ...</td>\n",
              "      <td>https://bn.wikipedia.org/wiki/%E0%A6%9A%E0%A7%...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>రెయ్యలగడ్ద గ్రామ విస్తీర్ణత ఎంత?</td>\n",
              "      <td>రెయ్యలగడ్ద</td>\n",
              "      <td>telugu</td>\n",
              "      <td>{'answer_start': [259], 'answer_text': ['27 హె...</td>\n",
              "      <td>రెయ్యలగడ్ద, విశాఖపట్నం జిల్లా, గంగరాజు మాడుగుల...</td>\n",
              "      <td>https://te.wikipedia.org/wiki/%E0%B0%B0%E0%B1%...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5f3b0e80-3d5a-4c74-9a4e-fe94e61dfd95')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5f3b0e80-3d5a-4c74-9a4e-fe94e61dfd95 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5f3b0e80-3d5a-4c74-9a4e-fe94e61dfd95');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bacbdd1b-d091-46b8-9fba-b4b93ef65a90\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bacbdd1b-d091-46b8-9fba-b4b93ef65a90')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bacbdd1b-d091-46b8-9fba-b4b93ef65a90 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
        "\n",
        "train_set = dataset[\"train\"]\n",
        "validation_set = dataset[\"validation\"]\n",
        "\n",
        "df_train = train_set.to_pandas()\n",
        "df_val = validation_set.to_pandas()\n",
        "\n",
        "print(len(df_train))\n",
        "print(len(df_val))\n",
        "\n",
        "df_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15Hvm5s-Q38p"
      },
      "outputs": [],
      "source": [
        "# Get train and validation data for each language\n",
        "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
        "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
        "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
        "\n",
        "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
        "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
        "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
        "\n",
        "\n",
        "# For testing\n",
        "df_val_english = df_val[df_val['language'] == 'english']\n",
        "df_train_english = df_train[df_train['language'] == 'english']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WyIm3zKzQ38p",
        "outputId": "dff6a01d-47da-4ede-adc4-10e6908a3282"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text  answerable\n",
              "30   Wound care encourages and speeds wound healing...           1\n",
              "47   Brothers Amos and Wilfrid Ayre founded Burntis...           1\n",
              "59   For species of mammals, larger brains (in abso...           1\n",
              "77   As from 31 March 1989, fishing vessel registra...           1\n",
              "106  When Quezon City was created in 1939, the foll...           1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-80f2c684-4597-4d12-a262-35fd6d0a6e8f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>answerable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Wound care encourages and speeds wound healing...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Brothers Amos and Wilfrid Ayre founded Burntis...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>For species of mammals, larger brains (in abso...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>As from 31 March 1989, fishing vessel registra...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>When Quezon City was created in 1939, the foll...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80f2c684-4597-4d12-a262-35fd6d0a6e8f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-80f2c684-4597-4d12-a262-35fd6d0a6e8f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-80f2c684-4597-4d12-a262-35fd6d0a6e8f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d0aa023e-ceec-46b2-b715-11a2b9534b1d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0aa023e-ceec-46b2-b715-11a2b9534b1d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d0aa023e-ceec-46b2-b715-11a2b9534b1d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Create a new dataframe with the combined documents and questions and add if they are answerable\n",
        "df_train_bengali_merged = pd.DataFrame({\n",
        "    'text':(df_train_bengali[\"document_plaintext\"] + df_train_bengali[\"question_text\"]),\n",
        "    'answerable':(df_train_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_train_arabic_merged = pd.DataFrame({\n",
        "    'text': (df_train_arabic[\"document_plaintext\"] + df_train_arabic[\"question_text\"]),\n",
        "    'answerable': (df_train_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "                                    })\n",
        "df_train_indonesian_merged = pd.DataFrame({\n",
        "    'text':(df_train_indonesian[\"document_plaintext\"] + df_train_indonesian[\"question_text\"]),\n",
        "    'answerable':(df_train_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_train_english_merged = pd.DataFrame({\n",
        "    'text':(df_train_english[\"document_plaintext\"] + df_train_english[\"question_text\"]),\n",
        "    'answerable':(df_train_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "\n",
        "\n",
        "## Same for validation data\n",
        "df_val_bengali_merged = pd.DataFrame({\n",
        "    'text':(df_val_bengali[\"document_plaintext\"] + df_val_bengali[\"question_text\"]),\n",
        "    'answerable':(df_val_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_val_arabic_merged = pd.DataFrame({\n",
        "    'text': (df_val_arabic[\"document_plaintext\"] + df_val_arabic[\"question_text\"]),\n",
        "    'answerable': (df_val_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "                                    })\n",
        "df_val_indonesian_merged = pd.DataFrame({\n",
        "    'text':(df_val_indonesian[\"document_plaintext\"] + df_val_indonesian[\"question_text\"]),\n",
        "    'answerable':(df_val_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_val_english_merged = pd.DataFrame({\n",
        "    'text':(df_val_english[\"document_plaintext\"] + df_val_english[\"question_text\"]),\n",
        "    'answerable':(df_val_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "\n",
        "df_val_english_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "def tokenize_text(texts, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_text = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids.append(encoded_text[\"input_ids\"])\n",
        "        attention_masks.append(encoded_text[\"attention_mask\"])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "# # first for english to test\n",
        "# train_input_ids, train_attention_masks = tokenize_text(df_train_english_merged[\"text\"].tolist())\n",
        "# val_input_ids, val_attention_masks = tokenize_text(df_val_english_merged[\"text\"].tolist())\n",
        "# train_labels = torch.tensor(df_train_english_merged[\"answerable\"].tolist())\n",
        "# val_labels = torch.tensor(df_val_english_merged[\"answerable\"].tolist())\n"
      ],
      "metadata": {
        "id": "udS3DYSa06OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # with english dataset\n",
        "# batch_size = 32\n",
        "\n",
        "# train_data = TensorDataset(train_input_ids.to('cuda'), train_attention_masks.to('cuda'), train_labels.to('cuda'))\n",
        "# train_sampler = RandomSampler(train_data)\n",
        "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# val_data = TensorDataset(val_input_ids.to('cuda'), val_attention_masks.to('cuda'), val_labels.to('cuda'))\n",
        "# val_sampler = RandomSampler(val_data)\n",
        "# val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "ucz_nOHB2Hpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if cuda is available\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T71fkMD29aY",
        "outputId": "02459170-ba6d-41af-ef00-86482612dfca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2, output_attentions=True)\n",
        "model.cuda()  # Use GPU for training if available\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxJP_aEc2dPw",
        "outputId": "59c1f417-831f-4a2f-f4ae-f0e20626e894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the below is taken from the ASDS exam - check whether this is the way to do it or combine with the rest\n",
        "# first check for english version and then adapt for other languages\n",
        "!pip install transformers[torch] accelerate\n",
        "\n"
      ],
      "metadata": {
        "id": "4frZSJ6L2rMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4ad019-fcc0-4dd0-f39c-a61f62c9f0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define parameters for the model\n",
        "training_args = TrainingArguments(output_dir=\"my_trainer\",\n",
        "                                  evaluation_strategy=\"steps\",\n",
        "                                  num_train_epochs=3.0,\n",
        "                                  per_device_train_batch_size=16,\n",
        "                                  eval_steps=500\n",
        "                                  )"
      ],
      "metadata": {
        "id": "pWD-6Sw1MZ4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the compute_metrics function for the trainer\n",
        "metric_f1 = load_metric('f1')\n",
        "metric_ac = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    outputs, labels = eval_pred\n",
        "    predictions = np.argmax(outputs, axis=-1)\n",
        "    f1 = metric_f1.compute(predictions=predictions, references=labels)\n",
        "    ac = metric_ac.compute(predictions=predictions, references=labels)\n",
        "    return f1 | ac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvB-DPQ22w0o",
        "outputId": "97c42106-5440-4854-a45b-c8ce7ace4116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-44264bb71349>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric_f1 = load_metric('f1')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # for first testing with english\n",
        "# trainer_eng = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=train_input_ids,\n",
        "#     eval_dataset=val_input_ids,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     tokenizer=tokenizer\n",
        "# )"
      ],
      "metadata": {
        "id": "4iRSttSd7Sub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "# epochs = 4\n",
        "# total_steps = len(train_dataloader) * epochs\n",
        "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "PAourslR2gAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model.device)\n",
        "# print(train_input_ids[0].device)\n",
        "# # print(attention_masks[0].device)\n",
        "# print(train_labels.device)\n"
      ],
      "metadata": {
        "id": "GpmWsRHQ6ug8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_input_ids.cuda()"
      ],
      "metadata": {
        "id": "qEJsKYlFfr8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training loop\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "#     for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "#         inputs = batch[:2]\n",
        "#         labels = batch[2]\n",
        "\n",
        "#         model.zero_grad()\n",
        "#         outputs = model(*inputs, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "#         total_loss += loss.item()  # Accumulate the loss\n",
        "#         loss.backward()\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "#         optimizer.step()\n",
        "#         scheduler.step()\n",
        "\n",
        "#     average_loss = total_loss / len(train_dataloader)  # Compute the average loss for the epoch\n",
        "\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     true_labels = []\n",
        "#     for batch in tqdm(val_dataloader, desc=f\"Evaluating Epoch {epoch + 1}\"):\n",
        "#         inputs = batch[:2]\n",
        "#         labels = batch[2]\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(*inputs)\n",
        "#         logits = outputs.logits\n",
        "#         predictions.extend(logits.argmax(dim=1).tolist())\n",
        "#         true_labels.extend(labels.tolist())\n",
        "\n",
        "#     accuracy = accuracy_score(true_labels, predictions)\n",
        "#     report = classification_report(true_labels, predictions, target_names=[\"Not Answerable\", \"Answerable\"])\n",
        "#     print(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\")\n",
        "#     print(report)\n",
        "\n",
        "#     # Saving to Google Drive\n",
        "#     save_path = \"/content/drive/My Drive/Colab Notebooks/NLP/\"\n",
        "#     with open(save_path + f\"results_epoch_{epoch}.txt\", 'w') as f:\n",
        "#         f.write(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\\n\")\n",
        "#         f.write(report)\n"
      ],
      "metadata": {
        "id": "-9Gmsd6uX5Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Indonesian\n",
        "\n",
        "train_input_ids_indonesian, train_attention_masks_indonesian = tokenize_text(df_train_indonesian_merged[\"text\"].tolist())\n",
        "val_input_ids_indonesian, val_attention_masks_indonesian = tokenize_text(df_val_indonesian_merged[\"text\"].tolist())\n",
        "train_labels_indonesian = torch.tensor(df_train_indonesian_merged[\"answerable\"].tolist())\n",
        "val_labels_indonesian = torch.tensor(df_val_indonesian_merged[\"answerable\"].tolist())\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_indonesian = TensorDataset(train_input_ids_indonesian.to('cuda'), train_attention_masks_indonesian.to('cuda'), train_labels_indonesian.to('cuda'))\n",
        "train_sampler_indonesian = RandomSampler(train_data_indonesian)\n",
        "train_dataloader_indonesian = DataLoader(train_data_indonesian, sampler=train_sampler_indonesian, batch_size=batch_size)\n",
        "\n",
        "val_data_indonesian = TensorDataset(val_input_ids_indonesian.to('cuda'), val_attention_masks_indonesian.to('cuda'), val_labels_indonesian.to('cuda'))\n",
        "val_sampler_indonesian = RandomSampler(val_data_indonesian)\n",
        "val_dataloader_indonesian = DataLoader(val_data_indonesian, sampler=val_sampler_indonesian, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "sEUUfCWN3p4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the trainer object\n",
        "trainer_indonesian = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_input_ids_indonesian,\n",
        "    eval_dataset=val_input_ids_indonesian,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "b1p4wc2PJsId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader_indonesian) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "bsKw_Dh6Js3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "    for batch in tqdm(train_dataloader_indonesian, desc=f\"Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(*inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader_indonesian)  # Compute the average loss for the epoch\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader_indonesian, desc=f\"Evaluating Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(*inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"Not Answerable\", \"Answerable\"])\n",
        "    print(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\")\n",
        "    print(report)\n",
        "\n",
        "    # Saving to Google Drive\n",
        "    save_path = \"/content/drive/My Drive/Colab Notebooks/NLP/\"\n",
        "    with open(save_path + f\"indonesian_results_epoch_{epoch}.txt\", 'w') as f:\n",
        "        f.write(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f}\\n\")\n",
        "        f.write(report)"
      ],
      "metadata": {
        "id": "IpV_1rKN29Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Bengali\n",
        "\n",
        "train_input_ids_bengali, train_attention_masks_bengali = tokenize_text(df_train_bengali_merged[\"text\"].tolist())\n",
        "val_input_ids_bengali, val_attention_masks_bengali = tokenize_text(df_val_bengali_merged[\"text\"].tolist())\n",
        "train_labels_bengali = torch.tensor(df_train_bengali_merged[\"answerable\"].tolist())\n",
        "val_labels_bengali = torch.tensor(df_val_bengali_merged[\"answerable\"].tolist())\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_bengali = TensorDataset(train_input_ids_bengali.to('cuda'), train_attention_masks_bengali.to('cuda'), train_labels_bengali.to('cuda'))\n",
        "train_sampler_bengali = RandomSampler(train_data_bengali)\n",
        "train_dataloader_bengali = DataLoader(train_data_bengali, sampler=train_sampler_bengali, batch_size=batch_size)\n",
        "\n",
        "val_data_bengali = TensorDataset(val_input_ids_bengali.to('cuda'), val_attention_masks_bengali.to('cuda'), val_labels_bengali.to('cuda'))\n",
        "val_sampler_bengali = RandomSampler(val_data_bengali)\n",
        "val_dataloader_bengali = DataLoader(val_data_bengali, sampler=val_sampler_bengali, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "s2xosqFqlV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the trainer object\n",
        "trainer_bengali = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_input_ids_bengali,\n",
        "    eval_dataset=val_input_ids_bengali,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "9QazchflKT1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader_bengali) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "LSvXOuJ9KT1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store losses\n",
        "train_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "    for batch in tqdm(train_dataloader_bengali, desc=f\"Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(*inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader_bengali)  # Compute the average loss for the epoch\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader_bengali, desc=f\"Evaluating Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(*inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"Not Answerable\", \"Answerable\"])\n",
        "    print(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\")\n",
        "    print(report)\n"
      ],
      "metadata": {
        "id": "6AGlWGEU4a_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Arabic\n",
        "\n",
        "train_input_ids_arabic, train_attention_masks_arabic = tokenize_text(df_train_arabic_merged[\"text\"].tolist())\n",
        "val_input_ids_arabic, val_attention_masks_arabic = tokenize_text(df_val_arabic_merged[\"text\"].tolist())\n",
        "train_labels_arabic = torch.tensor(df_train_arabic_merged[\"answerable\"].tolist())\n",
        "val_labels_arabic = torch.tensor(df_val_arabic_merged[\"answerable\"].tolist())\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_arabic = TensorDataset(train_input_ids_arabic.to('cuda'), train_attention_masks_arabic.to('cuda'), train_labels_arabic.to('cuda'))\n",
        "train_sampler_arabic = RandomSampler(train_data_arabic)\n",
        "train_dataloader_arabic = DataLoader(train_data_arabic, sampler=train_sampler_arabic, batch_size=batch_size)\n",
        "\n",
        "val_data_arabic = TensorDataset(val_input_ids_arabic.to('cuda'), val_attention_masks_arabic.to('cuda'), val_labels_arabic.to('cuda'))\n",
        "val_sampler_arabic = RandomSampler(val_data_arabic)\n",
        "val_dataloader_arabic = DataLoader(val_data_arabic, sampler=val_sampler_arabic, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "Gv3c87WBsjMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the trainer object\n",
        "trainer_arabic = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_input_ids_arabic,\n",
        "    eval_dataset=val_input_ids_arabic,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "PlNmmKEWKVND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader_arabic) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "v5j5uQw9KVNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "    for batch in tqdm(train_dataloader_arabic, desc=f\"Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(*inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader_arabic)  # Compute the average loss for the epoch\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader_arabic, desc=f\"Evaluating Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(*inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"Not Answerable\", \"Answerable\"])\n",
        "    print(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "V1r0WonD4gNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt for week 41 exercise\n",
        "\n",
        "# Assuming you want to visualize attention for a specific instance (e.g., the first instance in the validation dataset)\n",
        "instance_index = 0\n",
        "\n",
        "# Prepare input for the selected instance\n",
        "inputs = {\n",
        "    'input_ids': val_input_ids_arabic[instance_index].unsqueeze(0).to('cuda'),\n",
        "    'attention_mask': val_attention_masks_arabic[instance_index].unsqueeze(0).to('cuda')\n",
        "}\n",
        "\n",
        "# Pass the input through the model to get attentions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "attentions_arabic = outputs.attentions  # This will contain attention scores for different layers"
      ],
      "metadata": {
        "id": "cLufZMzFmmEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you want to visualize attention for layer 3, head 0\n",
        "layer_idx = 3\n",
        "head_idx = 0\n",
        "\n",
        "# Get the attention scores for the selected layer and head\n",
        "attention_matrix = attentions_arabic[layer_idx][0][head_idx].cpu().numpy()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(attention_matrix, cmap='YlGnBu', xticklabels=False, yticklabels=False)\n",
        "plt.title(f'Attention Map for Layer {layer_idx}, Head {head_idx}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b9NxRxN13TQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hg5yMH223eDf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIv8wSln/ZLxkXXjnY3Bj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLFlexer/nlp-course/blob/Emma/bert_classification_more_epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X72usktzzUIF",
        "outputId": "843c893a-6894-4daa-aeca-c293a663e28a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bpemb in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb) (4.66.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->bpemb) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb) (2023.7.22)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bpemb\n",
        "!pip install gensim\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add-in as occasionally receive an error which requires this to be added\n",
        "# uncomment if the issue arises\n",
        "!pip install transformer[torch]"
      ],
      "metadata": {
        "id": "j1FeucwT_K-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch\n",
        "import datasets\n",
        "datasets.logging.set_verbosity_error()\n",
        "from datasets import load_metric\n",
        "from google.colab import drive\n",
        "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# # uncomment if CAN'T CONNECT TO GPU (it happens...)\n",
        "# import psutil\n",
        "# import platform"
      ],
      "metadata": {
        "id": "erIIBBN-ebGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to save output of models so they can be reloaded\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "output_dir = '/content/drive/My Drive/Colab Notebooks/NLP/'\n"
      ],
      "metadata": {
        "id": "OD-nst6u-7jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU housekeeping code: you do not need to modify anything, simply\n",
        "# read through it to understand what is going on, and run as is\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# a helper function to format byte counts into KB, MB and so on\n",
        "def bytes_format(b):\n",
        "    if b < 1000:\n",
        "              return f'{b} B'\n",
        "    elif b < 1000000:\n",
        "        return f'{round(float(b/1000),2)} KB'\n",
        "    elif b < 1000000000:\n",
        "        return f'{round(float(b/1000000),2)} MB'\n",
        "    else:\n",
        "        return f'{round(float(b/1000000000),2)} GB'\n",
        "\n",
        "# a helper function to check the amount of available memory\n",
        "def memory_report():\n",
        "  if device!='cpu':\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name()}\")\n",
        "    #print(torch.cuda.memory_summary())\n",
        "    total = torch.cuda.get_device_properties(0).total_memory\n",
        "    reserved = torch.cuda.memory_reserved(0)\n",
        "    allocated = torch.cuda.memory_allocated(0)\n",
        "  #  free = reserved-allocated  # free inside memory_reserved\n",
        "    print(f\"Total cuda memory: {bytes_format(total)}, reserved: {bytes_format(reserved)}, allocated: {bytes_format(allocated)}\")\n",
        "  else:\n",
        "    # Print total memory available on CPU\n",
        "    print(f'Device is CPU {platform.processor()}. GPU is not available rn')\n",
        "    total_memory = psutil.virtual_memory().total\n",
        "    print(f\"Total CPU memory: {bytes_format(total_memory)}\")\n",
        "\n",
        "memory_report()"
      ],
      "metadata": {
        "id": "6T11MhtBdv1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AheXHlZxQ38o"
      },
      "outputs": [],
      "source": [
        "# Preamble\n",
        "import sys\n",
        "\n",
        "sys.path.append('..')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z5S6IwPQ38o"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n",
        "\n",
        "train_set = dataset[\"train\"]\n",
        "validation_set = dataset[\"validation\"]\n",
        "\n",
        "df_train = train_set.to_pandas()\n",
        "df_val = validation_set.to_pandas()\n",
        "\n",
        "print(len(df_train))\n",
        "print(len(df_val))\n",
        "\n",
        "df_train.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15Hvm5s-Q38p"
      },
      "outputs": [],
      "source": [
        "# Get train and validation data for each language\n",
        "df_train_bengali = df_train[df_train['language'] == 'bengali']\n",
        "df_train_arabic = df_train[df_train['language'] == 'arabic']\n",
        "df_train_indonesian = df_train[df_train['language'] == 'indonesian']\n",
        "\n",
        "df_val_bengali = df_val[df_val['language'] == 'bengali']\n",
        "df_val_arabic = df_val[df_val['language'] == 'arabic']\n",
        "df_val_indonesian = df_val[df_val['language'] == 'indonesian']\n",
        "\n",
        "\n",
        "# For testing\n",
        "df_val_english = df_val[df_val['language'] == 'english']\n",
        "df_train_english = df_train[df_train['language'] == 'english']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyIm3zKzQ38p"
      },
      "outputs": [],
      "source": [
        "# Create a new dataframe with the combined documents and questions and add if they are answerable\n",
        "df_train_bengali_merged = pd.DataFrame({\n",
        "    'text':(df_train_bengali[\"document_plaintext\"] + df_train_bengali[\"question_text\"]),\n",
        "    'answerable':(df_train_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_train_arabic_merged = pd.DataFrame({\n",
        "    'text': (df_train_arabic[\"document_plaintext\"] + df_train_arabic[\"question_text\"]),\n",
        "    'answerable': (df_train_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "                                    })\n",
        "df_train_indonesian_merged = pd.DataFrame({\n",
        "    'text':(df_train_indonesian[\"document_plaintext\"] + df_train_indonesian[\"question_text\"]),\n",
        "    'answerable':(df_train_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_train_english_merged = pd.DataFrame({\n",
        "    'text':(df_train_english[\"document_plaintext\"] + df_train_english[\"question_text\"]),\n",
        "    'answerable':(df_train_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "\n",
        "\n",
        "## Same for validation data\n",
        "df_val_bengali_merged = pd.DataFrame({\n",
        "    'text':(df_val_bengali[\"document_plaintext\"] + df_val_bengali[\"question_text\"]),\n",
        "    'answerable':(df_val_bengali[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_val_arabic_merged = pd.DataFrame({\n",
        "    'text': (df_val_arabic[\"document_plaintext\"] + df_val_arabic[\"question_text\"]),\n",
        "    'answerable': (df_val_arabic[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "                                    })\n",
        "df_val_indonesian_merged = pd.DataFrame({\n",
        "    'text':(df_val_indonesian[\"document_plaintext\"] + df_val_indonesian[\"question_text\"]),\n",
        "    'answerable':(df_val_indonesian[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "df_val_english_merged = pd.DataFrame({\n",
        "    'text':(df_val_english[\"document_plaintext\"] + df_val_english[\"question_text\"]),\n",
        "    'answerable':(df_val_english[\"annotations\"].apply(lambda x: 0 if x['answer_start'] == [-1] else 1))\n",
        "    })\n",
        "\n",
        "df_val_english_merged.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Tokenize and encode the text data\n",
        "def tokenize_text(texts, max_length=128):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded_text = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids.append(encoded_text[\"input_ids\"])\n",
        "        attention_masks.append(encoded_text[\"attention_mask\"])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "# # first for english to test\n",
        "# train_input_ids, train_attention_masks = tokenize_text(df_train_english_merged[\"text\"].tolist())\n",
        "# val_input_ids, val_attention_masks = tokenize_text(df_val_english_merged[\"text\"].tolist())\n",
        "# train_labels = torch.tensor(df_train_english_merged[\"answerable\"].tolist())\n",
        "# val_labels = torch.tensor(df_val_english_merged[\"answerable\"].tolist())\n"
      ],
      "metadata": {
        "id": "udS3DYSa06OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # with english dataset\n",
        "# batch_size = 32\n",
        "\n",
        "# train_data = TensorDataset(train_input_ids.to('cuda'), train_attention_masks.to('cuda'), train_labels.to('cuda'))\n",
        "# train_sampler = RandomSampler(train_data)\n",
        "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# val_data = TensorDataset(val_input_ids.to('cuda'), val_attention_masks.to('cuda'), val_labels.to('cuda'))\n",
        "# val_sampler = RandomSampler(val_data)\n",
        "# val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "ucz_nOHB2Hpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if cuda is available\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "8T71fkMD29aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2, output_attentions=True)\n",
        "model.cuda()  # Use GPU for training if available\n"
      ],
      "metadata": {
        "id": "UxJP_aEc2dPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the below is taken from the ASDS exam - check whether this is the way to do it or combine with the rest\n",
        "# first check for english version and then adapt for other languages\n",
        "!pip install transformers[torch] accelerate\n",
        "\n"
      ],
      "metadata": {
        "id": "4frZSJ6L2rMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define parameters for the model\n",
        "training_args = TrainingArguments(output_dir=\"my_trainer\",\n",
        "                                  evaluation_strategy=\"steps\",\n",
        "                                  num_train_epochs=3.0,\n",
        "                                  per_device_train_batch_size=16,\n",
        "                                  eval_steps=500\n",
        "                                  )"
      ],
      "metadata": {
        "id": "pWD-6Sw1MZ4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the compute_metrics function for the trainer\n",
        "metric_f1 = load_metric('f1')\n",
        "metric_ac = load_metric('accuracy')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    outputs, labels = eval_pred\n",
        "    predictions = np.argmax(outputs, axis=-1)\n",
        "    f1 = metric_f1.compute(predictions=predictions, references=labels)\n",
        "    ac = metric_ac.compute(predictions=predictions, references=labels)\n",
        "    return f1 | ac"
      ],
      "metadata": {
        "id": "rvB-DPQ22w0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Indonesian\n",
        "\n",
        "train_input_ids_indonesian, train_attention_masks_indonesian = tokenize_text(df_train_indonesian_merged[\"text\"].tolist())\n",
        "val_input_ids_indonesian, val_attention_masks_indonesian = tokenize_text(df_val_indonesian_merged[\"text\"].tolist())\n",
        "train_labels_indonesian = torch.tensor(df_train_indonesian_merged[\"answerable\"].tolist())\n",
        "val_labels_indonesian = torch.tensor(df_val_indonesian_merged[\"answerable\"].tolist())\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_indonesian = TensorDataset(train_input_ids_indonesian.to('cuda'), train_attention_masks_indonesian.to('cuda'), train_labels_indonesian.to('cuda'))\n",
        "train_sampler_indonesian = RandomSampler(train_data_indonesian)\n",
        "train_dataloader_indonesian = DataLoader(train_data_indonesian, sampler=train_sampler_indonesian, batch_size=batch_size)\n",
        "\n",
        "val_data_indonesian = TensorDataset(val_input_ids_indonesian.to('cuda'), val_attention_masks_indonesian.to('cuda'), val_labels_indonesian.to('cuda'))\n",
        "val_sampler_indonesian = SequentialSampler(val_data_indonesian)\n",
        "val_dataloader_indonesian = DataLoader(val_data_indonesian, sampler=val_sampler_indonesian, batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "sEUUfCWN3p4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the trainer object\n",
        "trainer_indonesian = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_input_ids_indonesian,\n",
        "    eval_dataset=val_input_ids_indonesian,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "b1p4wc2PJsId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader_indonesian) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "bsKw_Dh6Js3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "    for batch in tqdm(train_dataloader_indonesian, desc=f\"Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(*inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader_indonesian)  # Compute the average loss for the epoch\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader_indonesian, desc=f\"Evaluating Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(*inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"Not Answerable\", \"Answerable\"])\n",
        "    print(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\")\n",
        "    print(report)\n",
        "\n",
        "    # Saving to Google Drive\n",
        "    save_path = \"/content/drive/My Drive/Colab Notebooks/NLP/\"\n",
        "    with open(save_path + f\"indonesian_results_epoch_{epoch}.txt\", 'w') as f:\n",
        "        f.write(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f}\\n\")\n",
        "        f.write(report)"
      ],
      "metadata": {
        "id": "IpV_1rKN29Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Bengali\n",
        "\n",
        "train_input_ids_bengali, train_attention_masks_bengali = tokenize_text(df_train_bengali_merged[\"text\"].tolist())\n",
        "val_input_ids_bengali, val_attention_masks_bengali = tokenize_text(df_val_bengali_merged[\"text\"].tolist())\n",
        "train_labels_bengali = torch.tensor(df_train_bengali_merged[\"answerable\"].tolist())\n",
        "val_labels_bengali = torch.tensor(df_val_bengali_merged[\"answerable\"].tolist())\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_bengali = TensorDataset(train_input_ids_bengali.to('cuda'), train_attention_masks_bengali.to('cuda'), train_labels_bengali.to('cuda'))\n",
        "train_sampler_bengali = RandomSampler(train_data_bengali)\n",
        "train_dataloader_bengali = DataLoader(train_data_bengali, sampler=train_sampler_bengali, batch_size=batch_size)\n",
        "\n",
        "val_data_bengali = TensorDataset(val_input_ids_bengali.to('cuda'), val_attention_masks_bengali.to('cuda'), val_labels_bengali.to('cuda'))\n",
        "val_sampler_bengali = SequentialSampler(val_data_bengali)\n",
        "val_dataloader_bengali = DataLoader(val_data_bengali, sampler=val_sampler_bengali, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "s2xosqFqlV1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the trainer object\n",
        "trainer_bengali = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_input_ids_bengali,\n",
        "    eval_dataset=val_input_ids_bengali,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "9QazchflKT1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader_bengali) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "LSvXOuJ9KT1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store losses\n",
        "train_losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "    for batch in tqdm(train_dataloader_bengali, desc=f\"Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(*inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader_bengali)  # Compute the average loss for the epoch\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader_bengali, desc=f\"Evaluating Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(*inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"Not Answerable\", \"Answerable\"])\n",
        "    print(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\")\n",
        "    print(report)\n"
      ],
      "metadata": {
        "id": "6AGlWGEU4a_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Arabic\n",
        "\n",
        "train_input_ids_arabic, train_attention_masks_arabic = tokenize_text(df_train_arabic_merged[\"text\"].tolist())\n",
        "val_input_ids_arabic, val_attention_masks_arabic = tokenize_text(df_val_arabic_merged[\"text\"].tolist())\n",
        "train_labels_arabic = torch.tensor(df_train_arabic_merged[\"answerable\"].tolist())\n",
        "val_labels_arabic = torch.tensor(df_val_arabic_merged[\"answerable\"].tolist())\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_arabic = TensorDataset(train_input_ids_arabic.to('cuda'), train_attention_masks_arabic.to('cuda'), train_labels_arabic.to('cuda'))\n",
        "train_sampler_arabic = RandomSampler(train_data_arabic)\n",
        "train_dataloader_arabic = DataLoader(train_data_arabic, sampler=train_sampler_arabic, batch_size=batch_size)\n",
        "\n",
        "val_data_arabic = TensorDataset(val_input_ids_arabic.to('cuda'), val_attention_masks_arabic.to('cuda'), val_labels_arabic.to('cuda'))\n",
        "val_sampler_arabic = SequentialSampler(val_data_arabic)\n",
        "val_dataloader_arabic = DataLoader(val_data_arabic, sampler=val_sampler_arabic, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "Gv3c87WBsjMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the trainer object\n",
        "trainer_arabic = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_input_ids_arabic,\n",
        "    eval_dataset=val_input_ids_arabic,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "PlNmmKEWKVND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "epochs = 10\n",
        "total_steps = len(train_dataloader_arabic) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "v5j5uQw9KVNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0  # Initialize the total loss for the epoch\n",
        "\n",
        "    for batch in tqdm(train_dataloader_arabic, desc=f\"Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(*inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()  # Accumulate the loss\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader_arabic)  # Compute the average loss for the epoch\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    for batch in tqdm(val_dataloader_arabic, desc=f\"Evaluating Epoch {epoch + 1}\"):\n",
        "        inputs = batch[:2]\n",
        "        labels = batch[2]\n",
        "        with torch.no_grad():\n",
        "            outputs = model(*inputs)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(logits.argmax(dim=1).tolist())\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    report = classification_report(true_labels, predictions, target_names=[\"Not Answerable\", \"Answerable\"])\n",
        "    print(f\"Epoch {epoch + 1} - Accuracy: {accuracy:.4f} - Average Loss: {average_loss:.4f}\")\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "V1r0WonD4gNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attempt for week 41 exercise\n",
        "\n",
        "# Assuming you want to visualize attention for a specific instance (e.g., the first instance in the validation dataset)\n",
        "instance_index = 0\n",
        "\n",
        "# Prepare input for the selected instance\n",
        "inputs = {\n",
        "    'input_ids': val_input_ids_arabic[instance_index].unsqueeze(0).to('cuda'),\n",
        "    'attention_mask': val_attention_masks_arabic[instance_index].unsqueeze(0).to('cuda')\n",
        "}\n",
        "\n",
        "# Pass the input through the model to get attentions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "attentions_arabic = outputs.attentions  # This will contain attention scores for different layers"
      ],
      "metadata": {
        "id": "cLufZMzFmmEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you want to visualize attention for layer 3, head 0\n",
        "layer_idx = 3\n",
        "head_idx = 0\n",
        "\n",
        "# Get the attention scores for the selected layer and head\n",
        "attention_matrix = attentions_arabic[layer_idx][0][head_idx].cpu().numpy()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(attention_matrix, cmap='YlGnBu', xticklabels=False, yticklabels=False)\n",
        "plt.title(f'Attention Map for Layer {layer_idx}, Head {head_idx}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b9NxRxN13TQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hg5yMH223eDf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
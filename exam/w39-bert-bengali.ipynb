{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/MLFlexer/nlp-course/blob/malthe/assignments/w4_bert_ben.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ZTpCe2R1L_yd"},"source":["# Week 39 BERT Bengali\n","This notebook uses code from lab 6: https://github.com/MLFlexer/nlp-course/blob/cbbb4bc13d7d5f639ada243104b9d85efe1dc166/labs/notebooks_2023/lab_6.ipynb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mAF--cPLf5y","outputId":"e7836240-0678-45c4-da92-a2e03df66b9e"},"outputs":[],"source":["!pip install update transformers\n","!pip install datasets\n","!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4d_FN4UqwhT"},"outputs":[],"source":["from datasets import load_dataset\n","from datasets import load_metric\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForQuestionAnswering\n","from transformers import AutoConfig\n","from functools import partial\n","import torch\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from torch.optim.lr_scheduler import LambdaLR\n","from torch import nn\n","from collections import defaultdict, OrderedDict\n","# MODEL_NAME = 'xlm-roberta-base'\n","MODEL_NAME = 'bert-base-multilingual-uncased'\n","NUM_SUBSAMPLES = 4779\n","#bengali: 4779\n","#Arabic: 29598\n","#Indonesian: 11394\n","LANGUAGE = \"bengali\" # \"arabic\" \"indonesian\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kC8A3_ytq43o"},"outputs":[],"source":["def enforce_reproducibility(seed=42):\n","    # Sets seed manually for both CPU and CUDA\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # For atomic operations there is currently\n","    # no simple way to enforce determinism, as\n","    # the order of parallel operations is not known.\n","    # CUDNN\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # System based\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","device = torch.device(\"cpu\")\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","\n","enforce_reproducibility()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XB3XXwrpLZLp"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"copenlu/answerable_tydiqa\")\n","\n","filtered_dataset = dataset.filter(lambda entry: entry[\"language\"] in [LANGUAGE])\n","\n","train_set = filtered_dataset[\"train\"]\n","validation_set = filtered_dataset[\"validation\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2Hu9T0qh0IL"},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","train_set_df = train_set.to_pandas()\n","train_set_df['id'] = range(len(train_set_df))\n","validation_set_df = validation_set.to_pandas()\n","validation_set_df['id'] = range(len(validation_set_df))\n","\n","train_set = Dataset.from_pandas(train_set_df)\n","validation_set = Dataset.from_pandas(validation_set_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zeTDJL35LvwO","outputId":"749262f2-79dc-4f22-a274-5334794b9898"},"outputs":[],"source":["print(len(validation_set))\n","train_set[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PXY8QobAruAB"},"outputs":[],"source":["tk = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lO4rSfT3saW4"},"outputs":[],"source":["def get_train_features(tk, samples):\n","  '''\n","  Tokenizes all of the text in the given samples, splittling inputs that are too long for our model\n","  across multiple features. Finds the token offsets of the answers, which serve as the labels for\n","  our inputs.\n","  '''\n","  batch = tk.batch_encode_plus(\n","        [[q,c] for q,c in zip(samples['question_text'], samples['document_plaintext'])],\n","        padding='max_length',\n","        truncation='only_second',\n","        stride=128,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True\n","    )\n","\n","  # Get a list which maps the input features index to their original index in the\n","  # samples list (for split inputs). E.g. if our batch size is 4 and the second sample\n","  # is split into 3 inputs because it is very large, sample_mapping would look like\n","  # [0, 1, 1, 1, 2, 3]\n","  sample_mapping = batch.pop('overflow_to_sample_mapping')\n","  # Get all of the character offsets for each token\n","  offset_mapping = batch.pop('offset_mapping')\n","\n","  # Store the start and end tokens\n","  batch['start_tokens'] = []\n","  batch['end_tokens'] = []\n","\n","  # Iterate through all of the offsets\n","  for i, offsets in enumerate(offset_mapping):\n","    # Get the right sample by mapping it to its original index\n","    sample_idx = sample_mapping[i]\n","    # Get the sequence IDs to know where context starts so we can ignore question tokens\n","    sequence_ids = batch.sequence_ids(i)\n","\n","    # Get the start and end character positions of the answer\n","    ans = samples['annotations'][sample_idx]\n","    start_char = ans['answer_start'][0]\n","    end_char = start_char + len(ans['answer_text'][0])\n","    # while end_char > 0 and (end_char >= len(samples['context'][sample_idx]) or samples['context'][sample_idx][end_char] == ' '):\n","    #   end_char -= 1\n","\n","    # Start from the first token in the context, which can be found by going to the\n","    # first token where sequence_ids is 1\n","    start_token = 0\n","    while sequence_ids[start_token] != 1:\n","      start_token += 1\n","\n","    end_token = len(offsets) - 1\n","    while sequence_ids[end_token] != 1:\n","      end_token -= 1\n","\n","    # By default set it to the CLS token if the answer isn't in this input\n","    if start_char < offsets[start_token][0] or end_char > offsets[end_token][1]:\n","      start_token = 0\n","      end_token = 0\n","    # Otherwise find the correct token indices\n","    else:\n","      # Advance the start token index until we have passed the start character index\n","      while start_token < len(offsets) and offsets[start_token][0] <= start_char:\n","        start_token += 1\n","      start_token -= 1\n","\n","      # Decrease the end token index until we have passed the end character index\n","      while end_token >= 0 and offsets[end_token][1] >= end_char:\n","        end_token -= 1\n","      end_token += 1\n","\n","    batch['start_tokens'].append(start_token)\n","    batch['end_tokens'].append(end_token)\n","\n","  #batch['start_tokens'] = np.array(batch['start_tokens'])\n","  #batch['end_tokens'] = np.array(batch['end_tokens'])\n","\n","  return batch\n","\n","def collate_fn(inputs):\n","  '''\n","  Defines how to combine different samples in a batch\n","  '''\n","  input_ids = torch.tensor([i['input_ids'] for i in inputs])\n","  attention_mask = torch.tensor([i['attention_mask'] for i in inputs])\n","  start_tokens = torch.tensor([i['start_tokens'] for i in inputs])\n","  end_tokens = torch.tensor([i['end_tokens'] for i in inputs])\n","\n","  # Truncate to max length\n","  max_len = max(attention_mask.sum(-1))\n","  input_ids = input_ids[:,:max_len]\n","  attention_mask = attention_mask[:,:max_len]\n","\n","  return {'input_ids': input_ids, 'attention_mask': attention_mask, 'start_tokens': start_tokens, 'end_tokens': end_tokens}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["26d4aa7cf7284d44beccaffc11cb051c","67897069762c40658ced65a742355869","13ecca4ce65b48fb9288073930d0fd45","2fc457ef6897481482750337181d8a98","82553d004cad48e69feefaace5bd5944","1a994b43feab49b29bef35159151c868","ff3a59810f56463ea86f075032454ce8","8d9b7c9dad2542d1be687f4bb4143ad6","f070dd90187d40b182bb7d3260acbe78","498e82b151cc40848c8ae1f528bdb4db","82c025e832744f87acfb987639e0e875"]},"id":"8dBau9r7seZU","outputId":"45994b4f-0a79-4395-f152-1c2e29f7f137"},"outputs":[],"source":["tokenized_dataset = train_set.map(partial(get_train_features, tk), batched=True, remove_columns=train_set.column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w66oxEb9tBdt","outputId":"5d3d90cb-52aa-4fb2-a540-fddb56a24110"},"outputs":[],"source":["tokenized_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExRWEMevtE8z"},"outputs":[],"source":["samples = random.sample(list(range(len(tokenized_dataset))), NUM_SUBSAMPLES)\n","tokenized_dataset = tokenized_dataset.select(samples)\n","train_dl = DataLoader(tokenized_dataset, collate_fn=collate_fn, shuffle=True, batch_size=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fuszid5REYXG"},"outputs":[],"source":["import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1t6iqEbtc9D"},"outputs":[],"source":["def train(\n","    model: nn.Module,\n","    train_dl: DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    schedule: LambdaLR,\n","    n_epochs: int,\n","    device: torch.device\n","):\n","  \"\"\"\n","  The main training loop which will optimize a given model on a given dataset\n","  :param model: The model being optimized\n","  :param train_dl: The training dataset\n","  :param optimizer: The optimizer used to update the model parameters\n","  :param n_epochs: Number of epochs to train for\n","  :param device: The device to train on\n","  \"\"\"\n","\n","  # Keep track of the loss and best accuracy\n","  losses = []\n","  best_acc = 0.0\n","  pcounter = 0\n","\n","  # Iterate through epochs\n","  for ep in range(n_epochs):\n","\n","    loss_epoch = []\n","\n","    #Iterate through each batch in the dataloader\n","    for batch in tqdm(train_dl):\n","      # VERY IMPORTANT: Make sure the model is in training mode, which turns on\n","      # things like dropout and layer normalization\n","      model.train()\n","\n","      # VERY IMPORTANT: zero out all of the gradients on each iteration -- PyTorch\n","      # keeps track of these dynamically in its computation graph so you need to explicitly\n","      # zero them out\n","      optimizer.zero_grad()\n","\n","      # Place each tensor on the GPU\n","      batch = {b: batch[b].to(device) for b in batch}\n","\n","      # Pass the inputs through the model, get the current loss and logits\n","      outputs = model(\n","          input_ids=batch['input_ids'],\n","          attention_mask=batch['attention_mask'],\n","          start_positions=batch['start_tokens'],\n","          end_positions=batch['end_tokens']\n","      )\n","      loss = outputs['loss']\n","      losses.append(loss.item())\n","      loss_epoch.append(loss.item())\n","\n","      # Calculate all of the gradients and weight updates for the model\n","      loss.backward()\n","\n","      # Optional: clip gradients\n","      #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","      # Finally, update the weights of the model and advance the LR schedule\n","      optimizer.step()\n","      scheduler.step()\n","      gc.collect()\n","  return losses"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9oBUnR-tezT","outputId":"568b0c18-e623-4221-9e29-715ae02969e4"},"outputs":[],"source":["model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JkQShWSQth50","outputId":"bdb49912-8bbe-405d-92f3-465174210281"},"outputs":[],"source":["# Create the optimizer\n","lr=2e-5\n","n_epochs = 5\n","weight_decay = 0.01\n","warmup_steps = 200\n","\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","      'weight_decay': weight_decay},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","# optimizer = Adam(optimizer_grouped_parameters, lr=1e-3)\n","# scheduler = None\n","optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    warmup_steps,\n","    n_epochs * len(train_dl)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aP17eWFRtk2x","outputId":"329b2b05-2196-4bf7-8e55-923799635af8"},"outputs":[],"source":["losses = train(\n","    model,\n","    train_dl,\n","    optimizer,\n","    scheduler,\n","    n_epochs,\n","    device\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dz4_lMebtpbX"},"outputs":[],"source":["def get_validation_features(tk, samples):\n","  # First, tokenize the text. We get the offsets and return overflowing sequences in\n","  # order to break up long sequences into multiple inputs. The offsets will help us\n","  # determine the original answer text\n","  batch = tk.batch_encode_plus(\n","        [[q,c] for q,c in zip(samples['question_text'], samples['document_plaintext'])],\n","        padding='max_length',\n","        truncation='only_second',\n","        stride=128,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True\n","    )\n","\n","  # We'll store the ID of the samples to calculate squad score\n","  batch['example_id'] = []\n","  # The overflow sample map tells us which input each sample corresponds to\n","  sample_map = batch.pop('overflow_to_sample_mapping')\n","\n","  for i in range(len(batch['input_ids'])):\n","    # The sample index tells us which of the values in \"samples\" these features belong to\n","    sample_idx = sample_map[i]\n","    sequence_ids = batch.sequence_ids(i)\n","\n","    # Add the ID to map these features back to the correct sample\n","    batch['example_id'].append(samples['id'][sample_idx])\n","\n","    #Set offsets for non-context words to be None for ease of processing\n","    batch['offset_mapping'][i] = [o if sequence_ids[k] == 1 else None for k,o in enumerate(batch['offset_mapping'][i])]\n","\n","  return batch\n","\n","def val_collate_fn(inputs):\n","  input_ids = torch.tensor([i['input_ids'] for i in inputs])\n","  attention_mask = torch.tensor([i['attention_mask'] for i in inputs])\n","\n","  # Truncate to max length\n","  max_len = max(attention_mask.sum(-1))\n","  input_ids = input_ids[:,:max_len]\n","  attention_mask = attention_mask[:,:max_len]\n","\n","  return {'input_ids': input_ids, 'attention_mask': attention_mask}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ff373df92b834036ad963a91955bbd1b","955f869efde2433188550c262be29fa6","217e97a5a69c41f8be85c6123affeb03","ed08cd57502444c9abb8d49259d2f18b","8d10165cde2942d1ba9a2c512d1aad14","7b824e331c524d8a89d0b8ac67bc78fa","95a3a1b8b32843f19ab7ad0b6b62b683","10448e87ef3d4dde898f230ff819a659","8787200ae0ab42d2a280649406dccd3d","b0d1a10c9c944dbd8c0d7866efd1c588","cc7b263869274af79be78e140481a476"]},"id":"1ywVmDAstrPx","outputId":"7cb292fc-7fef-4f4e-fc58-b5831af0add8"},"outputs":[],"source":["validation_dataset = validation_set.map(partial(get_validation_features, tk), batched=True, remove_columns=validation_set.column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MnBTW-pVuC6w"},"outputs":[],"source":["def predict(model: nn.Module, valid_dl: DataLoader):\n","  \"\"\"\n","  Evaluates the model on the given dataset\n","  :param model: The model under evaluation\n","  :param valid_dl: A `DataLoader` reading validation data\n","  :return: The accuracy of the model on the dataset\n","  \"\"\"\n","  # VERY IMPORTANT: Put your model in \"eval\" mode -- this disables things like\n","  # layer normalization and dropout\n","  model.eval()\n","  start_logits_all = []\n","  end_logits_all = []\n","\n","  # ALSO IMPORTANT: Don't accumulate gradients during this process\n","  with torch.no_grad():\n","    for batch in tqdm(valid_dl, desc='Evaluation'):\n","      batch = {b: batch[b].to(device) for b in batch}\n","\n","      # Pass the inputs through the model, get the current loss and logits\n","      outputs = model(\n","          input_ids=batch['input_ids'],\n","          attention_mask=batch['attention_mask']\n","      )\n","      # Store the \"start\" class logits and \"end\" class logits for every token in the input\n","      start_logits_all.extend(list(outputs['start_logits'].detach().cpu().numpy()))\n","      end_logits_all.extend(list(outputs['end_logits'].detach().cpu().numpy()))\n","\n","\n","    return start_logits_all,end_logits_all\n","\n","def post_process_predictions(examples, dataset, logits, tokenizer, num_possible_answers = 20, max_answer_length = 30):\n","  all_start_logits, all_end_logits = logits\n","  # Build a map from example to its corresponding features. This will allow us to index from\n","  # sample ID to all of the features for that sample (in case they were split up due to long input)\n","  example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","  features_per_example = defaultdict(list)\n","  for i, feature in enumerate(dataset):\n","      features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","  # Create somewhere to store our predictions\n","  predictions = OrderedDict()\n","\n","  # Iterate through each sample in the dataset\n","  for j, sample in enumerate(tqdm(examples)):\n","\n","    # Get the feature indices (all of the features split across the batch)\n","    feature_indices = features_per_example[j]\n","    # Get the original context which predumably has the answer text\n","    context = sample['document_plaintext']\n","\n","    preds = []\n","\n","    min_score_threshold = None\n","\n","    # Iterate through all of the features\n","    for ft_idx in feature_indices:\n","\n","      # Get the start and end answer logits for this input\n","      start_logits = all_start_logits[ft_idx]\n","      end_logits = all_end_logits[ft_idx]\n","\n","      # Get the offsets to map token indices to character indices\n","      offset_mapping = dataset[ft_idx]['offset_mapping']\n","\n","\n","      # Update minimum null prediction.\n","      cls_index = dataset[ft_idx][\"input_ids\"].index(tokenizer.cls_token_id)\n","      feature_min_score_threshold = start_logits[cls_index] + end_logits[cls_index]\n","      if min_score_threshold is None or min_score_threshold < feature_min_score_threshold:\n","          min_score_threshold = feature_min_score_threshold\n","\n","      # Sort the logits and take the top N\n","      start_indices = np.argsort(start_logits)[::-1][:num_possible_answers]\n","      end_indices = np.argsort(end_logits)[::-1][:num_possible_answers]\n","\n","      # Iterate through start and end indices\n","      for start_index in start_indices:\n","        for end_index in end_indices:\n","\n","          # Ignore this combination if either the indices are not in the context\n","          if start_index >= len(offset_mapping) or end_index >= len(offset_mapping) or offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n","            continue\n","\n","          # Also ignore if the start index is greater than the end index of the number of tokens\n","          # is greater than some specified threshold\n","          if start_index > end_index or end_index - start_index + 1 > max_answer_length:\n","            continue\n","\n","          ans_text = context[offset_mapping[start_index][0]:offset_mapping[end_index][1]]\n","          preds.append({\n","              'score': start_logits[start_index] + end_logits[end_index],\n","              'text': ans_text\n","          })\n","\n","    if len(preds) > 0:\n","      # Sort by score to get the top answer\n","      best_answer = sorted(preds, key=lambda x: x['score'], reverse=True)[0]\n","    else:\n","      best_answer = {'score': 0.0, 'text': \"\"}\n","\n","    # if the best answer is below the threshold for lowest score, give it the empty string\n","\n","    answer = best_answer[\"text\"] if best_answer[\"score\"] > min_score_threshold else \"\"\n","    predictions[sample['id']] = answer\n","  return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"48-vkpvnuJ1q","outputId":"9aab5790-7fb2-45da-d612-e7d06ebb893d"},"outputs":[],"source":["val_dl = DataLoader(validation_dataset, collate_fn=val_collate_fn, batch_size=32)\n","logits = predict(model, val_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnDrQ9zB1vt_","outputId":"b425f343-75a7-467c-87ee-014bb1f851ac"},"outputs":[],"source":["validation_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTy-WKaxuLKN","outputId":"aab51fb7-4a11-4796-c82b-391245d928a9"},"outputs":[],"source":["predictions = post_process_predictions(validation_set, validation_dataset, logits, tk)\n","formatted_predictions = [{'id': k, 'prediction_text': v} for k,v in predictions.items()]\n","gold = [{'id': example['id'], 'answers': example['annotations'][\"answer_text\"][0]} for example in validation_set]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ko5CJBuYygA-"},"outputs":[],"source":["from __future__ import print_function\n","from collections import Counter\n","import string\n","import re\n","import argparse\n","import json\n","import sys\n","\n","\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def f1_score(prediction, ground_truth):\n","    prediction_tokens = normalize_answer(prediction).split()\n","    ground_truth_tokens = normalize_answer(ground_truth).split()\n","    if len(prediction_tokens) == 0 and len(ground_truth_tokens) == 0:\n","      return 1\n","    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n","    num_same = sum(common.values())\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(prediction_tokens)\n","    recall = 1.0 * num_same / len(ground_truth_tokens)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def exact_match_score(prediction, ground_truth):\n","    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n","\n","\n","def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n","    scores_for_ground_truths = []\n","    for ground_truth in ground_truths:\n","        score = metric_fn(prediction, ground_truth)\n","        scores_for_ground_truths.append(score)\n","    return max(scores_for_ground_truths)\n","\n","\n","def evaluate_squad(dataset, predictions):\n","    f1 = exact_match = total = 0\n","    for article in dataset:\n","        for paragraph in article['paragraphs']:\n","            for qa in paragraph['qas']:\n","                total += 1\n","                if qa['id'] not in predictions:\n","                    message = 'Unanswered question ' + qa['id'] + \\\n","                              ' will receive score 0.'\n","                    print(message, file=sys.stderr)\n","                    continue\n","                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n","                prediction = predictions[qa['id']]\n","                exact_match += metric_max_over_ground_truths(\n","                    exact_match_score, prediction, ground_truths)\n","                f1 += metric_max_over_ground_truths(\n","                    f1_score, prediction, ground_truths)\n","\n","    exact_match = 100.0 * exact_match / total\n","    f1 = 100.0 * f1 / total\n","\n","    return {'exact_match': exact_match, 'f1': f1}\n","\n","def compute_squad(predictions, references):\n","  pred_dict = {prediction[\"id\"]: prediction[\"prediction_text\"] for prediction in predictions}\n","  dataset = [\n","      {\n","          \"paragraphs\": [\n","              {\n","                  \"qas\": [\n","                      {\n","                          \"answers\": [{\"text\": ref[\"answers\"]} ],\n","                          \"id\": ref[\"id\"],\n","                      }\n","                      for ref in references\n","                  ]\n","              }\n","          ]\n","      }\n","  ]\n","  score = evaluate_squad(dataset=dataset, predictions=pred_dict)\n","  return score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h99TMCPDuj6Q","outputId":"8fdbf5f0-0e7d-4360-84d5-9e4bf86de8ba"},"outputs":[],"source":["compute_squad(references=gold, predictions=formatted_predictions)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPa10zJj5RPx8xvnlp24B+/","gpuType":"T4","include_colab_link":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"10448e87ef3d4dde898f230ff819a659":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13ecca4ce65b48fb9288073930d0fd45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d9b7c9dad2542d1be687f4bb4143ad6","max":11394,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f070dd90187d40b182bb7d3260acbe78","value":11394}},"1a994b43feab49b29bef35159151c868":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"217e97a5a69c41f8be85c6123affeb03":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10448e87ef3d4dde898f230ff819a659","max":1191,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8787200ae0ab42d2a280649406dccd3d","value":1191}},"26d4aa7cf7284d44beccaffc11cb051c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_67897069762c40658ced65a742355869","IPY_MODEL_13ecca4ce65b48fb9288073930d0fd45","IPY_MODEL_2fc457ef6897481482750337181d8a98"],"layout":"IPY_MODEL_82553d004cad48e69feefaace5bd5944"}},"2fc457ef6897481482750337181d8a98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_498e82b151cc40848c8ae1f528bdb4db","placeholder":"​","style":"IPY_MODEL_82c025e832744f87acfb987639e0e875","value":" 11394/11394 [00:18&lt;00:00, 742.98 examples/s]"}},"498e82b151cc40848c8ae1f528bdb4db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67897069762c40658ced65a742355869":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a994b43feab49b29bef35159151c868","placeholder":"​","style":"IPY_MODEL_ff3a59810f56463ea86f075032454ce8","value":"Map: 100%"}},"7b824e331c524d8a89d0b8ac67bc78fa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82553d004cad48e69feefaace5bd5944":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82c025e832744f87acfb987639e0e875":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8787200ae0ab42d2a280649406dccd3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d10165cde2942d1ba9a2c512d1aad14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d9b7c9dad2542d1be687f4bb4143ad6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"955f869efde2433188550c262be29fa6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b824e331c524d8a89d0b8ac67bc78fa","placeholder":"​","style":"IPY_MODEL_95a3a1b8b32843f19ab7ad0b6b62b683","value":"Map: 100%"}},"95a3a1b8b32843f19ab7ad0b6b62b683":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0d1a10c9c944dbd8c0d7866efd1c588":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc7b263869274af79be78e140481a476":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed08cd57502444c9abb8d49259d2f18b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0d1a10c9c944dbd8c0d7866efd1c588","placeholder":"​","style":"IPY_MODEL_cc7b263869274af79be78e140481a476","value":" 1191/1191 [00:01&lt;00:00, 1147.95 examples/s]"}},"f070dd90187d40b182bb7d3260acbe78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ff373df92b834036ad963a91955bbd1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_955f869efde2433188550c262be29fa6","IPY_MODEL_217e97a5a69c41f8be85c6123affeb03","IPY_MODEL_ed08cd57502444c9abb8d49259d2f18b"],"layout":"IPY_MODEL_8d10165cde2942d1ba9a2c512d1aad14"}},"ff3a59810f56463ea86f075032454ce8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":4}
